[["index.html", "kj的学习笔记 前言", " kj的学习笔记 刘柯榉 2024-6-17 前言 在我一开始学习R语言的时候，就是使用王敏杰老师的R语言教程。这既是我第一次接触R语言，也是我第一次接触这种网页书籍（后来才知道这就是bookdown包做的）。我从2024.5.31开始学习R Markdown，在了解了一两天后，很快就被R Markdown的魅力所吸引。我没想到那些网页书籍就是用R Markdown制作的！虽然我之前建立过个人公众号（kj的R语言笔记）来记录学习，但在使用体验上不及R Markdown。于是，我很自然地产生了新的想法——我想用R Markdown来记录我现在及之后的学习经历。而这本笔记的第一篇内容理所应当是我的R Markdown学习笔记。 "],["content.html", "内容组成", " 内容组成 显然，目前的内容安排已经远超当初的设想了，哈哈哈 2025.6.5 刘柯榉 在我的设想中，这篇笔记主要由专项、模型与方法、可视化、案例分享这五部分组成。 专项记录的是我对某一块知识点的学习，例如R Markdown、Manim。 模型与方法记录的是一些本人较为常用的模型或者统计方法，例如Dagum基尼系数分解法。 多源数据分析记录的是不同数据类型的常见分析方法，例如有网络型数据、文本数据、地理空间数据等等。 可视化算是我比较感兴趣的一个点，主要记录一些图或者视频动画的制作方法，例如桑基图等等。特别是视频动画，如果条件允许的话，我打算未来创建个人账号来发布视频。 案例分享则是上述内容的具体实现。在自学过程中，我非常感谢那些知识分享者的无私奉献，一篇好的文章确实能让人振奋不已。但可惜巧妇难为无米之炊，并不是每次都能找到合适的数据去上手。于是，我将在这部分提供数据来方便大家实操。 本人能力、精力有限，这篇笔记的定位就是个人“笔记”而非详尽的“教程”，仅仅记录本人的一些学习过程，难免有遗漏、错误之处，见谅！。 随着内容的增加，难免有知识上或排版上的错误，烦请各位指出 E-mail Address: 1019046689@qq.com "],["introduction.html", "个人简介", " 个人简介  From LNU经统 to BNU应统 欢迎报考辽宁大学！欢迎报考北京师范大学！  羽毛球爱好者 大腿是酸痛的、眼神是涣散的、小臂是无力的、步伐是凌乱的、上网是艰难的、轮转是盲目的、杀球是撒娇的…  LOL老年玩家（2012—2024） 小学，在课上背英雄技能，盼着周免英雄；初中，周末熬夜打游戏；高中，逐渐佛系；大学：进化为老年玩家，大乱斗常客。总结：啥都玩，啥都不会，想到玩啥就玩啥。 现在啊，也基本不玩了ORZ "],["rmd.html", "1 R Markdown", " 1 R Markdown 本部分内容为本人根据R Markdown: The Definitive Guide、R Markdown Cookbook、Pandoc手册（Markdown）、markdown官方教程、R Markdown入门教程及各路资料整理所得。个人建议按照 统计之都入门教程 &gt; Markdown教程 &gt; Guide = Cookbook &gt; Pandoc手册 的顺序进行阅读，其中Guide和Cookbook值得精读。 主要参考内容如下： R Markdown: The Definitive Guide R Markdown Cookbook Pandoc手册（Markdown） markdown官方教程 R Markdown入门教程 knitr_options knitr文档 kableExtra文档 如何编辑数学公式 "],["rmd_1.html", "1.1 安装与创建", " 1.1 安装与创建 首先需要下载rmarkdown包，可直接在RStudio中下载。 install.packages(&#39;rmarkdown&#39;) 如果你想将R Markdown文件输出为PDF文件，你需要额外安装TinyTeX包(https://yihui.org/tinytex/)，并进行下载，这可能需要等待一会儿。 install.packages(&#39;tinytex&#39;) tinytex::install_tinytex() 当需要的包下载完后，可按如下顺序创建R MarkDown文件：File -&gt; New File -&gt; R MarkDown。 在弹窗左侧是类型（文档、演示、shiny、其他模板），右侧则是更加细致的选项，如标题、作者、日期及输出格式，点击’OK’即可完成创建。注意R Markdown文件的后缀名为.Rmd。 图 1.1: 创建RMD文件 在完成创建后，可以在工作栏处找到’knit’（旁边有蓝色毛线圈的那个），点击即可输出为特定的文档。或者在控制台用函数rmarkdown::render()来输出，只要输入要导出的RMD文件及导出格式即可。 "],["rmd_2.html", "1.2 初识RMD", " 1.2 初识RMD R Markdown，顾名思义，是R和Markdown的组合。Markdown是一种标记语言，允许人们用易读易写的纯文本格式编写文档。而R Markdown则是Markdown的扩展，是一种结合了R代码（也可以结合其他语言的代码）、Markdown以及结果的强大工具，它允许用户在一个文档中同时编写文本、插入R代码并执行，然后将结果（如数据摘要、统计图表等）直接嵌入文档中。 R Markdown主要依靠knitr包和Pandoc来完成工作，其工作流如下所示。knitr是谢益辉大神开发的一个用R来优雅、灵活、快速地生成动态报告的包。Pandoc是一个文档转换工具，它可以将文档从一种格式转换为另一种格式，同时保留文档的格式和结构。 图 1.2: 工作流 RMD文档由三部分组成：元数据（metadata）、文本与代码。 元数据出现在文档开头，由’- - -‘隔开，用来控制标题、作者、日期、输出格式等基础信息。特别是输出格式这一部分，在后面的章节会再次提及。值得注意的是，元数据是用YAML的语法编写，在具体编辑的时候要注意缩进以及适当换行，并且使用’: ’来隔开键与值（注意空格）。如果你想了解更多，可查阅Pandoc手册。 在设置好元数据后，你就可以利用markdown语法来进行创作了。这一部分将在1.3节介绍。 代码可以分为两个部分。一个是内联（inline）代码，首尾用反引号（键盘左上角）来将代码括起来，即 `code`1 ，如果你想用R来运行内联代码的话，可以用`r code`。另一个是代码块（code chunk），首尾用三个反引号隔开（注意换行）或缩进四个空格来表示纯代码，如果你想用R来运行代码块的话，可以改为```{r} code ```2，注意{r}后面得换行。关于代码块的设置将会在1.3.7节详细介绍。 以上就是RMD的基本组成。接下来可以尝试着自己创建一个RMD文档并输出为HTML。 --- title: &quot;demo&quot; author: &quot;lkj&quot; date: &quot;`r format(Sys.time(), &quot;%Y/%m/%d&quot;)`&quot; output: html_document --- 利用`ggplot2`包绘制iris数据集 ```{r} library(ggplot2) ggplot(iris,aes(x=Sepal.Length,y=Sepal.Width,color=Species))+ geom_point() ``` iris数据集共有`r nrow(iris)`个观测值 你输出的结果应该如下所示。 图 1.3: 用ggplot2绘制图片 在RStudio中，RMD除了直接编辑代码的Source模式，还有类似Word这种所见即所得的Visual模式。在Visual模式中，你可以直接观察到RMD的效果，并且可以在工具栏处找到插入图片、表格、emoji等功能，更多的功能等待你自行探索。 图 1.4: Source模式与Visual模式 当你想在反引号内使用n个反引号时，需要在外部用n+1个反引号括起来，即n+1原则。这里code左右各有一个反引号，那么在外部左右应该各有两个反引号。↩︎ 事实上，这里涉及到如何展示r内联代码本身或者r代码块本身的问题，可以参考这里。↩︎ "],["rmd_3.html", "1.3 语法介绍", " 1.3 语法介绍 下面介绍个人认为较为常用语法，在内容上会有所缩减，若想了解更为全面的语法，建议优先查看markdown官方教程，再以其他教程或手册的内容作为补充。 1.3.1 标题 n个’#‘代表第n级标题，注意’#’后面跟空格。 # 一级标题 这是一级标题 ## 二级标题 这是二级标题 ### 三级标题 这是三级标题 图 1.5: 不同等级的标题 除了标题等级外，还可以为标题添加标识符，即# 标题{#标识符}。通过分配标识符，你就可以在文中进行交叉引用来跳转到对应的章节，只需在特定的地方使用[关键词](#标识符)即可。 特别地，你可以使用{-}或{.unnumbered}来表示不给该标题编号。这些未编号的标题主要为那些非主体部分的章节，例如前言、参考文献等等。如果你既不想为标题编号，又不想其出现在目录中，你可以设置{.unlisted .unnumbered}。 ### 鸢尾花 {#iris} 详见[鸢尾花](#iris) 图 1.6: 标题标识符 1.3.2 字体样式 斜体：_text_ 或 *text* 粗体：**text** 斜体+粗体：***text***或___text___ 斜体+粗体（强调内部）：t***ex***t 下标：~text~ 上标：^text^ 删除线：~~text~~ 图 1.7: 字体样式 1.3.3 换行 平时的换行习惯（直接按回车键）是不起作用的。你可以在第一段后面加两个及以上空格后再按回车键。或者按两下回车键，即RMD中段落与段落之间存在一个空行。注意若没有换行在有些时候将会影响显示的结果。 # 法一 第一行。空格空格 第二行 # 法二 第一行 第二行 1.3.4 链接 超链接就是[text](link)的形式。图片链接（其实就是插入图片）则是![alt text or image title](path)，注意多了个!。 无论是超链接还是图片链接，都可以设置悬浮文本（就是鼠标放上去会有文本显示），悬浮文本'text'放在link或者path后面，中间空格相隔即可。 此外，你还可以设置图片超链接（即点击图片进行跳转），只要把图片链接的形式放到超链接中的text处即可。 如果你直接在文本中写入URL，那么他会自动转为超链接。 超链接：[谢益辉](https://yihui.org/) 超链接+悬浮文本：[谢益辉](https://yihui.org/ &#39;博客&#39;) 超链接+斜体：*[谢益辉](https://yihui.org/)* 超链接+粗体：**[谢益辉](https://yihui.org/)** 图片链接（插入图片）+悬浮文本： ![《图片》](./pic/p7_1.png &#39;p7&#39;) 图片超链接： [![《图片》](./pic/p7_1.png &#39;p7&#39;)](https://yihui.org/) URL：https://yihui.org/ 图 1.8: 链接 1.3.5 引用 块引用，跟在’&gt;‘后面的文本即为块引用，注意换行时也需要有’&gt;‘。嵌套块引用则跟在两个’&gt;’后面。 脚注，第一种写法是text^[footnote]，第二种写法是text[^1]并隔开一行[^1]:footnote。实际上，第二种写法的脚注[^1]:footnote可以放在除块内（列表、块引用等）以外的任何地方，并且可以使用除数字外的其他唯一标识来区分，最终脚注会按先后顺序自动排序。 文本 &gt; 这是块引用 &gt; &gt; 这也是块引用 &gt; &gt;&gt; 这是嵌套块引用 文本^[这是脚注] 文本[^1] [^1]: 这也是脚注 图 1.9: 块引用与脚注 1.3.6 列表 无序列表：* text或- text或+ text。注意空格。另起一行缩进两个或两个以上空格表示子列表。 有序列表：1. text、2. text等等。另起一行缩进三个或三个以上空格表示子列表。 列表嵌套段落/块引用/代码块/图片：上下空行，前面缩进三个空格。 任务列表：- [x] text。其中x表示是否打钩，注意空格。任务列表具有交互性。 定义列表：首行定义术语，另起一行冒号+一个空格+定义。 列表需与周围内容用空行隔开。 无序列表： - 项目1 - 子项目1 - 项目2 有序列表： 1. 项目1 2. 项目2 1. 项目2_1 列表嵌套段落（其他同）: - 项目1 段落 - 项目2 任务列表： - [x] 任务1 - [ ] 任务2 定义列表： 项目一 : 定义1 项目二 : 定义2 图 1.10: 列表 1.3.7 代码 内联代码和代码块的基本介绍已经在1.2节有所提及。接下来介绍r代码块的具体设置。 knitr包使得R能够和Markdown有机结合在一起。在knitr包中提供了丰富的参数选项，这里仅介绍较为常见的一些设置，完整内容请参考官网。 代码块的设置可以写在{r}中（必须写在同一行），形如{r label, tag=value}。也可以写在代码块内的#|后面，如下所示。 ```{r} #| label #| tag=value code ``` label 设置该代码块的标签（注意是在r的空格后面，并且label不用带引号）。也可以同tag=value一样进行设置，此时形如{r, label='text', tag=value}，注意有逗号。注意名称中不要包含空格、下划线和点，路径也是。 eval 是否运行代码块，默认为TRUE。可以输入数值型向量，例如c(1,3)表示运行第1、3行代码，-c(1,3)表示运行除了第1、3行以外的所有代码。 echo 是否呈现源代码，默认为TRUE。可以输入数值型向量，例如1:5表示呈现第1至5行代码，-1表示呈现除了第1行以外的所有源代码。 results 用来控制如何呈现文本输出结果，默认为'markup'，字符串。可选值为'markup'、'asis'、'hold'、'hide'。 markup 根据输出格式，用适当的环境来标记文本输出。例如在HTML中，输出结果就是被包含在框框里面。 # results=&#39;markup&#39; cat(&#39;被包在围栏里\\n&#39;) ## 被包在围栏里 print(&#39;被包在围栏里&#39;) ## [1] &quot;被包在围栏里&quot; asis 对输出结果不进行任何标记，该怎样就怎样。 # results=&#39;asis&#39; cat(&#39;该怎样就怎样\\n&#39;) 该怎样就怎样 print(&#39;该怎样就怎样&#39;) [1] “该怎样就怎样” hold 等所有代码呈现完之后再统一呈现结果。 # results=&#39;hold&#39; cat(&#39;统一呈现结果\\n&#39;) print(&#39;统一呈现结果&#39;) ## 统一呈现结果 ## [1] &quot;统一呈现结果&quot; hide 隐藏文本输出结果。'hide'等价于FALSE表示。 # results=&#39;hide&#39; cat(&#39;你看不到我\\n&#39;) print(&#39;你看不到我&#39;) collapse 如果可以的话，是否将源代码和输出结果放在同一个块中，默认为FALSE。 一般的输出结果就和results='markup'的示例一样 # collapse=TRUE cat(&#39;在同一个块中\\n&#39;) ## 在同一个块中 print(&#39;在同一个块中\\n&#39;) ## [1] &quot;在同一个块中\\n&quot; include 是否将源代码和结果显示出来，默认为TRUE。 warning 是否在输出结果中显示警告信息，默认为TRUE。若为FALSE，则不显示；若为NA，则警告信息将会被打印在控制台。 message 是否在输出结果中显示提示信息，默认为TRUE。 error 是否在遇到错误时继续运行后面的代码，默认为FALSE。若为FALSE，则遇到错误就停止，并且错误也会显示在控制台里。若为TRUE，则即使遇到错误也会继续运行后面的代码。除了TRUE和FALSE选项，还可以输入数字0、1、2。0如同TRUE，2如同FALSE，而1则表示在遇到错误时停止，但不会再控制台里报错。 tidy 是否格式化输出R代码，默认为FALSE。若为TRUE，则使用formatR包（得提前下载）的tidy_source()函数来对R代码进行格式化处理，也就是用formatR包的风格来呈现源代码。 # tidy=FALSE if(TRUE){1} else {2} # tidy=TRUE if (TRUE) { 1 } else { 2 } tidy.opts 为tidy传递参数，列表。 prompt 是否为R代码添加提示符，默认为FALSE。若为TRUE，则会在行首添加’&gt;‘和’+’符号。 &gt; # prompt=TRUE &gt; if (TRUE) { + 1 + } else { + 2 + } comment 用以控制输出结果的前缀，默认为’##‘。你可以用comment=''来将’##’去掉。 # comment=&#39;##&#39; if(TRUE){1} else {2} ## [1] 1 # comment=&#39;&#39; if(TRUE){1} else {2} [1] 1 highlight 是否高亮语法，默认为TRUE。 cache 是否对该代码块进行缓存，默认为FALSE。对于缓存的代码块，将会生成一个文件夹来存储信息。在第二次使用时将会跳过该代码块，而从文件夹中提取信息，除非该代码块有所改动。 1.3.8 表格 你可以在Visual模式中，找到工具栏里的’Insert’选项来插入表格。 你也可以使用Markdown语法来创建表格。用三个及以上的’-‘来创建标题，并用’|‘隔开每列，向左右两侧添加’:’来实现对齐（示例中依次是左对齐、居中、右对齐）。 |col1|col2|col3| |:---|:---:|---:| |text|text|text| 而knitr包提供了kable()函数来较为方便地控制表格。kable()能够将矩阵或数据框转化为表格，但仅能在HTML、PDF、Word中使用。注意kable()无法精细地设置单元格样式或合并单元格。 kable(x, format, digits = getOption(&quot;digits&quot;), row.names = NA, col.names = NA, align, caption = NULL, label = NULL, format.args = list(), escape = TRUE, ...) x 矩阵或数据框。 format 设置表格样式，字符串。可选值有'pipe'、'simple'、'latex'、'html'、'rst'、'jira'、'org'。注意可选值应当与文档的输出格式相匹配，'pipe'和'simple'适用于所有输出格式，RMD文件默认使用'pipe'。如果你只需要一种非默认的表格样式，你可以在全局设置中设置表格样式，如options(knitr.table.format = 'html')。 kable(iris[1:2,], format=&#39;pipe&#39;) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa kable(iris[1:2,], format=&#39;simple&#39;) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa digits 保留小数点位数，数值或数值向量。若为数值向量，则根据数值向量长度从第一列开始对应过去，例如表格有5列，数字向量长度为3，则仅对前三列生效。若遇到字符串型的列，可以在数值向量中设置0或NA。 format.args 为format()传递参数来格式化数值，列表。例如可以对数值采用科学计数法，或者每隔三位用’,’隔开。 df = data.frame(x = c(12345,12138),y = c(13548987,13548581)) kable(df, format.args=list(scientific=TRUE)) x y 1.2345e+04 1.354899e+07 1.2138e+04 1.354858e+07 df = data.frame(x = c(12345, 12138),y = c(13548987, 13548581)) kable(df, format.args = list(big.mark = &#39;,&#39;)) x y 12,345 13,548,987 12,138 13,548,581 row.names 是否包含行名，默认为TRUE。默认情况下，如果行名既不是NULL也不等于1:nrow(x)，则包含行名。 col.names 表格的列名，字符串向量。 align 对齐方式，字符串或字符串向量。可选值为'l'、'c'、'r'，分别为左对齐、居中对齐、右对齐。默认为数值型列右对齐，其余列左对齐。除了设置字符串向量来确定每一列的对齐方式，你还可以输入一个多字符的字符串，例如align='lcrr'等价于align=c('l','c','r','r')。 caption 设置表格标题，字符串。注意kable()并不能让表格标题居中，你需要求助别的包（如kableExtra包）或者在HTML中自定义样式。 label 设置表格的索引标签，字符串。若不想设置标签，则label = NA。 escape 当输出HTML或Latex的表格时，是否对特殊字符进行转义，默认为TRUE。 有时表格中会出现缺失值，默认用NA展示。你可以在全局设置中设置缺失值的替换值，如options(knitr.kable.NA = '-') df = data.frame(x = c(123, NA),y = c(NA, 15)) options(knitr.kable.NA = &#39;—&#39;) kable(df) x y 123 — — 15 如果你想并排放置两个表格，那么你可以使用kables()（仅适用于HTML及PDF），该函数的x参数接收列表，列表中的每个元素均为kable()对象，其余参数同kable()。 kables( list( kable(iris[1:2,1:2], caption = &#39;鸢尾花&#39;), kable(mtcars[1:3,1:3], caption = &#39;车车&#39;) ), caption = &#39;两个表格并排&#39; ) 表 1.1: 两个表格并排 表 1.1: 鸢尾花 Sepal.Length Sepal.Width 5.1 3.5 4.9 3.0 表 1.1: 车车 mpg cyl disp Mazda RX4 21.0 6 160 Mazda RX4 Wag 21.0 6 160 Datsun 710 22.8 4 108 如果你想在for循环中生成表格，那么你可以采取如下的办法。注意results='asis'、print()是必需的。末尾的cat('\\n')是为了更好地区分打印出来的表格，否则有些时候就会弄混。 for (i in 1:2) { print(knitr::kable(iris[1:2,])) cat(&#39;\\n&#39;) } Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa # 如果没有results=&#39;asis&#39;、print() # 直接就打印不出来了 for (i in 1:2) { knitr::kable(iris[1:2,]) cat(&#39;\\n&#39;) } 虽然kable()相较Markdown语法创建表格更为方便，但仍有不足之处。kableExtra包则进一步拓展了kable()，大家可翻阅相关资料增进了解。 1.3.9 图片 在1.3.4节已经介绍了如何插入图片以及如何为图片设置超链接。这里介绍knitr包对图片的设置。 knitr包支持两种方式生成图片：一是由R绘制得到，二是从本地插入。前者是在r代码块中输入绘图代码即可完成，例如图1.2。后者通过为knitr::include_graphics()函数提供图片路径即可从本地插入图片。同代码块一样，二者都可以在{r, tag=value}中对图片进行设置。 fig.width/fig.height 图像的宽/高，单位为英寸，默认均为7。 fig.dim 是fig.width和fig.height的简写，即fig.dim=c(4,5)等价于fig.width=4和fig.height=5。 fig.asp 图像的高度除以宽度，即高宽比。当设置好高度（宽度）时，可根据高宽比来控制对应的宽度（高度）。 out.width/out.height 不同于fig.width和fig.height设置图像本身的宽度与高度，out.width和out.height控制图像在输出文档中的宽度与高度。对于不同的输出格式，可取不同的值。例如对于HTML，可以将图片的宽度设置为out.width='300px'，也就是宽为300像素。或者有out.width='75%'，即图片宽度占页面宽度的75%。图1.1就是out.width='75%'，你可以对该图点击右键，选择’检查’，即可在页面源码处看到该参数。 out.extra 为特定的输出文档传入额外的参数，字符串。例如在HTML中，out.extra的值将会被传入到&lt;img&gt;标签中，如out.extra='style=\"border:5px solid orange;\"'（css的内联样式）将会为图片添加宽为5像素的橙色边框。 dpi 每英寸的点数，默认为72。dpi越大，图像越清晰（dpi x 英寸 = 像素）。 fig.align 对齐方式，可选值为'default'、'left'、' right'、'center'，默认为'default'，表示不会做任何对齐调整。 注：该选项不适用word输出。 fig.cap 图片标题。 fig.link 为图片添加的超链接，字符串。 fig.alt 在HTML输出中，为图片添加alt属性，字符串。当图片无法正常显示时，将显示该段文本描述。如果提供了fig.cap，则其赋值给fig.alt。 fig.env LaTex中插图的类型，默认为'figure'。 fig.scap LaTex中的短标题。 fig.lp 图片的标签前缀，默认为'fig:'。实际上，图片的真实标签由该前缀和代码块的标签组合而成。图片的完整标签可用于索引。(不建议修改，可能会影响到交叉引用) fig.id 在HTML输出中，是否为图片在&lt;img&gt;标签中添加唯一标识的id属性，默认为NULL。默认id值将由fig.lp、chunk label、fig.cur组成。其中fig.cur是隐藏的选项，表示图片当前的次序号。 fig.pos 用于\\begin{figure}[]的图片位置参数，可选值为'H'（放置在当前位置，即尽可能靠近代码块）、't'（放置在页面顶部）、'b'（放置在页面底部）、'p'（放置在单独一面）、'!H'（强制放置在当前位置），默认为''。（只针对pdf） fig.path 为生成的图片设置存储路径，默认为'figure/'，即与当前RMD文件同目录的figure文件夹里。代码块的标签label及生成的次序将会成为图片的存储名称。完整路径形如'figure/label-1.png'。 fig.keep 哪些图片需要被展示，可选值为'high'、'none'、'all'、'first'、'last'及数值向量，默认为'high'。 high 仅展示高级绘图命令（例如产生新图的命令）的结果，低级绘图命令（例如增添细节的命令）会被作用到已有的图片之中。 # fig.keep = &#39;high&#39; # 高级绘图命令，如plot() plot(1) # 低级绘图命令，如abline() abline(0, 1) abline(1,-1) none 不展示任何图片。 # fig.keep = &#39;none&#39; # 高级绘图命令，如plot() plot(1) # 低级绘图命令，如abline() abline(0, 1) abline(1,-1) all 展示所有图片，低级绘图命令会另外产生新的图片。 # fig.keep = &#39;all&#39; # 高级绘图命令，如plot() plot(1) # 低级绘图命令，如abline() abline(0, 1) abline(1,-1) first 只展示第一幅图。 # fig.keep = &#39;first&#39; # 高级绘图命令，如plot() plot(1) # 低级绘图命令，如abline() abline(0, 1) abline(1,-1) last 只展示最后一幅图。 # fig.keep = &#39;last&#39; # 高级绘图命令，如plot() plot(1) # 低级绘图命令，如abline() abline(0, 1) abline(1,-1) 数值向量 以数值向量为索引展示对应的图片。 # fig.keep = c(2) # 高级绘图命令，如plot() plot(1) # 低级绘图命令，如abline() abline(0, 1) abline(1,-1) fig.show 如何呈现图片，可选值为'asis'、'hold'、'animate'、'hide'，默认为'asis'。 asis 图片跟在对应的代码后面。 # fig.show = &#39;asis&#39; plot(1) abline(0,1) plot(1) abline(1,-1) hold 所有图片等代码块运行完后再出现。 # fig.show = &#39;hold&#39; plot(1) abline(0,1) plot(1) abline(1,-1) animate 以动画的形式展示图片，详见animation.hook。 # fig.show = &#39;animate&#39; # animation.hook = &#39;gifski&#39; plot(1) abline(0,1) plot(1) abline(1,-1) hide 并不展示图片。 # fig.show = &#39;hide&#39; plot(1) abline(0,1) plot(1) abline(1,-1) animation.hook 在HTML中用何种方式生成动画，可选值为'ffmpeg'、'gifski'，默认为`‘ffmpeg’。前者生成WebM视频，后者生成gif动图。 在选择'ffmpeg'时，系统报错Could not find ffmpeg command.You should either change the animation.fun hook option or install ffmpeg with libvpx enabled.，需要配置相应的环境。由于之前在下载gganimate包时同时安装了gifski包，所以选择'gifski'能够正常生成动图。 个人观点：如果想表达这种具有变化性特征的一系列图片或者数据，gif动图或者其他包（如shiny，gganimate）已经能较好地满足需求了。如果想要以视频的形式呈现的话，可以尝试利用HTML语法来插入视频。 interval 帧之间的间隔秒数，默认为1。 dev 图形设备，默认LaTex为'pdf'，HTML/Markdown为'png'。图像大小总是以英寸（inches）为单位。代码块选项dev、fig.ext、fig.width、fig.height、dpi的值可以为向量，表示对同一内容生成不同规格的图。 dev.args 以列表的形式为特定的dev传入更为详细的参数。如果dev为向量，则为嵌套列表。 fig.ext 文件扩展名。如果你为同一内容的同一图形设备（不同图形设备自然有不同后缀名）生成多种不同规格的图片，那么新生成的图片将会覆盖掉旧的图片。此时，你需要为其添加不同的扩展名。例如dev=c('png'),fig.width = c(10, 6)仅会生成一幅图，而dev=c('png'), fig.width = c(10, 6), fig.ext=c('1.png', '2.png')将会生成两幅扩展名分别为’1.png’和’2.png’的图片。 1.3.10 数学公式 关于如何编辑数学公式大家可以参考这篇博客。 "],["rmd_4.html", "1.4 文档元素", " 1.4 文档元素 1.4.1 分割线 三个及以上的*或-或_，注意需用空格与周边内容隔开。 text ----- text ----- text 1.4.2 转义符 对于特殊字符，可以在其前面添加\\进行转义。例如\\_text\\_将会输出为’_text_’而非斜体。又如markdown中多个空格将会被视为一个空格，你也可以在每个空格前添加反斜杠使其保留下来。 1.4.3 分页符 添加\\newpage即可，注意上下要空行。 （Cookbook中说是HTML输出也能起作用，但貌似不太行） It is a LaTeX command, but the rmarkdown package is able to recognize it for both LaTeX output formats and a few non-LaTeX output formats including HTML, Word, and ODT. 1.4.4 设置动态标题 在元数据的title中，你可以添加内联代码来实现动态输入信息的效果。 --- title: &quot;基于`r nrow(iris)`条鸢尾花数据的分析&quot; --- 当标题所需的动态元素在正文中产生时，你可以不用一开始就在元数据处设置title，直到你所需的元素出现即可。 --- author: lkj date: 2024/7/17 output: bookdown::html_document2 --- 这里是正文 ```{r} x &lt;- runif(1, min=1, max=100) ``` --- title: &quot;震惊！`r round(x, 2)`%的人竟干过这种事！&quot; --- 这里是正文 1.4.5 自动更新时间 和上一节的设置一样，只需在元数据的date处设置相应的内联代码即可。 --- date: &quot;`r format(Sys.time(),&#39;%Y-%m-%d&#39;)`&quot;, --- 1.4.6 获取元数据信息 当rmarkdown文件完成编译时，所有的元数据信息将会被存储在rmarkdown::metadata这一列表对象当中。你可以在r代码块中提取该列表包含的元数据信息，例如rmarkdown::metadata$title表示获取元数据中的标题信息。 1.4.7 参考文献 要想在文末添加参考文献，可以在元数据处写入bibliography，并在后面添加你的.bib文件。.bib文件是用于管理参考文献的文件，不同类型的参考文献有不同的格式要求，可参考知乎的这篇文章。 有些时候，你可能需要不同格式的参考文献，这就得依靠.csl文件来设置参考文献的格式。你需要在元数据处的csl声明你用的.csl文件。你可以在zotero查询你想要的格式并下载对应的.csl文件，或者在这里自定义格式（我还没试过）。 下面介绍具体使用方法： 打开zotero，以’china’（或者’chinese’）为关键词进行搜索，点击相应的链接即可下载对应的.csl文件。这里下载’china’关键词下的China National Standard GB/T 7714-2015 (numeric, 中文)文件。 将.csl文件放在与.Rmd文件相同的目录中。（例如.css文件也是得和.Rmd文件同目录） 在文献资源平台上找到需要的参考文献，并导出Bibtex引用格式。这里以“万方”为例。 图 1.11: 点击引用 图 1.12: 点击Bibtex 将下载来的文件放到.Rmd文件所在的目录中。我下载过来的文件是.txt格式，如果有其他文献，那么可以下载过来后粘贴到同一个.txt文件中，并将其重命名为reference.bib，之后便可在元数据处写入bibliography: reference.bib及csl: china-national-standard-gb-t-7714-2015-numeric.csl。 @article{姜玲, author={姜玲 and 张爱宁}, title={ 中国省际循环经济发展差异的空间计量分析 }, organization={甘肃省科学技术情报研究所 and 甘肃省科学技术情报研究所}, journal={资源开发与市场}, year={2014}, volume={30}, number={1}, pages={32-34,44}, month={1}, } @article{张红梅, author={张红梅 and 李黎力}, title={ 国内大循环的供给网络 ——基于省际流出增加值数据的考察 }, organization={北京语言大学 and 中国人民大学}, journal={学习与探索}, year={2021}, volume={}, number={4}, pages={111-119}, month={4}, } 在正文处使用[@key]即可引用对应的文献，其中key就是第一个逗号前的内容（如果太长则可以任意修改，只要是唯一的就行）。 如果有些文献在文中没有直接引用，则默认不显示这些参考文献。除非你在元数据的notice处声明了'@key'，不同文献之间用逗号隔开。或者直接notice: '@*'表示显示所有的参考文献。 --- title: &quot;示例&quot; output: html_document bibliography: reference.bib csl: china-national-standard-gb-t-7714-2015-numeric.csl notice: &#39;@张红梅&#39; --- 姜玲等学者[@姜玲]在其文章中指出，balabala... # 参考文献 图 1.13: 参考文献示例 事实上，如果你经常阅读文献，那么下载专门的文献管理工具将会是更好的选择，例如Zotero。 当生成完参考文献后，其默认会被放在文档的末端。如果要改变其位置，例如你还需在参考文献后添加附录，你可以使用&lt;div id=\"refs\"&gt;&lt;/div&gt;来控制参考文献的位置，该html语句在哪参考文献就在哪。这条语句在其他文档输出中也有效，例如PDF。 1.4.8 交叉引用 交叉引用允许你在文档中能够跳转到目标章节、图、表、公式等等。 首先实现章节的跳转，有如下三种方法： [目标标题] [文本][目标标题] [文本](#目标标题的标识符) 而其余元素的交叉引用需要满足相应的条件才能实现： 使用bookdown的文件输出格式3，例如从一般的output: html_document变为output: bookdown::html_document2，同理，还有pdf_document2、word_document2 图表需要有标题。在bookdown下的输出文档，有标题的图表才能被编号。没有编号是不能交叉引用的。 相应的代码块或公式也需要有标签。代码块的标签详见第1.3.7节，公式的标签只要在输入的公式后面跟上(\\#eq:label)即可。 满足条件之后，就可以按\\@ref(type:label)的形式进行交叉引用，其中type可替换为fig、tab、eq等，分别代表图、表、公式，label就是代码块或者公式的标签。例如我这里输入\\@ref(fig:rmd-p13)就会出现1.13。这个只是图片的编号，所以我真实的输入会是图\\@ref(fig:rmd-p13)，即图1.13。 既然提到了bookdown有关的输出文档，还是说一下，关于bookdown的配置文件有不少，例如_output.yml、_bookdown.yml，我没精力去详细介绍，因为这又是一个填不完的坑，建议你查询官方文档。你可以在.Rmd文件（例如html_document2类型的输出格式）的同目录中添加_bookdown.yml文件，在该文件中输入下述内容（注意缩进），就能够使图表标题的前缀从字母fig或tab变成你自定义的文本。 language:   label:     fig: “图”     tab: “表” 1.4.9 多位作者 如果一篇文档有多位作者的话，那么可以直接以字符串的形式输入，例如author: '小明，小红，小刚'（有无引号均可）。或许你想为每位作者附加一些基本信息，那么你可以尝试无序列表及脚注，如下所示： # 无序列表，每位作者单独一行 author: - 小明，基本信息 - 小红，基本信息 # 脚注，在脚注处标明额外信息 author: &#39;小明^[基本信息]， 小红^[基本信息]&#39; # 使用脚注时注意和上标作区分，逗号后面空格隔开即可 1.4.10 将模型输出为公式 equatiomatic包的extract_eq能够将拟合的模型输出为公式，如下所示: model &lt;- lm(Sepal.Length~Sepal.Width, data=iris) equatiomatic::extract_eq(model) # 提取公式 \\[ \\operatorname{Sepal.Length} = \\alpha + \\beta_{1}(\\operatorname{Sepal.Width}) + \\epsilon \\] equatiomatic::extract_eq(model, use_coefs=TRUE) # 提取具体数值的公式 \\[ \\operatorname{\\widehat{Sepal.Length}} = 6.53 - 0.22(\\operatorname{Sepal.Width}) \\] 如何为提取出来的公式编号尚不清楚，如果不能的话还是手动输入吧 1.4.11 流程图 DiagrammeR包利用graphviz或mermaid来绘制流程图，对应的函数分别为grViz和mermaid。两个函数都得以字符串的形式来接收对应风格的绘图语言。关于选择何种函数就见仁见智了，二者示例如下： DiagrammeR::grViz(&quot;digraph { graph [layout = dot, rankdir = LR] node [shape = box] rec1 [label = &#39;Step 1 打开冰箱&#39;] rec2 [label = &#39;Step 2 塞入大象&#39;] rec3 [label = &#39;Step 3 关上冰箱&#39;] rec1 -&gt; rec2 -&gt; rec3 }&quot;, width=400, height=100) DiagrammeR::mermaid(&#39; graph LR node_1[Step 1 打开冰箱] --&gt; node_2[Step 2 塞入大象] node_2[Step 2 塞入大象] --&gt; node_3[Step 3 关上冰箱] &#39;, width=400, height=100) 相关语法介绍可详见graphviz官网和mermaid官网（有中英文官网，这个是中文的）。 mermaid官网还介绍了其他类型的图形，例如甘特图、桑基图。 1.4.12 注释 在编辑.Rmd文件时，如果需要注释，可以使用HTML的注释语法&lt;!--注释--&gt;（注释内容可跨行）。也可以选中要注释的内容，使用快捷键Ctrl + Shift + C进行注释。 1.4.13 缩进 在markdown中，如果你想用空格来表示缩进，这将会是徒劳的。在开头使用’| ‘后即可正常保留空格（四个空格对应一个汉字符）及之后的换行符。注意’| ’后面跟了个空格。 &gt; | 昨天是你 &gt; | 今天是我 &gt; | 明天又是谁     昨天是你 今天是我     明天又是谁 1.4.14 字体颜色 对于HTML输出，可以用形如&lt;span style='color: red'&gt;text&lt;/span&gt;的方式来对text设置字体颜色。 对于PDF输出，可以用形如\\textcolor{red}{text}的方式来对text设置字体颜色。 甚至可以使用自定义函数来格式化输出文本。你只需用内联代码的形式输入该自定义函数及目标参数即可。 colorize &lt;- function(x, color) { sprintf(&quot;&lt;span style=&#39;color: %s;&#39;&gt;%s&lt;/span&gt;&quot;, color, x) } 所以我是打算就直接用bookdown下面的文件输出格式了，功能会比一般的文件输出格式要多↩︎ "],["rmd_5.html", "1.5 HTML文档", " 1.5 HTML文档 rmarkdown中兼容HTML语法，这里仅补充Markdown语法未涉及到的部分。 如何查看网页元素：在一个HTML页面中，单击右键后选择’检查’（或者直接按’F12’），然后就会跳出网页元素界面。在单击左上角的按钮后，你就可以移到网页元素上查看相应的源码。 图 1.14: 查询网页元素 1.5.1 标签 超链接标签 在rmarkdown中写入[text](link)默认为&lt;a href=\"link\"&gt;text&lt;/a&gt;，其中href表示超链接。若你想让超链接在新标签页打开，有如下三种方法： 直接在&lt;a&gt;标签中修改 &lt;a href=\"link\" target=\"_blank\"&gt;text&lt;/a&gt;，其中target=\"_blank\"表示跳转方式为在新标签页打开。这只针对一个超链接。 添加&lt;base target=\"_blank\"&gt; 在正文开头处添加&lt;base target=\"_blank\"&gt;，即可让该HTML页面中的所有超链接均在新标签页中打开。 lua过滤器 你现在看到的这本笔记是由bookdown将一个个HTML文档组合而成，所以我采用了lua过滤器，使得Pandoc在编译文档时能够自行为所有超链接添加target=\"_blank\"属性。请参考下述lua文件（新建txt文件，扩展名改成’.lua’即可）。 function Link(el) if el.tag == &#39;Link&#39; then el.attributes.target = &quot;_blank&quot; end return el end return { {Link = Link} } 同时你需要在YAML中声明。 # 对于gitbook就是在_output.yml中 bookdown::gitbook: pandoc_args: [&quot;--lua-filter=/path/to/your/lua_filter.lua&quot;] # 对于普通rmarkdown文件就是在开头的YAML中 output: html_document： pandoc_args: [&quot;--lua-filter=/path/to/your/lua_filter.lua&quot;] 关于lua过滤器详情可参考Pandoc Lua Filters 除了添加网页的超链接外，还能提供本地文件的下载链接，形如&lt;a href='path/to/your/file.zip'&gt;下载文件&lt;/a&gt; 下划线标签 &lt;u&gt;text&lt;/u&gt; 1.5.2 目录 在HTML输出中，你可以在元数据处声明添加目录（table of contents，toc）及相关设置。 注意布尔值得用小写，且冒号后面得有空格 toc: true/false 是否生成目录。 toc_float: true/false 是否设置悬浮目录。若为true，则目录会在页面的左侧一栏，跟随页面移动；若为false，则目录将会固定在文档开头。 还可以为其添加更为细致参数，collapsed和smooth_scroll。 collapsed: true/false 默认为true。若为true，则目录将会被折叠；若为false，则目录会被完全展开。 smooth_scroll: true/false 默认为true。当你点击目录时，若为true，则会丝滑地滚动到目标位置；若为false，则会生硬地跳转到目标位置。 toc_depth: int 整数，设置目录深度。例如若为3，则最多只显示到三级标题。 --- title: &#39;TOC&#39; output: html_document: toc: true toc_depth: 3 toc_float: collapsed: true smooth_scroll: true --- 1.5.3 外观 和目录一样，你可以在元数据处声明主题、语法高亮及CSS样式。 theme 控制文档的主题。除了默认default外，其余可选值为bootstrap、cerulean、cosmo、darkly、flatly、journal、lumen、paper、readable、sandstone、simplex、spacelab、united、yeti，还可以传入null不使用任何主题。 个人青睐default。 --- title: &#39;theme&#39; output: html_document: theme: default --- highlight 控制语法高亮的样式。除了默认default外，其余可选值为tango、pygments、kate、monochrome、espresso、zenburn、haddock、breezedark、textmate，还可以传入null不使用任何语法高亮。 个人青睐textmate。 --- title: &#39;theme&#39; output: html_document: highlight: textmate --- css css样式既可以在主题和语法高亮设置好之后对特定部分起作用，也可以另辟蹊径（为theme和highlight传入null），即完全的自定义。 引入css样式有如下几种方法： 内联样式，即直接添加在HTML的标签里，如&lt;h1 style='text-align:center'&gt;一级标题&lt;/h1&gt;'&gt;表示居中的一级标题。 内部样式，在正文开头处将自定义样式写在&lt;style&gt;自定义样式&lt;/style&gt;标签内。 外部引入，在元数据处即可声明引入外部的.css文件（一般名为’style.css’）。第三种方法便于管理，因此较为常用。 --- title: &#39;css_style&#39; output: html_document: css: style.css --- 嵌入代码块，即将代码块的引擎更换为css，并在内部定义css样式。 ```{css, echo=FALSE} p.caption { color: #777; margin-top: 10px; } ``` 你可以在网页源码中找到文本、图片、表格等元素所处的标签，然后通过css样式来对其进行自定义。网上关于css样式的文章还是挺多的，例如你可以在CSDN里查询相关资料，这里就不再赘述了。 1.5.4 选项卡 # 标题{.tabset}会将其所有的子章节转变为选项卡，你可以在不同选项卡之间切换。在.tabset后面，你可以接着添加属性.tabset-pills和属性.tabset-fade。前者会让选项卡的背景变为深蓝色，后者会让选项卡的内容渐入（注意属性之间得用空格隔开）。一般情况下，会默认显示第一个选项卡。如果你想优先显示某一个选项卡，那么你可以在对应的子章节标题后面添加’{.active}’。当你结束编辑选项卡时，需要输入与上级标题同样多的#与{-} ### 上级标题 {.tabset .tabset-pills .tabset-fade} #### 选项卡1 正文 #### 选项卡2 表格 #### 选项卡3 {.active} 图片 ### {-} 图 1.15: 选项卡 1.5.5 表格 你可以在元数据处用df_print声明用哪种方式呈现表格，可选值有default、kable、tibble、paged和自定义函数，其中default为data.frame类型的表格。这些可选值本质就是调用相应的函数来打印表格。 --- title: &#39;df_print&#39; output: html_document: df_print: kable --- df_print: default 等价于调用data.frame()函数来输出表格。 # df_print: default data.frame(head(iris)) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa df_print: kable 等价于调用knitr::kable()函数来输出表格。 # df_print: kable knitr::kable(head(iris)) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa df_print: tibble 等价于调用tibble::tibble()函数来输出表格。 # df_print: tibble tibble::tibble(head(iris)) ## # A tibble: 6 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa df_print: paged 等价于调用rmarkdown::paged_table()函数来输出表格。 # df_print: paged rmarkdown::paged_table(iris) df_print: paged还可以同代码块一样在{r}中添加额外的参数来进行精细化输出。相应的参数如下： 选项 用途 默认值 max.print 打印行数的上限 1000 rows.print 每页展示几行 10 cols.print 每页展示几列 10 cols.min.print 每页至少展示几列 - pages.print 表格下方有几个索引页数 - paged.print 是否使用分页表格 TRUE rownames.print 是否要打印行名 TRUE 个人感觉，从观感上来看，首先就可以排除掉default和tibble了，剩下kable和paged。我试过了，kable不能像paged一样在{r}中添加额外的参数，加之df_print的本质是调用函数，所以我打算放弃掉df_print这个选项，直接在打印表格时用knitr::kable()或者rmarkdown::paged_table()来进行精细化输出。 1.5.6 代码块及其输出 1.5.6.1 代码块输出代码块 有时会遇到输出```{r} code ```的需求，这就需要用如下的代码块进行包裹。注意最外层的反引号数量总是比内部的反引号数量多。 ````{verbatim} ```{r} code ``` ```` 1.5.6.2 样式 代码块及其输出的样式可以在{r}中进行设置。其中，class.source用于控制代码块的样式，class.output用于控制文本输出的样式。部分可选样式如下：\"bg-primary\"、\"bg-success\"、\"bg-info\"、\"bg-warning\"、\"bg-danger\"。 或者，你也可以自定义CSS样式（如何引入css样式详见1.5.3节），为代码块或输出分配新的类名。 图 1.16: 自定义代码块及其输出样式 在代码块的设置中，echo允许在文档中是否呈现源码。但有些时候，你可能一开始需要隐藏代码，而在有需要的时候再显现出来。这就得依靠在元数据处的code_folding选项。当code_folding: show时，会显示所有的代码；当code_folding: hide时，会隐藏所有的代码。无论是显示还是隐藏，在文档开头处以及各个代码块旁边都会提供选项，来让你手动选择是’显示’还是’隐藏’代码。 --- title: &#39;code_folding&#39; output: html_document: code_folding: hide --- 当然，code_folding选项是对所有代码块而言的，你可以针对性地让某些代码块隐藏或展示。在{r}中，当code_folding: hide时，你可以通过class.source='fold-show'来让该代码块展示出来；当code_folding: show时，你可以通过class.source='fold-hide'来让该代码块隐藏起来。遗憾的是，code_folding只能够控制部分引擎的代码块（例如{r}、{python}、{stan}…），并不能控制{css}，此时，只需要在{css}中传入class.source='foldable'即可，即{css, class.source='foldable'}。 你也可以为代码块及其输出添加滚动条来限制它的高度。在HTML输出中，你可以在{css}代码块中添加max-height和overflow-y来限制代码块的高度及添加竖向滚动条。 ```{css, echo=FALSE} pre { max-height: 300px; overflow-y: auto; } pre[class] { max-height: 100px; } ``` 由于代码块的内容一般是被包含在&lt;pre class='r'&gt;&lt;/pre&gt;，其输出则是被包含在&lt;pre&gt;&lt;/pre&gt;里。因此上面的{css}样式表达的意思是对于所有的&lt;pre&gt;标签，限定最大高度为300px，同时添加竖向滚动条，而对于拥有类属性的&lt;pre&gt;标签，则限定最大高度为100px，也就是将前面的300px限定高度给覆盖掉了。最终呈现的效果就是代码块及其输出如果超过它们的限定高度，则都会添加竖向滚动条，其中代码块的限定高度为100px，输出的限定高度为300px。当然，你也可以自定义新的类，对特定的代码块（class.source）或输出（class.output）添加竖向滚动条。 ```{css, echo=FALSE} .scroll-style { max-height: 300px; overflow-y: auto; } ``` 1.5.7 添加HTML文件 你可以自行创建相关的HTML文件，在用rmarkdown输出核心HTML文件前导入这些HTML文件，从而丰富核心HTML文件的内容。 in_header 在HTML文件的&lt;head&gt;部分中包含的内容。这通常用于添加自定义css样式或元数据。 before_body 在HTML文件的&lt;body&gt;标签之前包含的内容。这可以用于添加自定义的HTML、JavaScript或其他内容。 after_body 在HTML文件的&lt;body&gt;标签之后包含的内容。这同样可以用于添加自定义的HTML、JavaScript或其他内容。 注意HTML文件应当与.Rmd文件处于同一个目录中 --- title: &#39;includes&#39; output: html_document: includes: in_header: header.html before_body: doc_prefix.html after_body: doc_suffix.html --- 1.5.8 Pandoc参数 有时候你需要使用Pandoc的内容来丰富你的输出文件，可能这些内容并不会在元数据处有专门的选项供你选择。此时，你可以通过pandoc_args来传入你需要的内容。这在1.5.1节中介绍lua过滤器时已经有所提及。更多的内容请参考Pandoc手册。 1.5.9 共享选项 可能你的多个rmarkdown文件都会有相同的output设置，此时，你可以在同一个目录下新建_output.yml文件。在该文件中，和平时的rmarkdown文件不同，你不需要有---和output，直接写相应的选项即可。示例如下： html_document: toc: true toc_float: true toc_depth: 3 highlight: textmate pandoc_args: [&quot;--lua-filter=lua_filter.lua&quot;] 1.5.10 下载文件 你可以在元数据处声明code_download: true，来为读者提供一个下载按钮，从而下载HTML文件的源代码，即.Rmd文件。 output: html_document: code_download: true 除了下载源代码外，还可以下载任意文件。在第1.5.1节已经提及到利用超链接来提供下载链接，这里将介绍如何利用xfun包中的函数提供下载链接。在使用前需要有这些包提供支持。 xfun::pkg_load2(c(&quot;htmltools&quot;, &quot;mime&quot;)) xfun::embed_file()、xfun::embed_files()、xfun::embed_dir()分别用于在html文档中嵌入单个文件、多个文件、文件夹。使用时直接放在r代码块中运行即可。一般而言，对于这三个函数，你只需要用到path、name、text这三个参数。 path 提供文件路径或者文件夹路径。若为多个文件embed_files()，则提供字符串向量，其中每个元素为对应文件的路径。 name 下载文件的名称。一般默认即可，如果需要重命名的话，注意不要忘了文件名的后缀，下载多个文件或者文件夹时需要以压缩包的形式，如.zip。 text 超链接的文本。 "],["rmd_6.html", "1.6 PDF文档", " 1.6 PDF文档 在输出pdf文档之前，得先确保你已经安装了tinytex包，详见第1.1节。 1.6.1 中文文档 一般而言，大家会在pdf中使用中文和英文。如果你在元数据处将输出格式设定为output: pdf_document，很遗憾，即使你在文档中写有中文也不会正常输出。 对此，你需要下载rticles包。下载完成后，在新建rmarkdown文件时选中From Template，从中找到CTeX Documents模板即可。 图 1.17: 中文文档模板 新建文件中已经将元数据设置好了，之后你只要根据需要适当修改即可。 1.6.2 目录 你可以通过toc选项来设置目录，并根据toc_depth选项来控制目录深度（默认为2），即最多展示到第几级标题。number_sections控制是否为标题编号。 1.6.3 图与表 在元数据处，可以预先设置一些关于图片的参数。 fig_width/fig_height 用于控制图片的宽度与高度，默认为6.5x4.5。 fig_crop 控制是否启用pdfcrop，若系统中有，则默认为true。 官方文档介绍可通过tinytex::tlmgr_install(\"pdfcrop\")进行下载，并需配合ghostscript使用。 这一块我并不了解，仅仅是将pdfcrop和ghostscript下载过来罢了，不知道具体的使用方法。 fig_caption 在渲染图片时是否生成标题，默认为true。 dev 用于渲染图片的图形设备，默认为pdf。 即使设置了dev: png，在正文中导入.jpg图片也能正常显示。 output: rticles::ctex: fig_width: 7 fig_height: 6 fig_caption: true dev: pdf fig_crop: true 除了在元数据处设置一些参数，还可以再代码块中进行设置，这在第1.3.9节已经介绍过了。这里重新提及仅针对latex的相关设置。 out.width/out.height 控制图片在输出文档中的宽与高，会进行适当放缩（与物理意义上的fig.width与fig.height略有不同）。对于latex输出，可以设置为0.8\\\\linewidth、3in、8cm、40%（等价于0.4\\\\linewidth）。 out.extra 对于latex输出，out.extra表示的额外参数将会被输入到\\includegraphics[]中，例如out.extra='angle=90'表示图片旋转90度。 resize.width/resize.height 参数值将会被输入到\\resizebox{}{}中，用于调整TikZ图形的大小。 fig.env 设置图片环境，例如fig.env = 'marginfigure'在latex中表示\\begin{marginfigure}。这个选项要求与fig.cap配合使用，即图片得有标题。 很遗憾，当output为ctex时会报错，而当output为pdf_document时则正常，不知原因。 fig.scap 短标题，其参数值会被输入到\\caption[]中，通常会在“List of Figures”中展示。 在元数据处设置lof: true就可以召唤出插图清单了。注意lof: true不在output里面，而是与它同一个级别。 fig.lp 将被输入到\\label{}中的图片标签前缀。 fig.pos 用于\\begin{figure}[]的图片位置参数，可选值为'H'（放置在当前位置，即尽可能靠近代码块）、't'（放置在页面顶部）、'b'（放置在页面底部）、'p'（放置在单独一面）、'!H'（强制放置在当前位置），默认为''。 fig.ncol、fig.subcap、fig.sep 而对于表格，和html文档类似，可在元数据处指定表格的打印形式，可选值有default、kable、tibble、和自定义函数。 1.6.4 语法高亮 同第1.5.3节中的语法高亮。 "],["manim.html", "2 Manim", " 2 Manim Manim是Python的一个数学动画库，作者为“3Blue1Brown”。3Blue1Brown的视频非常精良，这种将知识与可视化结合的方式令人着迷。了解到他的动画均是由Manim制作，故开始上手学习。 本部分内容主要参考Manim社区文档。 除了Manim社区文档，还有热心网友提供该文档的中文版，同时你还可以在b站搜索相关的教程进行学习。 学习心得： 建议先去了解python中关于“类”的知识点。 在我看完Manim社区文档“Quickstart”部分后，秉持着“用时再学”的原则,便直接开始上手创建自己的动画。但很可惜的是，虽然我知道我想表达什么，但对于制作动画而言还是过于宽泛了。在面对这一庞大的工程时，我难以形成一份具体的视频脚本，也就导致了我寸步难行。究其原因，我缺乏视频制作经验，以及缺乏对工具的了解（知识的贫穷限制了视频制作的想象力）。 2024.9.28 终于将第一个Manim视频做完了。在掌握一定基础之后，发挥想象力，先将视频脚本构想出来，然后再去问AI或者翻找官方文档中的参考手册去看看有没有相应的办法，自己再捣鼓捣鼓就出来了。所以在“文本”这一节之后的所有内容就是提及自己用到的方法（例如物体移动、视角移动等），不做详尽的描述（做不完的）。 在“Manim动画”这章我会提供我的Manim动画源码。 "],["manim_1.html", "2.1 安装与创建", " 2.1 安装与创建 安装教程较多，大家可以自行搜索。这里就说说安装过程中遇到的问题，笔者用的是pycharm。 在pycharm中运行示例代码，报错说是找不到ffmpeg。这里可以参考(https://cloud.tencent.com/developer/article/1702673) 运行示例代码后没有报错，但也没有视频。打开终端，看路径是否与动画文件所在的路径一致，如果不一致，可以右键动画文件，在“打开于”中找到“终端”，此时终端的路径也会变为动画文件所在的文件夹中。之后在终端处输入命令manim -pql example.py Example即可。 附上使用的示例： # example.py from manim import * class Example(Scene): def construct(self): t1=Text(&#39;你好！Manim&#39;) self.add(t1) self.wait(1) self.play(t1.animate.shift(UP*3)) circle=Circle().set_color(BLUE) self.play(FadeIn(circle)) self.play(circle.animate.set_fill(BLUE, opacity=0.5), run_time=1) self.wait(1) square=Square().set_color(RED).set_fill(RED, opacity=0.5) self.play(ReplacementTransform(circle, square, run_time=1)) self.play(square.animate.set_fill(RED, opacity=0)) self.wait(1) 图 2.1: 示例 "],["manim_2.html", "2.2 初识Manim", " 2.2 初识Manim Manim最核心的三个部分为Mobjects、Animations、Scenes，即数学对象、动画、场景。 还是以上一节的代码为例。 我们创建了文本Text、圆形Circle、正方形Square，他们都是Mobjects。同时，我们用.animate方法将Mobjects的其他方法动画化（如平移.shift()、填充.set_fill()），用FadeIn()类完成渐入的动画效果，用ReplacementTransform()类完成将圆变换为正方形的动画效果。而所有这些都得在Scenes上进行，也就是将Mobjects添加.add()到画布上，或者在画布上完成动画效果.play()，并且所有的代码都得定义在construct()方法之中，而这个方法又属于一个继承了Scene类的类（这里是Example类）中。 其余细节可以查看代码中的注释。 # example.py from manim import * class Example(Scene): def construct(self): # 创建文本对象 t1=Text(&#39;你好！Manim&#39;) # 将文本对象添加到画布中，默认在画面中心 self.add(t1) # 视频等待1s self.wait(1) # 将文本上移三个单位这一行为动画化 self.play(t1.animate.shift(UP*3)) # UP为常量，表示向上平移一个单位，同理有DOWN、LEFT、RIGHT等 # 创建圆形对象并设置边框颜色 circle=Circle().set_color(BLUE) # BLUE为常量，表示蓝色，同理有RED、GREEN等 # 为圆形对象播放渐入动画 self.play(FadeIn(circle)) # 将圆形对象设置填充色这一行为动画化，不透明度为0.5，持续时间为1s self.play(circle.animate.set_fill(BLUE, opacity=0.5), run_time=1) self.wait(1) # 创建正方形对象，并设置边框颜色和填充色 square=Square().set_color(RED).set_fill(RED, opacity=0.5) # 实现从圆形变换为正方形的动画 # ReplacementTransform表示变换前后对象主体变为后者，即样子变了，名字也变了 # 如果是Transform(circle, square)则表示不更改对象主体，变换后依旧是前者，即样子变了但名字不变 self.play(ReplacementTransform(circle, square, run_time=1)) # 由于是ReplacementTransform，则主体变为正方形，故对正方形填充颜色这一行为进行动画化 self.play(square.animate.set_fill(RED, opacity=0)) self.wait(1) 再来说说输出命令manim -pql example.py Example。manim是必带的，-pql表示以低画质-ql制作视频，并在代码运行后预览-p动画，也就是会直接跳出视频来，不用去生成的文件夹里面找。之后再跟上python文件名example.py和你创建的类名Example即可。 "],["manim_3.html", "2.3 输出设置", " 2.3 输出设置 2.3.1 输出文件夹 在2.2节中，当你运行manim -pql example.py Example之后，就会在同目录中生成media文件夹，里面根据不同的数据类型设置了不同文件夹，如文本、视频、图片等等。你可以在videos文件夹下找到对应的文件名，从中可以找到最终的成片。而在partial_movie_files文件夹中则存放着一小段一小段的视频，可以发现最终的视频就是由这些不同的小视频组合在一起的。 2.3.2 片段 借助self.next_section()，你可以手动设置切片位置，使得除了生成完整视频外，还能生成不同视频片段，这些视频片段存储在与成片同目录的sections文件夹中。需要注意的是，每个视频片段不能仅有一帧内容，也就是说需要包含动画变换或者self.wait()。一旦引入了self.next_section()，事实上，你可以不用在def construct(self):的下一行紧跟self.next_section()，因为他会自动生成。这就好比剪一张纸条，self.next_section()的位置就是要剪开的位置，你是不需要在最开始那剪一刀的。 next_section()有三个参数：name、type、skip_animations。 name 字符串，该片段的名称，视频片段的文件名结构为“类名_编号_name”。 type 字符串，官方文档并没有介绍其用处，目前我只知道他的默认值为DefaultSectionType类的NORMAL属性。 skip_animations 布尔值，是否跳过该片段内的所有动画内容（包括self.wait()），默认为False。若为True，则在成片中并没有该段切片，但sections文件夹中依然会保留该段切片。 注意当你使用self.next_section()时，需要使用manim -pql --save_sections example.py Example才能将片段保存在sections文件夹中。 from manim import * class Example(Scene): def construct(self): t1 = Text(&#39;你好！Manim&#39;) self.add(t1) self.wait(1) self.play(t1.animate.shift(UP * 3)) self.next_section(&#39;part_1&#39;) circle = Circle().set_color(BLUE) self.play(FadeIn(circle)) self.play(circle.animate.set_fill(BLUE, opacity=0.5), run_time=1) self.wait(1) self.next_section(&#39;part_2&#39;, skip_animations=True) square = Square().set_color(RED).set_fill(RED, opacity=0.5) self.play(ReplacementTransform(circle, square, run_time=1)) self.play(square.animate.set_fill(RED, opacity=0)) self.wait(1) 2.3.3 命令行标志 对于manim -pql example.py Example，其结构为manim [OPTIONS] FILE [SCENES]。 OPTIONS就是一些参数标志，例如-p表示在渲染后预览视频，-ql代表分辨率854x480及15 FPS。更多的参数设置请参见第2.4.1节 SCENES为你创建的类名。如果一个文件中仅有一个类，则可以省略该类名。如果有多个类，则指定输出特定类，类与类之间用空格隔开，如manim -pql example.py ExampleOne ExampleTwo。如果要输出所有的类，可以不用一一指名，直接在OPTIONS中使用-a标志即可。注意，一个类就相当于一个成片，一次性输出多个类意味着输出多个独立的成片。 "],["manim_4.html", "2.4 配置", " 2.4 配置 Manim有着非常广泛的配置系统，可以根据调整命令行参数(command-line interface, CLI)、更改ManimConfig类属性、创建配置文件。其中最为常用的就是调整命令行参数。 2.4.1 命令行参数 Manim中最为常用的命令就是render，也就是渲染命令。其一般的格式为manim render [OPTIONS] FILE [SCENES]。如果没有详细指名命令的话，则Manim默认使用render命令，也就是我们常见的manim [OPTIONS] FILE [SCENES]。 在终端处输入manim --help后，就可以发现除了render外的其他命令及选项，通过manim [command]或manim [command] --help查询详情。这里主要介绍render命令的相关设置。在终端处输入manim render --help即可查看渲染命令的一些参数设置，如下所示。 Manim Community v0.18.1 Usage: manim render [OPTIONS] FILE [SCENE_NAMES]... Render SCENE(S) from the input FILE. FILE is the file path of the script or a config file. SCENES is an optional list of scenes in the file. Global options: -c, --config_file TEXT Specify the configuration file to use for render settings. --custom_folders Use the folders defined in the [custom_folders] section of the config file to define the output folder structure. --disable_caching Disable the use of the cache (still generates cache files). --flush_cache Remove cached partial movie files. --tex_template TEXT Specify a custom TeX template file. -v, --verbosity [DEBUG|INFO|WARNING|ERROR|CRITICAL] Verbosity of CLI output. Changes ffmpeg log level unless 5+. --notify_outdated_version / --silent Display warnings for outdated installation. --enable_gui Enable GUI interaction. --gui_location TEXT Starting location for the GUI. --fullscreen Expand the window to its maximum possible size. --enable_wireframe Enable wireframe debugging mode in opengl. --force_window Force window to open when using the opengl renderer, intended for debugging as it may impact performance --dry_run Renders animations without outputting image or video files and disables the window --no_latex_cleanup Prevents deletion of .aux, .dvi, and .log files produced by Tex and MathTex. --preview_command TEXT The command used to preview the output file (for example vlc for video files) Output options: -o, --output_file TEXT Specify the filename(s) of the rendered scene(s). -0, --zero_pad INTEGER RANGE Zero padding for PNG file names. [0&lt;=x&lt;=9] --write_to_movie Write the video rendered with opengl to a file. --media_dir PATH Path to store rendered videos and latex. --log_dir PATH Path to store render logs. --log_to_file Log terminal output to file. Render Options: -n, --from_animation_number TEXT Start rendering from n_0 until n_1. If n_1 is left unspecified, renders all scenes after n_0. -a, --write_all Render all scenes in the input file. --format [png|gif|mp4|webm|mov] -s, --save_last_frame Render and save only the last frame of a scene as a PNG image. -q, --quality [l|m|h|p|k] Render quality at the follow resolution framerates, respectively: 854x480 15FPS, 1280x720 30FPS, 1920x1080 60FPS, 2560x1440 60FPS, 3840x2160 60FPS -r, --resolution TEXT Resolution in &quot;W,H&quot; for when 16:9 aspect ratio isn&#39;t possible. --fps, --frame_rate FLOAT Render at this frame rate. --renderer [cairo|opengl] Select a renderer for your Scene. -g, --save_pngs Save each frame as png (Deprecated). -i, --save_as_gif Save as a gif (Deprecated). --save_sections Save section videos in addition to movie file. -t, --transparent Render scenes with alpha channel. --use_projection_fill_shaders Use shaders for OpenGLVMobject fill which are compatible with transformation matrices. --use_projection_stroke_shaders Use shaders for OpenGLVMobject stroke which are compatible with transformation matrices. Ease of access options: --progress_bar [display|leave|none] Display progress bars and/or keep them displayed. -p, --preview Preview the Scene&#39;s animation. OpenGL does a live preview in a popup window. Cairo opens the rendered video file in the system default media player. -f, --show_in_file_browser Show the output file in the file browser. --jupyter Using jupyter notebook magic. Other options: --help Show this message and exit. Made with &lt;3 by Manim Community developers. 虽然选项挺多的，但实际考察后，对我来说估计只能用到其中一部分。 下面是一些例子。 manim -pql -o 样例 demo.py Example 输出的视频文件名称为“样例” manim -pql -n 1,4 demo.py Example 只渲染第2个到第4个动画 manim -apql demo.py 输出demo.py文件中所有的类 manim -pql –format gif demo.py Example 输出格式为gif manim -pqh –progress_bar display demo.py Example 视频质量高，并显示渲染进度条 manim -pql -r 1920,1080 –fps 54 demo.py Example 设置分辨率为1920x1080，帧率为54FPS 2.4.2 ManimConfig类 通过更改config对象的属性是确定Manim全局配置最直接的方式。config对象在ManimConfig类中，主要有两种方式（官方文档中建议第一种方式）来设置属性，如下所示。 from manim import * config.background_color = WHITE config[&quot;background_color&quot;] = WHITE ... config对象的所有属性如下所示。需要注意的是，ManimConfig具有内部一致性，更改了某项属性就会影响与其相关联的属性，例如更改y轴的半径frame_y_radius就会影响到画面的高度frame_height。 平面直角坐标系的原点处于画面中心，向上是y轴正半轴，向右是x轴正半轴 [&#39;aspect_ratio&#39;, &#39;assets_dir&#39;, &#39;background_color&#39;, &#39;background_opacity&#39;, &#39;bottom&#39;, &#39;custom_folders&#39;, &#39;disable_caching&#39;, &#39;dry_run&#39;, &#39;ffmpeg_loglevel&#39;, &#39;flush_cache&#39;, &#39;frame_height&#39;, &#39;frame_rate&#39;, &#39;frame_size&#39;, &#39;frame_width&#39;, &#39;frame_x_radius&#39;, &#39;frame_y_radius&#39;, &#39;from_animation_number&#39;, &#39;fullscreen&#39;, &#39;images_dir&#39;, &#39;input_file&#39;, &#39;left_side&#39;, &#39;log_dir&#39;, &#39;log_to_file&#39;, &#39;max_files_cached&#39;, &#39;media_dir&#39;, &#39;media_width&#39;, &#39;movie_file_extension&#39;, &#39;notify_outdated_version&#39;, &#39;output_file&#39;, &#39;partial_movie_dir&#39;, &#39;pixel_height&#39;, &#39;pixel_width&#39;, &#39;plugins&#39;, &#39;preview&#39;, &#39;progress_bar&#39;, &#39;quality&#39;, &#39;right_side&#39;, &#39;save_as_gif&#39;, &#39;save_last_frame&#39;, &#39;save_pngs&#39;, &#39;scene_names&#39;, &#39;show_in_file_browser&#39;, &#39;sound&#39;, &#39;tex_dir&#39;, &#39;tex_template&#39;, &#39;tex_template_file&#39;, &#39;text_dir&#39;, &#39;top&#39;, &#39;transparent&#39;, &#39;upto_animation_number&#39;, &#39;use_opengl_renderer&#39;, &#39;verbosity&#39;, &#39;video_dir&#39;, &#39;window_position&#39;, &#39;window_monitor&#39;, &#39;window_size&#39;, &#39;write_all&#39;, &#39;write_to_movie&#39;, &#39;enable_wireframe&#39;, &#39;force_window&#39;] 2.4.3 配置文件 配置文件中你可以先创建一个.txt文件，在其中进行设置之后，再修改后缀名为.cfg即可创建配置文件，而且这个配置文件必须命名为manim.cfg。在配置文件中，必须把[CLI]放在第一行，之后再根据前一节的config对象的属性进行设置，设置方法如下所示。注意这里的属性必须为全称，部分属性不能向命令行参数那样简写！ [CLI] background_color = WHITE 在这个例子中，我们将背景色改成了白色。之后需将该文件放置在与脚本文件的相同目录中。当你按常规方法（如manim -pql demo.py Example）输出视频时，Manim会自动找到.cfg文件进行配置并反映在视频当中。 上述这种放在与脚本文件相同目录中的配置文件叫做“folder-wide config file”，其作用范围就是在一个文件夹内。如果你有多个视频需要产出，而且他们恰好需要有共同的配置，此时你可以选择将该配置文件放置在特定的文件夹中，从而作用在多个视频中，此种配置文件即“user-wide config file”，也就是针对用户而言的。配置文件在不同系统中要放置在不同地方，具体如下所示。其中UserDirectory就是你的主文件夹。 我的UserDirectory就是在C盘里，需要在“查看”里勾选“隐藏的项目”才能找到下面的文件夹。 Windows: UserDirectory/AppData/Roaming/Manim/manim.cfg MacOS: UserDirectory/.config/manim/manim.cfg Linux: UserDirectory/.config/manim/manim.cfg 总体而言，调整命令行参数最为便捷，而后两种方法则有更多的选项可供选择。而且各个配置方式的优先级从低到高如下所示。所谓“优先级”就是如果各个配置方式中存在冲突的部分，则以优先级较高的方式为准。 Library-wide config file 默认的配置。 user-wide config file, if it exists 在主文件夹的配置文件 folder-wide config file, if it exists OR custom config file, if passed via –config_file 与脚本文件相同目录的配置文件 other CLI flags 其余命令行标志 any programmatic changes made after the config system is set ManimConfig类 "],["manim_5.html", "2.5 文本", " 2.5 文本 Manim中的文本主要可以分为两类，分别是非LaTeX文本和LaTeX文本。前者使用Pango，归为text_mobject类；后者用LaTeX，归为Tex_mobject类。非LaTeX文本有三种：Text、MarkupText、Paragraph。LaTeX有五种：Tex、MathTex、Title、BulletedList、SingleStringMathTex（不谈）。除此之外，还有code_mobject类的代码文本及numbers类的数字型文本。 2.5.1 Text class Text(text, fill_opacity=1.0, stroke_width=0, *, color=ManimColor(&#39;#FFFFFF&#39;), font_size=48, line_spacing=-1, font=&#39;&#39;, slant=&#39;NORMAL&#39;, weight=&#39;NORMAL&#39;, t2c=None, t2f=None, t2g=None, t2s=None, t2w=None, gradient=None, tab_width=4, warn_missing_font=True, height=None, width=None, should_center=True, disable_ligatures=False, use_svg_cache=False, **kwargs) 表 2.1: Text类的方法与属性 方法 属性 font_list animate init_colors animation_overrides color depth fill_color font_size hash_seed height n_points_per_curve sheen_factor stroke_color width 继承自SVGMobject 生成文本的最简单方法是使用Text类。除了输出英文字母，它还能输出非英文字母的语言，例如中文、日文、韩文。示例如下所示。 官方文档中介绍的就是中、日、韩语言都能显示，但我实操的时候发现韩语显示不出。这也无伤大雅，我估计我只用得到到英语和中文。 from manim import * class Example(Scene): def construct(self): text_1 = Text(&quot;Hello world&quot;, font_size=100) text_2 = Text(&quot;你好，世界&quot;, font_size=100) text_3 = Text(&quot;こんにちは，せかい&quot;, font_size=100) text_4 = Text(&quot;안녕, 세계&quot;, font_size=100) self.add(text_1.shift(UP*2)) self.add(text_2.next_to(text_1, DOWN)) self.add(text_3.next_to(text_2, DOWN)) self.add(text_4.next_to(text_3, DOWN)) 图 2.2: 各语言文本示例 字体 首先你得知道你能使用哪些字体，使用下面的代码查看你能使用的字体。 import manimpango print(manimpango.list_fonts()) 然后，为font参数赋值即可选择对应字体，如Text('Hello', font='Times New Roman') 斜体和粗细 slant 参数slant控制斜体，可选值为NORMAL、ITALIC、OBLIQUE。后两个都表示斜体，但ITALIC表示’Roman Style’，OBLIQUE表示’Italic Style’，存在细微区别。如Text(\"Hello world\", slant=ITALIC)。 weight 参数weight控制字体粗细，粗细程度从小到大为THIN、ULTRALIGHT、LIGHT、BOOK、NORMAL、MEDIUM、SEMIBOLD、BOLD、ULTRABOLD、HEAVY、ULTRAHEAVY。如Text(\"Hello world\", weight=BOLD)。 from manim import * class Example(Scene): def construct(self): t_1 = Text(&quot;Hello world&quot;, slant=ITALIC) t_2 = Text(&quot;Hello world&quot;, weight=BOLD) self.add(t_1.shift(UP)) self.add(t_2.next_to(t_1, DOWN)) 图 2.3: 斜体与粗细示例 颜色 纯色 纯色可通过color参数为文本设置颜色，例如Text('Hello World', color=BLUE)。 渐变色 渐变色可通过gradient参数进行设置。gradient参数接受一个元组，里面每个元素都是颜色常量，并且其顺序决定了渐变顺序。例如Text('Hello World', gradient=(BLUE, YELLOW, RED))。 自定义文本颜色 有时你可能相对文本中的部分文字设置特定的颜色或者渐变色，此时你可以使用t2c和t2g进行个性化设置。 t2c参数用来设置部分文本的纯色。你可以为其传入一个字典，用切片的方式定位目标文本或者直接传入目标文本来进行上色。例如Text('Hello World', t2c={'[1:4]':BLUE, 'rld':RED})。 t2g参数用来设置部分文本的渐变色。同理，也是接收一个字典，只不过这里的值要变为元组。例如Text('明天你还在这里吗', t2g={'[1:4]': (BLUE, YELLOW), '这里': (YELLOW, RED)})。 t2g可能存在bug，会导致不能对目标文本进行渐变色设置，可能整个文本都会被设置渐变色。如下面的t_4。 能被切片的方式选择字符说明Text()字符串对象是可迭代的、可被索引的。 from manim import * class Example(Scene): def construct(self): t_1 = Text(&#39;Hello World&#39;, color=BLUE) t_2 = Text(&#39;Hello World&#39;, gradient=(BLUE, YELLOW, RED)) t_3 = Text(&#39;Hello World&#39;, t2c={&#39;[1:4]&#39;:BLUE, &#39;rld&#39;:RED}) t_4 = Text(&#39;Hello World&#39;, t2g={&#39;[1:4]&#39;:(BLUE, YELLOW), &#39;rld&#39;:(YELLOW ,RED)}) t_5 = Text(&#39;明天你还在这里吗&#39;, t2g={&#39;[1:4]&#39;: (BLUE, YELLOW), &#39;这里&#39;: (YELLOW, RED)}) self.add(t_1.shift(2*UP)) self.add(t_2.next_to(t_1, DOWN)) self.add(t_3.next_to(t_2, DOWN)) self.add(t_4.next_to(t_3, DOWN)) self.add(t_5.next_to(t_4, DOWN)) 图 2.4: 文本颜色 大小 font_size font_size是Text()的参数值(默认为font_size=48)，可直接在Text()中设定，如Text('Hello World', font_size=50)。 scale scale是Text()的属性，用于调整字体大小的倍数，如Text('Hello World').scale(0.5)表示默认大小的0.5倍。 from manim import * class Example(Scene): def construct(self): t_1 = Text(&#39;Hello World&#39;, font_size=50) t_2 = Text(&#39;Hello World&#39;).scale(0.5) self.add(t_1.shift(UP)) self.add(t_2.next_to(t_1, DOWN)) 图 2.5: 文本颜色 貌似这里也有bug，t_2被设置了渐变色。 行间距与换行 在字符串中可以添加\\n实现换行效果，并通过Text()中的line_spacing来调整行间距，如下所示。 from manim import * class Example(Scene): def construct(self): t_1 = Text(&#39;Hello\\nWorld&#39;, line_spacing=3) t_2 = Text(&#39;Hello\\nWorld&#39;) group = VGroup(t_1, t_2) self.add(group.arrange(LEFT)) 图 2.6: 行间距与换行 2.5.2 MarkupText class MarkupText(text, fill_opacity=1, stroke_width=0, color=None, font_size=48, line_spacing=-1, font=&#39;&#39;, slant=&#39;NORMAL&#39;, weight=&#39;NORMAL&#39;, justify=False, gradient=None, tab_width=4, height=None, width=None, should_center=True, disable_ligatures=False, warn_missing_font=True, **kwargs) (#tab:manim_t2)MarkupText类的方法与属性 方法 属性 font_list animate animation_overrides color depth fill_color font_size hash_seed height n_points_per_curve sheen_factor stroke_color width 继承自SVGMobject MarkupText即使用了PangoMarkup的标记文本，和html语言类似，他允许使用“标签”来直接修改对应“标签”内的文本样式。 PangoMarkup is a small markup language like html and it helps you avoid using “range of characters” while coloring or styling a piece a Text. 下面罗列了一些常见的标签： &lt;b&gt;bold&lt;/b&gt;, &lt;i&gt;italic&lt;/i&gt; and &lt;b&gt;&lt;i&gt;bold+italic&lt;/i&gt;&lt;/b&gt; &lt;ul&gt;underline&lt;/ul&gt; and &lt;s&gt;strike through&lt;/s&gt; &lt;tt&gt;typewriter font&lt;/tt&gt; &lt;big&gt;bigger font&lt;/big&gt; and &lt;small&gt;smaller font&lt;/small&gt; &lt;sup&gt;superscript&lt;/sup&gt; and &lt;sub&gt;subscript&lt;/sub&gt; &lt;span underline=&quot;double&quot; underline_color=&quot;green&quot;&gt;double underline&lt;/span&gt; &lt;span underline=&quot;error&quot;&gt;error underline&lt;/span&gt; &lt;span overline=&quot;single&quot; overline_color=&quot;green&quot;&gt;overline&lt;/span&gt; &lt;span strikethrough=&quot;true&quot; strikethrough_color=&quot;red&quot;&gt;strikethrough&lt;/span&gt; &lt;span font_family=&quot;sans&quot;&gt;temporary change of font&lt;/span&gt; &lt;span foreground=&quot;red&quot;&gt;temporary change of color&lt;/span&gt; &lt;span fgcolor=&quot;red&quot;&gt;temporary change of color&lt;/span&gt; &lt;gradient from=&quot;YELLOW&quot; to=&quot;RED&quot;&gt;temporary gradient&lt;/gradient&gt; &lt;tt&gt;标签内的字体就像打字机打出来的一样 &lt;sup&gt;和&lt;sub&gt;分别表示上标和下标 foreground和fgcolor都是作用于文本颜色（貌似效果一样？） 在&lt;span&gt;标签里可以使用十六进制颜色(如#ff0000)或者CSS命名颜色(如Coral)。如果你想在其中继续使用Manim的颜色常量，那么你就得在字符串前添加f，以格式化的形式嵌入颜色常量(如MarkupText(f'&lt;span fgcolor=\"{RED}\"&gt;Hello&lt;/span&gt;'))。 from manim import * class Example(Scene): def construct(self): t_1 = MarkupText(&#39;Do you need &lt;tt&gt;help&lt;/tt&gt;?&#39;) t_2 = MarkupText(&#39;H&lt;sub&gt;2&lt;/sub&gt;O and OH&lt;sup&gt;+&lt;/sup&gt;&#39;) t_3 = MarkupText(&#39;&lt;span foreground=&quot;red&quot;&gt;Hello&lt;/span&gt;&#39;) t_4 = MarkupText(&#39;&lt;span fgcolor=&quot;red&quot;&gt;Hello&lt;/span&gt;&#39;) t_5 = MarkupText(&#39;&lt;span fgcolor=&quot;#ff0000&quot;&gt;Hello&lt;/span&gt;&#39;) t_6 = MarkupText(&#39;&lt;span fgcolor=&quot;Coral&quot;&gt;Hello&lt;/span&gt;&#39;) t_7 = MarkupText(f&#39;&lt;span fgcolor=&quot;{RED}&quot;&gt;Hello&lt;/span&gt;&#39;) group = VGroup(t_1, t_2, t_3, t_4, t_5, t_6, t_7) self.add(group.arrange(DOWN)) 图 2.7: span示例 &lt;gradient&gt;支持十六进制颜色或者Manim自带的颜色常量。但&lt;gradient&gt;在实际应用中稍显复杂。对于带连字ligatures（姑且翻译为“连字”吧，有些语言会把两个或者多个字符视为一个字符，也就是连字）的语言，&lt;gradient&gt;有时并不能正确识别渐变色的起始字符与终止字符，起始字符和终止字符可能会提前或推迟。对此，你可以使用&lt;gradient&gt;的offset属性来调整起始字符和结束字符的位置。例如offset='1'表示起始字符提前一个位置；offset='1,2'表示起始字符提前一个位置，终止字符提前两个位置；又如offset=',-1'表示终止字符推迟一个位置。对于不依赖连字ligatures的语言，你可以直接在MarkupText()中设置disable_ligatures=True。 貌似中文可以不用考虑“连字”问题，英语的话可能会遇到 当你在一个字符串中使用了&lt;gradient&gt;，并同时使用了overline或underline或strikethrough，你可能需要使用offset进行适当调整。 from manim import * class Example(Scene): def construct(self): t_1 = MarkupText(&#39;我想打&lt;gradient from=&quot;BLUE&quot; to=&quot;GREEN&quot;&gt;羽毛球&lt;/gradient&gt;&#39;) t_2 = MarkupText(&#39;hello &lt;gradient from=&quot;RED&quot; to=&quot;YELLOW&quot;&gt;world&lt;/gradient&gt; bye&#39;) t_3 = MarkupText(&#39;hello &lt;gradient from=&quot;RED&quot; to=&quot;YELLOW&quot; offset=&quot;1,2&quot;&gt;world&lt;/gradient&gt; bye&#39;) t_4 = MarkupText(&#39;&lt;span underline=&quot;double&quot; underline_color=&quot;green&quot;&gt;Do&lt;/span&gt; &lt;gradient from=&quot;RED&quot; to=&quot;YELLOW&quot;&gt;you need&lt;/gradient&gt; help?&#39;) t_5 = MarkupText(&#39;&lt;span underline=&quot;double&quot; underline_color=&quot;green&quot;&gt;Do&lt;/span&gt; &lt;gradient from=&quot;RED&quot; to=&quot;YELLOW&quot; offset=&quot;-2&quot;&gt;you need&lt;/gradient&gt; help?&#39;) t_6 = MarkupText(&#39;&lt;span underline=&quot;single&quot; underline_color=&quot;green&quot;&gt;Do&lt;/span&gt; &lt;gradient from=&quot;RED&quot; to=&quot;YELLOW&quot; offset=&quot;-1&quot;&gt;you need&lt;/gradient&gt; help?&#39;) group = VGroup(t_1, t_2, t_3, t_4, t_5, t_6) self.add(group.arrange(DOWN)) 图 2.8: gradient示例 如果你需要在文本中使用&lt;、&gt;和&amp;，得进行转义，用&amp;lt;、&amp;gt、&amp;amp;依次替代。 MarkupText()中的参数justify=True能够实现文本左右对齐，如下面所示。 from manim import * class Example(Scene): def construct(self): ipsum_text = ( &quot;Lorem ipsum dolor sit amet, consectetur adipiscing elit.&quot; &quot;Praesent feugiat metus sit amet iaculis pulvinar. Nulla posuere &quot; &quot;quam a ex aliquam, eleifend consectetur tellus viverra. Aliquam &quot; &quot;fermentum interdum justo, nec rutrum elit pretium ac. Nam quis &quot; &quot;leo pulvinar, dignissim est at, venenatis nisi.&quot; ) justified_text = MarkupText(ipsum_text, justify=True).scale(0.4) not_justified_text = MarkupText(ipsum_text, justify=False).scale(0.4) just_title = Title(&quot;Justified&quot;) njust_title = Title(&quot;Not Justified&quot;) self.add(njust_title, not_justified_text) self.play( FadeOut(not_justified_text), FadeIn(justified_text), FadeOut(njust_title), FadeIn(just_title), run_time=5 ) self.wait(1) python会自动将圆括号中的几个连续无间隔字符串拼接在一起。 2.5.3 Paragraph class Paragraph(*text, line_spacing=-1, alignment=None, **kwargs) (#tab:manim_t3)Paragraph类的方法与属性 方法 属性 animate animation_overrides color depth fill_color height n_points_per_curve sheen_factor stroke_color width 继承自VGroup Paragraph适合用于大段文本，它将每一行视作VGroup的一个元素。Paragraph主要接收text、line_spacing、alignment这三个参数。 text 每一行的内容将作为独立的字符串输入进去，例如Paragraph('再见了妈妈', '我今晚就要远航', '别为我担心')就表示第一行为“再见了妈妈”，第二行为“我今晚就要远航”，第三行就是“别为我担心”。 line_spacing 控制行间距，默认为-1，表示自动调整行间距。 alignment 对齐方式，默认为None，可选值为'center'、'left'、'right'。 from manim import * class Example(Scene): def construct(self): paragraph = Paragraph(&#39;再见了妈妈&#39;, &#39;我今晚就要远航&#39;, &#39;别为我担心&#39;, alignment = &#39;center&#39;, line_spacing = 2) paragraph.color = &#39;#FFB6C1&#39; self.add(paragraph) 图 2.9: Paragraph示例 2.5.4 Tex class Tex(*tex_strings, arg_separator=&#39;&#39;, tex_environment=&#39;center&#39;, **kwargs) (#tab:manim_t4)Tex类的方法与属性 方法 属性 animate animation_overrides color depth fill_color font_size hash_seed height n_points_per_curve sheen_factor stroke_color width 继承自MathTex 在Tex()中也可像Text()一样直接设置color、font_size等参数。因为往上有相同的父类 使用Tex()即可插入LaTeX。由于LaTeX经常使用特殊字符（例如’\\’），这在python中也具有特殊意义，所以推荐使用Tex(r'字符串')来输入字符串。 from manim import * class Example(Scene): def construct(self): t1 = Tex(&#39;Hello&#39;, &#39;World&#39;, &#39;Bye&#39;, arg_separator=&#39;-&#39;) self.add(t1) 图 2.10: Tex示例 2.5.5 MathTex class MathTex(*tex_strings, arg_separator=&#39; &#39;, substrings_to_isolate=None, tex_to_color_map=None, tex_environment=&#39;align*&#39;, **kwargs) (#tab:manim_t5)MathTex类的方法与属性 方法 属性 get_part_by_tex animate get_parts_by_tex animation_overrides index_of_part color index_of_part_by_tex depth set_color_by_tex fill_color set_color_by_tex_to_color_map font_size set_opacity_by_tex hash_seed sort_alphabetically height n_points_per_curve sheen_factor stroke_color width 继承自SingleStringMathTex 关于如何编辑数学公式大家可以参考这篇博客。 MathTex()中的字符串默认处在数学环境中，对应有Tex(r'$字符串$')，相当于是数学公式的Tex()简写。 from manim import * class Example(Scene): def construct(self): t1 = MathTex(r&#39;f(k)={n \\choose k}p^{k}(1-p)^{n-k}&#39;) t2 = Tex(r&#39;$f(k)={n \\choose k}p^{k}(1-p)^{n-k}$&#39;) self.add(t1.shift(UP)) self.add(t2.shift(DOWN)) 图 2.11: MathTex示例 字符串上色 MathTex能够接收多个字符串，最后会用arg_separator参数（默认为空格）将其隔开。对此，可以通过索引或者字符匹配（匹配的结果是对应的完整字符串）来选中特定的字符串进行设置，如下所示： Tex()也同样适用，毕竟其父类就是MathTex()。 from manim import * class Example(Scene): def construct(self): tex_1 = Tex(&#39;Hello&#39;, r&#39;$\\bigstar$&#39;, r&#39;\\LaTeX&#39;, font_size=58) tex_1[1].set_color(RED) tex_2 = MathTex(r&#39;\\sum_{i=0}^n i^2&#39;, &#39;=&#39;, r&#39;\\frac{(n^2+n)(2n+1)}{6}&#39;, font_size=58) tex_2.set_color_by_tex(&#39;i=&#39;, RED) self.add(tex_1.shift(UP)) self.add(tex_2.shift(DOWN)) 图 2.12: 子字符串上色示例 目标字符上色 如果想要匹配某个字符而不是字符所在的整个字符串，则可以用到substrings_to_isolate参数，它能够在set_color_by_tex时将目标字符串单独分离出来。或者用两对花括号{{.}}将目标字符框起来也能起到同样作用。二者如下所示： from manim import * class Example(Scene): def construct(self): e_1 = MathTex( r&#39;e^x = x^0 + x^1 + \\frac{1}{2} x^2 + \\frac{1}{6} x^3 + \\cdots + \\frac{1}{n!} x^n + \\cdots&#39; ) e_1.set_color_by_tex(&#39;x&#39;, BLUE) e_2 = MathTex( r&#39;e^x = x^0 + x^1 + \\frac{1}{2} x^2 + \\frac{1}{6} x^3 + \\cdots + \\frac{1}{n!} x^n + \\cdots&#39;, substrings_to_isolate=&#39;x&#39; ) e_2.set_color_by_tex(&#39;x&#39;, BLUE) e_3 = MathTex( r&#39;e^{{x}} = {{x}}^0 + {{x}}^1 + \\frac{1}{2} {{x}}^2 + \\frac{1}{6} {{x}}^3 + \\cdots + \\frac{1}{n!} {{x}}^n + \\cdots&#39; ) e_3.set_color_by_tex(&#39;x&#39;, BLUE) group=VGroup(e_1, e_2, e_3) self.add(group.arrange(DOWN)) 图 2.13: 目标字符上色示例 除了多个字符串的索引，还可以对一个字符串（即Mobject对象）中的不同组成部分进行索引。index_labels()能够显示Mobject对象的组成部分的索引，如下所示： from manim import * class Example(Scene): def construct(self): text = MathTex(r&quot;\\binom{2n}{n+2}&quot;, font_size=96) # MathTex中仅有一个字符串，所以只索引了第一项 # 若有多个字符串则可以通过对应的索引选中目标字符串 self.add(index_labels(text[0])) text[0][1:3].set_color(YELLOW) text[0][3:6].set_color(RED) self.add(text) 图 2.14: 目标字符上色示例 字体 MathTex()可通过tex_template参数来修改字体。在TexFontTemplates类中有诸多字体可供选择，通过提取其属性值来进行赋值，例如tex_template = TexFontTemplates.french_cursive。可选字体如官网所示。 2.5.6 Title class Title(*text_parts, include_underline=True, match_underline_width_to_text=False, underline_buff=0.25, **kwargs) (#tab:manim_t6)Title类的方法与属性 方法 属性 animate animation_overrides color depth fill_color font_size hash_seed height n_points_per_curve sheen_factor stroke_color width 继承自Tex Title的作用就是在画面正上方显示标题。如果你想在标题中输入中文，那么一定要设置tex_template参数，并为其赋值为tex_template = TexTemplateLibrary.ctex。 Tex若想显示中文也得这样设置，但一般用Text来显示中文会更多。 from manim import * class Example(Scene): def construct(self): banner = ManimBanner() title = Title(&#39;Hello你好&#39;, tex_template=TexTemplateLibrary.ctex, match_underline_width_to_text=True) self.add(banner, title) 图 2.15: Title示例 2.5.7 BulletedList class BulletedList(*items, buff=0.5, dot_scale_factor=2, tex_environment=None, **kwargs) (#tab:manim_t7)Title类的方法与属性 方法 属性 fade_all_but animate animation_overrides color depth fill_color font_size hash_seed height n_points_per_curve sheen_factor stroke_color width 继承自Tex BulletedList实际上就是markdown中的无序列表。 from manim import * class Example(Scene): def construct(self): blist = BulletedList(&#39;项目 1&#39;, &#39;项目 2&#39;, &#39;项目 3&#39;, height=2, width=2, tex_template=TexTemplateLibrary.ctex) blist.set_color_by_tex(&#39;1&#39;, RED) blist.set_color_by_tex(&#39;2&#39;, GREEN) blist.set_color_by_tex(&#39;3&#39;, BLUE) self.add(blist) 图 2.16: BulletedList示例 "],["manim_6.html", "2.6 内置颜色", " 2.6 内置颜色 参考知乎的这篇文章。 "],["manim_7.html", "2.7 物体的位置与移动", " 2.7 物体的位置与移动 .next_to() .move_to() .shift() MoveToTarget() "],["manim_8.html", "2.8 动画", " 2.8 动画 Succession() 动画有先后顺序，前一个完成再接下一个 "],["manim_9.html", "2.9 镜头视角", " 2.9 镜头视角 self.camera.frame.animate.shift() 仅能在MovingCameraScene类下使用，而不是Scene类。 "],["manim_10.html", "2.10 存储对象", " 2.10 存储对象 Group() VGroup() self.mobjects "],["manim_11.html", "2.11 成品", " 2.11 成品 最小二乘与投影矩阵 "],["sql.html", "3 SQL", " 3 SQL 该章节为《MySQL必知必会》的学习笔记。 "],["sql_1.html", "3.1 连接MySQL", " 3.1 连接MySQL 3.1.1 R library(DBI) library(RMySQL) con &lt;- dbConnect( RMySQL::MySQL(), dbname = &quot;data_1&quot;, # 数据库名称（Navicat中的数据库名） host = &quot;localhost&quot;, # 服务器地址（本地为localhost，远程为IP） port = 3306, # 端口号（默认3306） user = &quot;root&quot;, # 用户名（Navicat连接使用的） password = &quot;password&quot; # 密码 ) dbListTables(con) # 查看所有表 df &lt;- dbReadTable(con, &quot;table1&quot;) 用dbReadTable()读取整个表，并存储为data frame格式的数据框，后续可用dplyr进行数据清洗。 当然，可以直接使用dbGetQuery()进行SQL查询，第一个参数是连接名称，第二个参数就是SQL语句。 df &lt;- dbGetQuery(con, &quot; SELECT * FROM table1 WHERE price &gt; 100 &quot;) 亦或者，用dplyr语法代替SQL语句进行查询。 df &lt;- tbl(con, &#39;table1&#39;) # 懒加载，构建连接，数据还未导入R中 result_df &lt;- df %&gt;% filter(price &gt; 100) %&gt;% # dplyr语法筛选数据 collect() # 将数据库中的数据导入到R中 这样导出的数据就是tibble格式的数据框，注意别忘了collect() "],["sql_2.html", "3.2 MySQL必知必会", " 3.2 MySQL必知必会 3.2.1 提要 sql语句可单行或多行，以分号结尾 可使用空格缩进来增强可读性 MySQL数据库的SQL语句不区分大小写，但关键字还是建议用大写 单行注释用#或--，多行注释用/* text */ 子句顺序 WHERE → GROUP BY → HAVING → SELECT（包括别名定义）→ ORDER BY → LIMIT 3.2.2 选择数据库与表 选择数据库 USE 数据库名; 查询所有可用的数据库 SHOW DATABASES; 查询当前选择的数据库内可用表的列表 SHOW TABLES; 查询某表的所有列 SHOW COLUMNS FROM 表名; 等价于DESCRIBE 表名; 3.2.3 检索数据 检索单个列 SELECT 列名 FROM 表名; 返回的数据顺序不一定与原始顺序一致 select语句包含的内容为空，则输出NULL；若空内容出现在from语句，则select语句输出空值 检索多个列 SELECT 列名_1, 列名_2, 列名_3 FROM 表名; 检索所有列 SELECT * FROM 表名; 行去重 SELECT DISTINCT 列名 FROM 表名; 无论检索多少列，DISTINCT作用于所有的列，而不单单是前置它的列。 限制结果 LIMIT子句用于控制输出的行数，且记第一行为行0。 SELECT 列名 FROM 表名 LIMIT 5; 表示输出不多于5行的记录。 SELECT 列名 FROM 表名 LIMIT 5, 4; 表示从行5（事实上是第6行）开始数4行。 等价于 SELECT 列名 FROM 表名 LIMIT 4 OFFSET 5; 使用完全限定的表名 可使用X.Y的形式来声明Y是来自于X的Y，其中X可为表名或数据库名 SELECT 表名.列名 FROM 数据库名.表名; 多分类表达式 SELECT CASE WHEN condition THEN result ELSE END AS col 3.2.4 排序检索数据 ORDER BY子句控制排序。 SELECT 列名 FROM 表名 ORDER BY 列名; 若按多列排序，则为ORDER BY 列名1, 列名2;，先根据列名1排序，再根据列名2排序。 默认升序，若想降序则在列名后跟DESC关键字，并且DESC只作用于其所跟的列。 SELECT 列名 FROM 表名 ORDER BY 列1 DESC, 列2; 有时需要对数据进行排名，下面介绍常见的三个窗口函数ROW_NUMBER()``RANK()``DENSE_RANK()。其中PARTITION表示分组标识，ORDER BY表示顺序标识。 ROW_NUMBER()直接给行编号，有多少行编多少号，如1、2、3、4。 RANK()对于相同值则相同排名，同时跳过排名，如1、1、3、4。 DENSE_RANK()对于相同值则相同排名，但不跳过排名，如1、1、2、3。 ROW_NUMBER() OVER ([PARTITION by col] ORDER BY col [ASC|DESC]) AS col RANK() OVER ([PARTITION by col] ORDER BY col [ASC|DESC]) AS col DENSE_RANK() OVER ([PARTITION by col] ORDER BY col [ASC|DESC]) AS col 返回的结果已经按ORDER BY进行排序 3.2.5 过滤数据 WHERE子句筛选符合条件的记录。 常规操作符 SELECT 列名 FROM 表名 WHERE 列名 = 5; 除了相等是=，其余大小比较符号都符合常用习惯。还有范围操作符BETWEEN X AND Y，表示介于X和Y之间的数值，包括两端。 特别的，返回值为空值NULL的记录：WHERE 列名 IS NULL;。 AND或OR 同样，可使用AND或OR来组合多个条件。 SELECT 列名 FROM 表名 WHERE 列1 = 5 AND 列2 = 4; 默认优先处理AND关键字，必要时可用括号()将条件进行分组 IN IN如同R里的%in%，筛选值是否在后续条件中，条件用圆括号包围。 SELECT 列名 FROM 表名 WHERE 列1 IN (2001, 2003); NOT NOT为取反操作，可对IN、BETWEEN、EXISTS等关键字取反。如WHERE 列1 NOT IN (2001, 2003); LIKE LIKE并不是操作符，而是谓词，在技术上有所区别，但结果是相同的 顾名思义，LIKE查找与条件值像不像的记录。常与%和_搭配，前者表示匹配任意个字符，后者表示匹配单个字符。 SELECT 列名 FROM 表名 WHERE 列1 LIKE &#39;jet%&#39;; MySQL在配置方式中可设置是否区分大小写 LIKE无法匹配NULL 一般不要把通配符放在pattern的开端，检索速度慢 正则表达式 REGEXP关键字后跟正则表达式。 LIKE是针对整个列值，而REGEXP则适合列值内，若LIKE不使用通配符的话，则二者存在差异 REGXEP可用REGEXP BINARY来实现区分大小写的功能，默认不区分大小写 |表示“或” [单个字符集]与[^单个字符集]表示匹配其中一个字符或否定 [1-9]或[a-z]表示某个范围的单个字符集 转义符\\\\ 事先预定的字符集，如[:alnum:]表示任意字母和数值 .、*、+、?、{n}、{n,}、{n,m} ^、$、[[:&lt;:]]、[[:&gt;:]] 可使用SELECT '字符串' REGEXP 正则表达式来简单检验正则表达式是否正确 3.2.6 计算字段 拼接字段 Concat()可用于列与列的拼接，类似unite()，并可用关键字AS设置别名，从而进行索引。 可用Trim()、RTrim()、LTrim()去掉空格 SELECT Concat(RTrim(列1), &#39;_&#39;, LTrim(列2)) AS 列3 FROM 表名; 计算字段 可在选择列时直接对列之间进行计算，如mutate()可直接在列之间进行计算一样，对新的列用AS命名即可。 SELECT 列1 * 列2 AS 列3 FROM 表名; 3.2.7 函数 不同SQL之间的SQL语句差异较小，而函数差异则较大。在使用函数时直接作用在列名上即可。 文本处理函数 字符串长度 LENGTH()与CHAR_LENGTH()略有差异，前者返回字节长度，后者才是真的字符串长度。 日期处理函数 DATEDIFF(date1,date2) 返回date1-date2的天数 数值处理函数 窗口函数 窗口函数先进行分组及排序操作后再在窗口内执行对应的函数。 可以适当了解窗口函数框架规范。 窗口函数并不合并行，保留原始数据的完整性，例如在窗口中使用COUNT函数，则窗口内的每行赋值同一个数值 ROW_NUMBER() 行编号 RANK() 排名跳号 DENSE_RANK() 排名不跳号 LAG(expression, offset, default_value) offest表示返回前多少行，默认为1；default_value表示若超出范围时的默认值，默认为NULL LEAD(expression, offset, default_value) offest表示返回后多少行，默认为1；default_value表示若超出范围时的默认值，默认为NULL FIRST_VALUE(expression) 输出按指定顺序排列后的第一个值 注意，若想查询分组后的第一个值，必要时需在主键前添加DISTINCT LAST_VALUE(expression) 其他函数 IFNULL(expression, alt_value) NULL值替换，若为NULL值则替换为alt_value IF(condition, value_if_true, value_if_false) IF函数可以SUM函数结合起来用于计数 3.2.8 汇总数据 下面提到的汇总函数也可作为窗口函数使用。 AVG() 求某列均值，以列名为输入值，仅能输入一个列，并且忽略NULL。 能够处理布尔表达式，如AVG(c.action = 'confirmed') COUNT() 对行数进行计数，若为COUNT(*)则无论是否有NULL都算进去，若为COUNT(列)则忽视NULL值。 MAX() MIN() SUM() 可根据算术符对多个列进行求和，如SUM(列1 * 列2)，忽略NULL。 有时可与DISTINCE关键字结合起来，如AVG(DISTINCT 列1) 3.2.9 数据分组 GROUP BY子句进行分组，可同时对多列进行分组，并在最后规定的分组上汇总。 在使用GROUP BY时，SELECT中的每个列（除了聚集计算语句外）都得在GROUP BY中给出（若是表达式则不能使用别名，而是指定相同的表达式）。 否则会按列的默认顺序返回对应的函数，从而导致“记录”的值不匹配 SELECT 列1, COUNT(*) AS 列 FROM 表名 GROUP BY 列1; GROUP BY 列 WITH ROLLUP将会根据分组顺序逐级向上汇总，在返回的数据中增加汇总行 若要对分组进行筛选，可使用HAVING子句。与WHERE子句的区别在于，WHERE是对行进行过滤，而HAVING可以过滤分组。 SELECT 列1, COUNT(*) AS 列 FROM 表名 WHERE 列2 &gt;= 10 GROUP BY 列1 HAVING COUNT(*) &gt;= 2; ORDER BY也能对分组后的数据进行排序。 3.2.10 子查询 所谓子查询，也就是SELECT语句的嵌套（用圆括号包围），将内层SELECT结构的结果作为条件输出给外层SELECT结构进行查询。 过滤 子查询作为条件传给外层的WHERE子句。 SELECT 列1 FROM 表1 WHERE 列2 IN (SELECT 列3 FROM 表2 WHERE 列4 = &#39;xxx&#39;); 除了IN，还可以与其他操作符结合在一起 计算字段 除了作为过滤条件外，还可用于计算字段传给外层的SELECT子句。 SELECT 列1, (SELECT COUNT(*) FROM 表2 WHERE 表2.列2 = 表1.列3) AS 列4 FROM 表1 ORDER BY 列1; 注意对于同名列需要区分表来源，这种情形也称之为相关子查询 3.2.11 表联结 主键，即某表中每条记录的唯一标识符，如ID。外键，即某表包含另一个表的主键的那一列，该列定义了两个表之间的关系。 为了简洁，可在FROM子句中用AS关键字为表取别名。 也可多次使用联结，并且每次联结的类型可以不同。 联结时除了匹配主键或外键，还可以用其他条件来联结，如ON a.player_id = t.player_id AND DATEDIFF(a.event_date, t.first_date) = 1 WHERE联结 在WHERE子句中添加两个表之间的配对关系（主键与外键），从而实现表联结。且该联结方式为内部联结。 SELECT 列1, 列2 FROM 表1, 表2 WHERE 表1.主键 = 表2.外键 ORDER BY 列1, 列2; 若想得到两个表之间的笛卡尔积（也就是全联结），则去掉WHERE子句即可 内部联结 内部联结INNER JOIN取表的交集。 SELECT 列1, 列2, 列3 FROM 表1 INNER JOIN 表2 ON 表1.主键 = 表2.外键 INNER JOIN 表3 ON 表1.外键 = 表3.主键; 建议用INNER JOIN替代WHERE联结 自联结 在自联结中，需要使用表别名。自联结的多张表中其中一张表作为主表，用于展示查询结果，其余表作为副表，用于提供查询条件。 SELECT t1.列1, t1.列2 FROM 表1 AS t1, 表1 AS t2 WHERE t1.列3 = t2.列3 AND t2.列4 = &#39;xxx&#39;; 这里通过列3来将t1和t2进行匹配，并通过t2.列4进行筛选，从而筛选出t1中符合条件的列 自然联结 自然联结排除列的多次出现，使每个列只返回一次。因此，只需要选中主表的所有列（SELECT 主表.*），其余副表中的列明确指名即可。 外部联结 外部联结包括左联结LEFT OUTER JOIN和右联结RIGHT OUTER JOIN，二者本质上是互通的。 左联结就是以LEFT OUTER JOIN左边的表为准，右联结则以RIGHT OUTER JOIN右边的表为准 SELECT 表1.列1, 表2.列2 FROM 表1 LEFT OUTER JOIN 表2 ON 表1.列1 = 表2.列1 3.2.12 组合查询 组合查询，就是将多条SELECT语句的结果整合到一起，作为单个查询结果返回。用关键字UNION连接多个SELECT语句。 组合查询时每个查询必须包含相同的列、表达式或聚集函数 组合查询即可用于对一张表的多次查询，也可用于在单次查询中从不同表返回类似结构的数据。 前一种情形可用多个WHERE子句实现相同效果 SELECT 列1, 列2 FROM 表1 WHERE 列2 &lt;= 5 UNION SELECT 列1, 列2 FROM 表1 WHERE 列3 IN (xxx,yyy); 在多次查询中，对与重复的行UNION会默认省去，使用UNION ALL关键字可返回所有匹配的行。 在对组合查询结果排序时只能在最后一条SELECT语句后面添加ORDER BY子句 3.2.13 全文本搜索 并非所有引擎都支持全文本搜索，MyISAM引擎支持全文本搜索，而InnoDB引擎则不支持 要实现全文本搜索，需要在创建表的时候将某些列指定为FULLTEXT，这样就能在后续使用Match()与Against()函数来进行搜索。Match()指定被搜索的列，该列必须在指定的FULLTEXT列中，若指定多列，则必须列出它们并且顺序与FULLTEXT中的一致。Against()指定要搜索的表达式，若要区分大小写则使用BINARY关键字。 不要在导入数据时使用FULLTEXT，而是等导入所有数据后再修改表，定义FULLTEXT SELECT 列1 FROM 表名 WHERE Match(列1) Against(&#39;xxx&#39;); 被搜索的表达式越早出现，则其在搜索结果中越靠前 查询扩展 使用全文本搜索时只能找到包含目标表达式的行，而使用查询扩展则会在全文本搜索的基础上找出与目标较为相似（即使没有目标表达式，但出现其他共同的词汇）的其他行。 SELECT 列1 FROM 表名 WHERE Match(列1) Against(&#39;xxx&#39; WITH QUERY EXPANSION); 布尔文本搜索 能更为精细地控制搜索模式，请详见原文。 3.2.14 插入数据 插入记录 使用INSERT子句插入记录。内容可填充为NULL， INSERT INTO 表名 ( 列名 ) VALUES( NULL, &#39;xxx&#39;, ... ); 在填充时如果不给出列名（也就是表名后的圆括号），则按列的默认顺序依次填充。 对于被定义为主键的列，可以不用为其填充，MySQL会自动填充 若要插入多行记录，则确保值的顺序与列的顺序一致，多行记录的值用圆括号括起来并用逗号隔开。 INSERT INTO 表名 ( 列名 ) VALUES( NULL, &#39;xxx&#39;, ... ), ( NULL, &#39;xxx&#39;, ... ); 插入检索出的数据 可以先用SELECT检索出数据，再把检索出的数据插入到另一个表中。 INSERT INTO 表1 ( ... ) SELECT ... FROM 表2; 注意SELECT子句中的列名不一定要与表1中的列名一样，在执行INSERT SELECT结构时仅关心SELECT子句中的位置关系，根据对应位置进行填充。 如果其中包含了主键列，你可以省略该列，MySQL会自动增量 SELECT子句也可包含WHERE子句 3.2.15 更新数据 更新 UPDATE语句用于更新数据。更新规则为“列 = 值”的形式，不同列之间用逗号隔开，利用WHERE筛选出要更新的行。 UPDATE 表名 SET 列1 = &#39;xxx&#39;, 列2 = &#39;yyy&#39; WHERE 列3 = &#39;zzz&#39;; 在更新过程中可能会报错，默认会恢复到原值，如果想无视错误而更新一部分的话，可使用UPDATE IGNORE语句 如果想删除某个列的值，可设置它为NULL 删除 DELETE语句删除行。 DELETE FROM 表名 WHERE 列 = &#39;...&#39;; 根据WHERE子句删除特定行，若没有WHERE则删除所有行，但不意味着删除该表。 3.2.16 表的操作 创建表 利用CREATE TABLE创建表。 CREATE TABLE 表名 IF NOT EXISTS ( 主键1 int NOT NULL AUTO_INCREMENT, 主键2 int NOT NULL, 列1 char(50) NOT NULL DEFAULT &#39;xxx&#39;, 列2 char (5) NULL, PRIMARY KEY (主键1, 主键2) ) ENGINE = InnoDB; 创建表时，需给出列名及其对应的数据类型。可根据PRIMARY KEY来指定主键，主键可设置自动增量AUTO_INCREMENT。 每个表只有一列允许AUTO_INCREMENT 可根据SELECT last_insert_id()来返回最后一个AUTO_INCREMENT值 可用DEFAULT来设置默认值 InnoDB引擎支持事务处理，但不支持全文本搜索；MEMORY速度快，适合临时表；MyISAM支持全文本搜索，但不支持事务处理 更新表 当表中存储数据以后，不建议再更新，因此在创建表时就得深思熟虑 使用ALTER TABLE语句进行更改表操作。对单个表进行多次更改时，可用逗号隔开每次更改。 ALTER TABLE 表名 ADD 列名 CHAR(20); ALTER TABLE 表名 DROP COLUMN 列名; ALTER TABLE 表1 ADD CONSTRAINT 外键约束名 FOREIGN KEY (外键字段名) REFERENCES 表2 (表2主键); 外键字段名是表中已经存在的列名，外键约束名则是该外键在整个数据库中的唯一标识符 删除表 DROP TABLE 表名; 重命名表 RENAME TABLE 表1 TO 新表1, 表2 TO 新表2; 3.2.17 使用视图 视图(view)就是一张虚拟表，既可又一张表构成，又可结合多个表的数据。 创建视图 CREATE VIEW 视图名 AS SELECT 列1, 列2, 列3 FROM 表1, 表2, 表3 WHERE 表1.主键 = 表2.表1外键 AND 表3.主键 = 表2.表3外键 AS前面的就是视图名 在创建好视图后，即可对视图进行一般的查询操作。 一般视图用于检索数据而非更新，故不介绍视图的更新操作 3.2.18 使用存储过程 存储过程类似自定义函数，你不一定有创建存储过程的权限，但可能会使用到它 游标是包含在存储过程中的一个概念，故不介绍 使用CALL关键字来调用存储过程。 CALL 存储过程名 (@参数1, @参数2, @参数3); 3.2.19 触发器 创建触发器也是需要安全访问权限，故了解即可 所谓触发器，就是在特定语句（仅限DELETE、INSERT、UPDATE）发生之前或之后自动执行的行为。 创建触发器 CREATE TRIGGER 触发器名 AFTER INSERT ON 表名 FOR EACH ROW 操作; CREATE TRIGGER关键字来创建触发器，AFTER INSERT定义触发器在INSERT语句发生后触发，第二行就是针对每个被插入行的具体操作了，具体操作可到时视情况而定。 删除触发器 DROP TRIGGER 触发器名; 使用触发器 INSERT触发器 在INSERT触发器内部可引用一个名为NEW的虚拟表，用于访问被插入的行。 CREATE TRIGGER 触发器名 AFTER INSERT ON 表名 FOR EACH ROW SELECT NEW.列1; 该代码表明在每次插入新的行后，返回这些行的列1。 DELETE触发器 在DELETE触发器内部可引用一个名为OLD的虚拟表，用于访问要被删除的行。 CREATE TRIGGER 触发器名 BEFORE DELETE ON 表名 FOR EACH ROW BEGIN ... END; 该代码定义了在DELETE语句前要执行的行为，具体行为被包含在BEGIN END块中，这使得能够该触发器能够容纳多条SQL语句。 UPDATE触发器 在UPDATE触发器内部可引用OLD和NEW的虚拟表，用于访问被插入与被删除的行。 CREATE TRIGGER 触发器名 BEFORE UPDATE ON 表名 FOR EACH ROW 操作; 3.2.20 事务处理 事务处理一种机制，确保某些操作应当成批执行，或者不执行，从而确保结果的完整性与过程的一致性。 事务指的是一组SQL语句 事务处理用来管理INSERT、UPDATE、DELETE语句 事务处理以START TRANSACTION关键字开头。 回退 ROLLBACK关键词撤销前面的所有操作，表的状态会回到START TRANSACTION前的状态。 START TRANSACTION; 操作; ROLLBACK; 提交 COMMIT是在前述所有操作成功的前提下才会执行，得到最终结果。 START TRANSACTION; 操作; COMMIT; 保留点 保留点相当于存档点，有时候不一定要回退到最开始的状态，可以中途定义一些保留点，然后回退到保留点即可。 START TRANSACTION; 操作; SAVEPOINT 保留点1; 操作; ROLLBACK TO 保留点1; "],["spider.html", "4 爬虫 ", " 4 爬虫 "],["spider_1.html", "4.1 提要", " 4.1 提要 robots.txt "],["shiny.html", "5 Shiny", " 5 Shiny Shiny是R中用于构建网页应用程序的一个框架，能够为用户呈现交互式数据分析结果。无需HTML、CSS或者JavaScript等知识，你就可以基于R代码搭建出任何人都可见的数据可视化网页。 本章节内容基于Mastering Shiny。 同时大家也可以参考shiny官网了解更多学习资料。 "],["shiny_1.html", "5.1 初始Shiny", " 5.1 初始Shiny 下面是一段demo。 library(shiny) ui &lt;- fluidPage( selectInput(&quot;dataset&quot;, label = &quot;Dataset&quot;, choices = ls(&quot;package:datasets&quot;)), verbatimTextOutput(&quot;summary&quot;), tableOutput(&quot;table&quot;) ) server &lt;- function(input, output, session) { # Create a reactive expression dataset &lt;- reactive({ get(input$dataset, &quot;package:datasets&quot;) }) output$summary &lt;- renderPrint({ # Use a reactive expression by calling it like a function summary(dataset()) }) output$table &lt;- renderTable({ dataset() }) } shinyApp(ui, server) 由四部分组成：导包、ui、server、以及shinyApp。 其中ui用于网页的ui设计，server用于背后的数据分析，shinyApp用于搭建shiny应用。 那么该如何创建shiny文件呢？ 新建脚本文件，完成代码后保存文件，脚本文件会自动识别出这是一个shiny应用，此时你的工具栏中的Run按钮会变为Run App File -&gt; New File -&gt; Shiny Web App File -&gt; New Project -&gt; New Directory -&gt; Shiny Application 至此，你可点击Run App按钮运行文件，或者使用Ctrl+Shift+Enter运行。 注意到，当你运行shiny应用时会弹出一个窗口，并且控制台处会出现类似Listening on http://127.0.0.1:4674的反馈。你可以直接将该URL复制到你的网页浏览器中，这样别人也可以看到你的应用，当然前提是你的shiny应用仍在运行中。 shiny应用是一直在运行的，你可以通过控制台右上角的红色停止按钮判断。当你点击该按钮，或者关闭shiny应用窗口时，方可停止。在停止之前，你不能在控制台中执行任何新命令。 "],["shiny_2.html", "5.2 UI设计", " 5.2 UI设计 更多的输入与输出组件参见shiny官网。 5.2.1 输入 sliderInput(&quot;min&quot;, &quot;Limit (minimum)&quot;, value = 50, min = 0, max = 100) 对于形如selectInput()的函数，都是将你的信息传递给server的组件。这类组件的基本参数为ID、标签及其余参数。其中ID是组件的唯一标识符，用于后续调用（这里的ID是min，后续在server函数中通过input$min来调用该组件的输入值）。标签是该组件呈现给用户时所用的标签（这里的标签为Limit (minimum)，也就是页面中滑条的标签）。其余参数往往是与组件类型相关的参数（这里value=50表示默认值，min与max则是滑条的上下限）。 ID是一个非常重要的概念，大多数的对象都可以设置ID，这样在server处就能够通过input$id来获取对应的输入 ui &lt;- fluidPage( textInput(&quot;name&quot;, &quot;What&#39;s your name?&quot;), passwordInput(&quot;password&quot;, &quot;What&#39;s your password?&quot;), textAreaInput(&quot;story&quot;, &quot;Tell me about yourself&quot;, rows = 3) ) 5.2.1.1 文本类组件 textInput() 适合单行文本输入 passwordInput() 适合密码类文本输入 textAreaInput() 适合多行文本输入 5.2.1.2 数值类组件 numericInput() 输入单个数值 sliderInput() 滑块选取数值。特别的，当默认值是一个长度为2的向量时，滑块条变成范围取值（双向），而非单个数值（单向） 5.2.1.3 日期类组件 dateInput() 输入日期 dateRangeInput() 输入日期范围 5.2.1.4 选择类组件 selectInput() 下拉式选择，可多选 selectizeInput() selectInput()的增强版，适合更为复杂、大量选择的输入场景 radioButtons() 按钮式选择，只能单选 checkboxGroupInput() 勾选式选择，可单选可多选 checkboxInput() 勾选式选择，只能单选 注意参数choiceNames表示呈现给用户的选项名称，参数choiceValues表示传递给服务器端的实际值，二者一一对应 5.2.1.5 文件传输类组件 fileInput() 将用户的文件传输到服务器端 5.2.1.6 按钮类组件 actionButton() 通过点击传输信息。其中可根据class参数更改按钮的外观，详见此处 5.2.2 输出 textOutput(&quot;text&quot;) 对于形如textOutput()的函数，用于接收服务器输出的结果。同理，输出的内容需要由唯一标识符ID来进行匹配。当输出的结果在服务器端中用output$ID完成赋值，在ui中的输出函数中即可输入相应的ID来调用结果。 ui &lt;- fluidPage( textOutput(&quot;text&quot;), verbatimTextOutput(&quot;code&quot;) ) server &lt;- function(input, output, session) { output$text &lt;- renderText({ &quot;Hello friend!&quot; }) output$code &lt;- renderPrint({ summary(1:10) }) } 5.2.2.1 文本类输出 textOutput() 用于一般的文本输出，常与渲染函数renderText()搭配 verbatimTextOutput() 用于代码结果的输出，常与渲染函数renderPrint()搭配 注意到，如果在渲染函数render中需要执行多行代码的话，则需要{}进行包裹 5.2.2.2 表格类输出 tableOutput() 输出静态表格，一次性展示完所有数据，常与渲染函数renderTable()搭配 dataTableOutput() 输出动态表格，可进行翻页，常与渲染函数renderDataTable()搭配 dataTableOutput()被弃用了，可以试试DT::DTOutput() 5.2.2.3 图像类输出 plotOutput() 输出图像，常与渲染函数renderPlot搭配 建议设置renderPlot(res = 96) 5.2.2.4 下载文件 downloadButton() downloadLink() "],["shiny_3.html", "5.3 反应式编程", " 5.3 反应式编程 反应式编程，简而言之，就是当输入变化时，所有相关的输出将会实时更新，而其余无关的输出则保持原状。因此，当你运行shiny程序时它并不会立即执行内部的代码，内部代码仅仅声明了处理的逻辑，是否运行以及何时运行都取决于shiny，也就是说，shiny程序是懒惰的。 shiny的编程风格属于声明式风格，并不像常规代码按照前后顺序运行，而是根据内部的逻辑链运行 5.3.1 服务端 回忆shiny应用程序的一般形式 library(shiny) ui &lt;- fluidPage( # front end interface ) server &lt;- function(input, output, session) { # back end logic } shinyApp(ui, server) ui表示交互界面，呈现给每个用户的内容是相同的。server表示服务器端，由于每个用户输入的信息不尽相同，因此shiny程序在每次创建新会话(session)时都会独立地激活server()。 5.3.1.1 输入 对于ui中的输入input，会将其存储为类似列表的对象，每个元素的名称都是ui中相应的ID，在server中可通过input$ID来调用输入值。 注意input不能修改，只能读取 5.3.1.2 输出 同input，output也是类似列表的对象。对于你想输出的内容可通过output$ID的赋予其唯一标识符，然后在ui中的输出函数中根据ID进行调用。 注意output$ID赋值时一定要搭配渲染函数`render`` 5.3.2 反应表达式 反应表达式创建了一种依赖关系，当且仅当输入变化时才会更新信息，并可重复使用，简化代码。它同时具有input与output的特点：作为信息更新后的output，作为渲染函数的input。 如果大于1次使用，都应考虑使用反应表达式 下面对两个正态分布数据进行模拟，比较这两段代码（仅在server部分有差异）： library(shiny) library(ggplot2) freqpoly &lt;- function(x1, x2, binwidth = 0.1, xlim = c(-3, 3)) { df &lt;- data.frame( x = c(x1, x2), g = c(rep(&quot;x1&quot;, length(x1)), rep(&quot;x2&quot;, length(x2))) ) ggplot(df, aes(x, colour = g)) + geom_freqpoly(binwidth = binwidth, size = 1) + coord_cartesian(xlim = xlim) } t_test &lt;- function(x1, x2) { test &lt;- t.test(x1, x2) # use sprintf() to format t.test() results compactly sprintf( &quot;p value: %0.3f\\n[%0.2f, %0.2f]&quot;, test$p.value, test$conf.int[1], test$conf.int[2] ) } ui &lt;- fluidPage( fluidRow( column(4, &quot;Distribution 1&quot;, numericInput(&quot;n1&quot;, label = &quot;n&quot;, value = 1000, min = 1), numericInput(&quot;mean1&quot;, label = &quot;µ&quot;, value = 0, step = 0.1), numericInput(&quot;sd1&quot;, label = &quot;σ&quot;, value = 0.5, min = 0.1, step = 0.1) ), column(4, &quot;Distribution 2&quot;, numericInput(&quot;n2&quot;, label = &quot;n&quot;, value = 1000, min = 1), numericInput(&quot;mean2&quot;, label = &quot;µ&quot;, value = 0, step = 0.1), numericInput(&quot;sd2&quot;, label = &quot;σ&quot;, value = 0.5, min = 0.1, step = 0.1) ), column(4, &quot;Frequency polygon&quot;, numericInput(&quot;binwidth&quot;, label = &quot;Bin width&quot;, value = 0.1, step = 0.1), sliderInput(&quot;range&quot;, label = &quot;range&quot;, value = c(-3, 3), min = -5, max = 5) ) ), fluidRow( column(9, plotOutput(&quot;hist&quot;)), column(3, verbatimTextOutput(&quot;ttest&quot;)) ) ) server &lt;- function(input, output, session) { output$hist &lt;- renderPlot({ x1 &lt;- rnorm(input$n1, input$mean1, input$sd1) x2 &lt;- rnorm(input$n2, input$mean2, input$sd2) freqpoly(x1, x2, binwidth = input$binwidth, xlim = input$range) }, res = 96) output$ttest &lt;- renderText({ x1 &lt;- rnorm(input$n1, input$mean1, input$sd1) x2 &lt;- rnorm(input$n2, input$mean2, input$sd2) t_test(x1, x2) }) } shinyApp(ui,server) library(shiny) library(ggplot2) freqpoly &lt;- function(x1, x2, binwidth = 0.1, xlim = c(-3, 3)) { df &lt;- data.frame( x = c(x1, x2), g = c(rep(&quot;x1&quot;, length(x1)), rep(&quot;x2&quot;, length(x2))) ) ggplot(df, aes(x, colour = g)) + geom_freqpoly(binwidth = binwidth, size = 1) + coord_cartesian(xlim = xlim) } t_test &lt;- function(x1, x2) { test &lt;- t.test(x1, x2) # use sprintf() to format t.test() results compactly sprintf( &quot;p value: %0.3f\\n[%0.2f, %0.2f]&quot;, test$p.value, test$conf.int[1], test$conf.int[2] ) } ui &lt;- fluidPage( fluidRow( column(4, &quot;Distribution 1&quot;, numericInput(&quot;n1&quot;, label = &quot;n&quot;, value = 1000, min = 1), numericInput(&quot;mean1&quot;, label = &quot;µ&quot;, value = 0, step = 0.1), numericInput(&quot;sd1&quot;, label = &quot;σ&quot;, value = 0.5, min = 0.1, step = 0.1) ), column(4, &quot;Distribution 2&quot;, numericInput(&quot;n2&quot;, label = &quot;n&quot;, value = 1000, min = 1), numericInput(&quot;mean2&quot;, label = &quot;µ&quot;, value = 0, step = 0.1), numericInput(&quot;sd2&quot;, label = &quot;σ&quot;, value = 0.5, min = 0.1, step = 0.1) ), column(4, &quot;Frequency polygon&quot;, numericInput(&quot;binwidth&quot;, label = &quot;Bin width&quot;, value = 0.1, step = 0.1), sliderInput(&quot;range&quot;, label = &quot;range&quot;, value = c(-3, 3), min = -5, max = 5) ) ), fluidRow( column(9, plotOutput(&quot;hist&quot;)), column(3, verbatimTextOutput(&quot;ttest&quot;)) ) ) server &lt;- function(input, output, session) { x1 &lt;- reactive(rnorm(input$n1, input$mean1, input$sd1)) x2 &lt;- reactive(rnorm(input$n2, input$mean2, input$sd2)) output$hist &lt;- renderPlot({ freqpoly(x1(), x2(), binwidth = input$binwidth, xlim = input$range) }, res = 96) output$ttest &lt;- renderText({ t_test(x1(), x2()) }) } shinyApp(ui,server) 在第一段代码中，shiny会把输出看成一个整体，即使你只改变第一个分布的参数，那么在运行时shiny也会重新对这两个分布进行抽样。而我们的本意是对第一个分布重新抽样，保持第二个分布的数据不变。 在第二段代码中，抽样过程都被包含在反应表达式reactive()中，这样shiny仅会在对应输入更新时运行此处的代码，否则保持原始数据。也就是说，反应表达式是一个模块化单元，就像平时使用的函数一样。 在使用反应表达式时，除了要有reactive()，在其他地方调用对象时还得有()，正如这里的x1()、x2() 5.3.3 控制更新 5.3.3.1 自动更新 我们除了在更改输入时想更新数据外，有时还想在同一输入下进行多次更新（如果可以的话）。例如在同一分布下（意味着输入不变），我们想看看多次抽样的结果（重复抽样操作）。这时，可以利用reactiveTimer(interval)，根据时间间隔interval来周期性地触发响应式更新的计时器函数，依赖于它的响应式表达式/输出按固定时间间隔自动重新计算，非常适合实现实时数据刷新、动态仪表盘等功能。 interval以毫秒为单位， 1s=1000ms server &lt;- function(input, output, session) { timer &lt;- reactiveTimer(500) x1 &lt;- reactive({ timer() rpois(input$n, input$lambda1) }) x2 &lt;- reactive({ timer() rpois(input$n, input$lambda2) }) output$hist &lt;- renderPlot({ freqpoly(x1(), x2(), binwidth = 1, xlim = c(0, 40)) }, res = 96) } 这里创建了500ms的计时器，并在x1和x2中添加了计时器，这使得在1s中就会更新两次x1和x2，并重新绘图。 5.3.3.2 手动更新 或许我们也想在同一输入下，手动点击按钮来进行一次更新，这时引入actionButton()。 library(shiny) library(ggplot2) freqpoly &lt;- function(x1, x2, binwidth = 0.1, xlim = c(-3, 3)) { df &lt;- data.frame( x = c(x1, x2), g = c(rep(&quot;x1&quot;, length(x1)), rep(&quot;x2&quot;, length(x2))) ) ggplot(df, aes(x, colour = g)) + geom_freqpoly(binwidth = binwidth, size = 1) + coord_cartesian(xlim = xlim) } ui &lt;- fluidPage( fluidRow( column(3, numericInput(&quot;lambda1&quot;, label = &quot;lambda1&quot;, value = 3), numericInput(&quot;lambda2&quot;, label = &quot;lambda2&quot;, value = 5), numericInput(&quot;n&quot;, label = &quot;n&quot;, value = 1e4, min = 0), actionButton(&quot;simulate&quot;, &quot;Simulate!&quot;) ), column(9, plotOutput(&quot;hist&quot;)) ) ) server &lt;- function(input, output, session) { x1 &lt;- reactive({ input$simulate rpois(input$n, input$lambda1) }) x2 &lt;- reactive({ input$simulate rpois(input$n, input$lambda2) }) output$hist &lt;- renderPlot({ freqpoly(x1(), x2(), binwidth = 1, xlim = c(0, 40)) }, res = 96) } shinyApp(ui,server) 此处利用actionButton()创建了一个ID为”simulate”，标签为”Simulate!“的按钮，并在x1和x2处的响应表达式中添加了input$simulate，这使得在每次点击时都能重新更新数据。 但这样的编程结果会创造两种依赖关系：x1和x2既依赖输入参数，又依赖按钮。当上述两种源头变化时，数据都会更新。倘若我们只想在点击按钮时更新数据（即使参数已经发生了变化），这里引入eventReactive()。eventReactive()有两个参数，第一个参数指定与谁创建依赖关系，第二个参数表示需要计算的内容。 需要计算的内容用{}包裹 server &lt;- function(input, output, session) { x1 &lt;- eventReactive(input$simulate, { rpois(input$n, input$lambda1) }) x2 &lt;- eventReactive(input$simulate, { rpois(input$n, input$lambda2) }) output$hist &lt;- renderPlot({ freqpoly(x1(), x2(), binwidth = 1, xlim = c(0, 40)) }, res = 96) } 这里的x1和x2仅会在点击按钮时更新一次数据。 5.3.4 信息反馈 现在介绍新的反应表达式——observeEvent()——用于监控特定事件并执行响应操作。 observeEvent()的第一个参数为依赖对象，第二个参数为要执行的代码块。与eventReactive()不同，observeEvent()并没有返回值。 ui &lt;- fluidPage( textInput(&quot;name&quot;, &quot;What&#39;s your name?&quot;), textOutput(&quot;greeting&quot;) ) server &lt;- function(input, output, session) { string &lt;- reactive(paste0(&quot;Hello &quot;, input$name, &quot;!&quot;)) output$greeting &lt;- renderText(string()) observeEvent(input$name, { message(paste0(&quot;Greeting performed: &quot;, input$name)) }) } 这里的observeEvent()监控input$name的变化。 "],["shiny_4.html", "5.4 页面布局", " 5.4 页面布局 5.4.1 页面函数 fluidPage() 用于创建流畅的页面布局。页面由行与列组成。行的存在是为了控制元素出现在同一行上，列的存在是为了定义元素在12单位宽的页面中占据多少宽度。 fluidRow()函数控制每一行的内容 fixedPage() 固定页面在典型显示器上的宽度限制为940像素，在较小和较大的显示器上分别为724像素或1170像素。 fixedRow()函数控制每一行的内容 fillPage() 尽可能地创建出填满浏览器窗口的页面 5.4.2 布局函数 布局函数就是控制一个页面中各个元素的布局 5.4.2.1 单页布局 下面就简单介绍经典布局——侧边栏布局sidebarLayout，常与标题栏titlePanel()搭配 library(shiny) ui &lt;- fluidPage( titlePanel(&quot;Central limit theorem&quot;), sidebarLayout( sidebarPanel( numericInput(&quot;m&quot;, &quot;Number of samples:&quot;, 2, min = 1, max = 100) ), mainPanel( plotOutput(&quot;hist&quot;) ) ) ) server &lt;- function(input, output, session) { output$hist &lt;- renderPlot({ means &lt;- replicate(1e4, mean(runif(input$m))) hist(means, breaks = 20) }, res = 96) } shinyApp(ui,server) 其基本布局为“标题栏+侧边栏+主面板”。 5.4.2.2 多页布局 标签卡式布局 tabsetPanel()创建标签卡这个整体，tabPanel()创建子标签卡的内容 ui &lt;- fluidPage( sidebarLayout( sidebarPanel( textOutput(&quot;panel&quot;) ), mainPanel( tabsetPanel( id = &quot;tabset&quot;, tabPanel(&quot;panel 1&quot;, &quot;one&quot;), tabPanel(&quot;panel 2&quot;, &quot;two&quot;), tabPanel(&quot;panel 3&quot;, &quot;three&quot;) ) ) ) ) server &lt;- function(input, output, session) { output$panel &lt;- renderText({ paste(&quot;Current panel: &quot;, input$tabset) }) } tabsetPanel()的参数id用于返回当前选中的子标签卡的名称 导航栏式布局 第一个布局函数是navlistPanel，可以说是tabsetPanel()的侧边形式 ui &lt;- fluidPage( navlistPanel( id = &quot;tabset&quot;, &quot;Heading 1&quot;, tabPanel(&quot;panel 1&quot;, &quot;Panel one contents&quot;), &quot;Heading 2&quot;, tabPanel(&quot;panel 2&quot;, &quot;Panel two contents&quot;), tabPanel(&quot;panel 3&quot;, &quot;Panel three contents&quot;) ) ) 注意Heaning 1与Heading 2类似侧边栏中的小标题 第二个布局函数是navbarPage()与navbarMenu()。前者控制导航栏置于顶部，后者生成一个下拉式菜单，里面包含了多个子标签卡 ui &lt;- navbarPage( &quot;Page title&quot;, tabPanel(&quot;panel 1&quot;, &quot;one&quot;), tabPanel(&quot;panel 2&quot;, &quot;two&quot;), tabPanel(&quot;panel 3&quot;, &quot;three&quot;), navbarMenu(&quot;subpanels&quot;, tabPanel(&quot;panel 4a&quot;, &quot;four-a&quot;), tabPanel(&quot;panel 4b&quot;, &quot;four-b&quot;), tabPanel(&quot;panel 4c&quot;, &quot;four-c&quot;) ) ) 5.4.3 bslib风格布局 除此之外，官网Shiny Layouts中还展示了基于bslib包的布局函数，个人认为更加美观与便捷。 使用时记得导入library(bslib) Navbars 导航栏式布局，导航栏被置于顶部或底部 Sidebars 经典侧边栏布局 Tabs 标签卡式布局 Panels &amp; Cards Panels不知道算什么风格，Cards就是在页面上呈现卡片式窗口 Arrange Elements 自定义布局 "],["shiny_5.html", "5.5 主题", " 5.5 主题 在页面函数中都会有参数theme，你可根据bslib::bs_theme()函数传入预制的主题。 ui &lt;- fluidPage( theme = bslib::bs_theme(bootswatch = &quot;darkly&quot;), ... ) 或者可自定义主题 theme &lt;- bslib::bs_theme( bg = &quot;#0b3d91&quot;, fg = &quot;white&quot;, base_font = &quot;Source Sans Pro&quot; ) 除了页面主题，输出的图像也能凭借thematic::thematic_shiny()设置自适应主题， library(ggplot2) ui &lt;- fluidPage( theme = bslib::bs_theme(bootswatch = &quot;darkly&quot;), titlePanel(&quot;A themed plot&quot;), plotOutput(&quot;plot&quot;), ) server &lt;- function(input, output, session) { thematic::thematic_shiny() output$plot &lt;- renderPlot({ ggplot(mtcars, aes(wt, mpg)) + geom_point() + geom_smooth() }, res = 96) } 更进一步，如果你了解HTML和CSS，那么你完全可以自定义主题，详见此处。 "],["shiny_6.html", "5.6 交互式图像与插入图像", " 5.6 交互式图像与插入图像 手册中介绍了交互式图像的相关知识。但个人觉得还是在shiny中直接使用plotly及相关可交互图像包更为便捷。 5.6.1 交互式图像 5.6.1.1 plotly plotly包中自带renderPlotly()与plotlyOutput()函数，负责在shiny中渲染并输出plotly图像。 这里就简单介绍plotly的用法，具体细节还是等有需要的时候自行查找官方手册。 plotly绘图的两种实现方式： 直接绘图 利用plot_ly()函数初始化对象，在传递参数时得使用=~ 根据不同的图形类型查询对应的官网手册，利用命名列表来传递参数 支持管道符 library(plotly) # volcano是R内置的矩阵 fig &lt;- plot_ly(z = ~volcano) %&gt;% add_surface( contours = list( z = list( show=TRUE, usecolormap=TRUE, highlightcolor=&quot;#ff0000&quot;, project=list(z=TRUE) ) ) ) fig &lt;- fig %&gt;% layout( scene = list( camera=list( eye = list(x=1.87, y=0.88, z=-0.64) ) ) ) fig 基于ggplot 直接用ggplot()绘图，再利用ggplotly()函数转化为plotly对象，之后可根据需要继续使用管道符来添加细节 注意，部分ggplot元素无法转化为plotly对象 fig &lt;- ggplot(data=iris)+ geom_point(aes(x=Sepal.Length, y=Sepal.Width, color=Species))+ theme_minimal() ggplotly(fig) 5.6.2 插入图像 使用renderImage()来渲染图片。该渲染函数需要一个提供了图像信息的列表。 output$photo &lt;- renderImage({ list( src = file.path(&quot;puppy-photos&quot;, paste0(input$id, &quot;.jpg&quot;)), contentType = &quot;image/jpeg&quot;, width = 500, height = 650 ) }, deleteFile = FALSE) 参数src指向存储了图像的本地路径；contentType表示图像格式，若已经提供了图像后缀名则会自动识别；由于renderImage()一开始的设计初衷是为了处理临时性文件，因此在渲染后会自动删除图像，因此需声明deleteFile=FALSE来保留图像。 "],["shiny_7.html", "5.7 用户反馈", " 5.7 用户反馈 用户反馈旨在提高用户的使用体验。就像下载文件有个进度条，进入某个页面有个滚动圈，这些都是在提醒用户它们在工作，而非“静止”。 5.7.1 验证 5.7.1.1 输入验证 将shinyFeedback包的useShinyFeedback()函数插入到ui中，并在server里设置相应的反馈函数来提醒用户。 useShinyFeedback()必须置于UI的顶部 仅限如下组件 shiny::dateInput() shiny::dateRangeInput() shiny::fileInput() shiny::numericInput() shiny::passwordInput() shiny::selectInput() shiny::sliderInput() shiny::textAreaInput() shiny::textInput() shinyWidgets::airDatepickerInput() shinyWidgets::pickerInput() 较常使用的反馈函数有feedback()、feedbackWarning()、feedbackDanger()、feedbackSuccess()。它们共有的参数如下所示： inputID：哪个输入要用到反馈，指定相应的ID show：逻辑判断是否要发出提醒 text：提醒的信息 color：提醒的信息颜色 icon：提醒的图标 在FontAwesome中根据需要寻找图标，找到对应的标签，如file，最后传递给参数icon=icon(\"file\")即可 library(shiny) ui &lt;- fluidPage( shinyFeedback::useShinyFeedback(), numericInput(&quot;n&quot;, &quot;n&quot;, value = 10), textOutput(&quot;half&quot;) ) server &lt;- function(input, output, session) { half &lt;- reactive({ even &lt;- input$n %% 2 == 0 shinyFeedback::feedbackWarning(&quot;n&quot;, !even, &quot;Please select an even number&quot;) #req(even) input$n / 2 }) output$half &lt;- renderText(half()) } # Create Shiny app ---- shinyApp(ui = ui, server = server) 即使输入的信息不是我们期望的，shiny也可能会正常运行。此时，我们需要req()（required）来判断输入的信息是否符合规范，当为FALSE时，反应表达式会在req()处停止，不会接着运行下面的内容。 此外，如果你需要等用户输入所有信息后再更新内容，那么req()也可承接多个条件判断，只有当这些条件都满足时才会运行下面的代码。 下面展示联合使用req()与shinyFeedback的例子。 library(shiny) ui &lt;- fluidPage( shinyFeedback::useShinyFeedback(), textInput(&quot;dataset&quot;, &quot;Dataset name&quot;), tableOutput(&quot;data&quot;) ) server &lt;- function(input, output, session) { data &lt;- reactive({ req(input$dataset) exists &lt;- exists(input$dataset, &quot;package:datasets&quot;) shinyFeedback::feedbackDanger(&quot;dataset&quot;, !exists, &quot;Unknown dataset&quot;) req(exists, cancelOutput = TRUE) get(input$dataset, &quot;package:datasets&quot;) }) output$data &lt;- renderTable({ head(data()) }) } # Create Shiny app ---- shinyApp(ui = ui, server = server) 当且仅当输入了数据集的名称且该数据集存在时，才会完整的运行这个反应表达式。其中req(exists, cancelOutput = TRUE)表示当为FALSE时，取消之后的所有输出，将结果保留在上一次符合规范的结果。 5.7.1.2 输出验证 有时将验证这一环节放在输出这更好。可用validate()来反馈信息。 library(shiny) ui &lt;- fluidPage( numericInput(&quot;x&quot;, &quot;x&quot;, value = 0), selectInput(&quot;trans&quot;, &quot;transformation&quot;, choices = c(&quot;square&quot;, &quot;log&quot;, &quot;square-root&quot;) ), textOutput(&quot;out&quot;) ) server &lt;- function(input, output, session) { output$out &lt;- renderText({ if (input$x &lt; 0 &amp;&amp; input$trans %in% c(&quot;log&quot;, &quot;square-root&quot;)) { validate(&quot;x can not be negative for this transformation&quot;) } switch(input$trans, square = input$x ^ 2, &quot;square-root&quot; = sqrt(input$x), log = log(input$x) ) }) } # Create Shiny app ---- shinyApp(ui = ui, server = server) 5.7.2 通知 有三种通知类型：1.在固定时间后自动消失的通知；2.在进程开始时显示，结束后消失的通知；3.更新式通知。 5.7.2.1 在固定时间后自动消失的通知 使用observeEvent()与showNotification()来监控行为并发出通知。 library(shiny) ui &lt;- fluidPage( actionButton(&quot;goodnight&quot;, &quot;Good night&quot;) ) server &lt;- function(input, output, session) { observeEvent(input$goodnight, { showNotification(&quot;So long&quot;, duration = 5) Sys.sleep(1) showNotification(&quot;Farewell&quot;, type = &quot;message&quot;) Sys.sleep(1) showNotification(&quot;Auf Wiedersehen&quot;, type = &quot;warning&quot;) Sys.sleep(1) showNotification(&quot;Adieu&quot;, type = &quot;error&quot;) }) } # Create Shiny app ---- shinyApp(ui = ui, server = server) 其中duration表示通知的持续时间，type表示通知的类型，也就是颜色的不同。 5.7.2.2 进程通知 对于进程通知，需要将通知的持续时间设置为duration=NULL，关闭通知右上角的关闭按钮closeButton=FALSE，并将其放在反应表达式里，这样才符合“在进程开始时出现，结束时自动消失”的样子。 此外，需要用removeNotification(id)来移除对应ID的通知。on.exit()确保了通知不会因为代码错误而卡住不消失。 on.exit()就是在函数或代码块中预先设置了一个退出处理程序，无论函数或代码块是否正常运行，当我退出时都要执行这里的退出处理程序。而参数add=TRUE表示在现有处理程序列表中添加新的处理程序，而不是覆盖旧的处理程序。另外还有参数after，大家自行了解吧 server &lt;- function(input, output, session) { data &lt;- reactive({ id &lt;- showNotification(&quot;Reading data...&quot;, duration = NULL, closeButton = FALSE) on.exit(removeNotification(id), add = TRUE) read.csv(input$file$datapath) }) } 5.7.2.3 更新式通知 在第一个例子中，我们创建了多条通知，新的通知会把就通知往上顶。而创建一个通知并多次调用它，就会覆盖前一个通知，从而实现更新式通知。 library(shiny) ui &lt;- fluidPage( tableOutput(&quot;data&quot;) ) server &lt;- function(input, output, session) { notify &lt;- function(msg, id = NULL) { showNotification(msg, id = id, duration = NULL, closeButton = FALSE) } data &lt;- reactive({ id &lt;- notify(&quot;Reading data...&quot;) on.exit(removeNotification(id), add = TRUE) Sys.sleep(1) notify(&quot;Reticulating splines...&quot;, id = id) Sys.sleep(1) notify(&quot;Herding llamas...&quot;, id = id) Sys.sleep(1) notify(&quot;Orthogonalizing matrices...&quot;, id = id) Sys.sleep(1) mtcars }) output$data &lt;- renderTable(head(data())) } # Create Shiny app ---- shinyApp(ui = ui, server = server) 这段代码首先创建了ID为id的通知，并为其建立了退出处理机制，在后续的通知中重复调用了同一个ID的通知，从而实现更新式通知。 5.7.3 进度条 设置进度条的技术尚未成熟，已有的方法还存在缺点（你需要将完整任务分割成多个数量已知且运行时间大致相等的小任务）。下面分别介绍shiny内置的进度条与waiter包的进度条。 显然，进度条非常适合在for循环中使用 5.7.3.1 shiny withProgress()函数的message参数用来显示进度条的文本信息，该函数默认进度条跨度为0~1，需要用{}来包裹需要显示进度条的代码块。 incProgress()函数表示每次进度更新的增量，由于默认进度条范围为0~1，因此每次进度更新的增量就是1/input$steps。 library(shiny) ui &lt;- fluidPage( numericInput(&quot;steps&quot;, &quot;How many steps?&quot;, 10), actionButton(&quot;go&quot;, &quot;go&quot;), textOutput(&quot;result&quot;) ) server &lt;- function(input, output, session) { data &lt;- eventReactive(input$go, { withProgress(message = &quot;Computing random number&quot;, { for (i in seq_len(input$steps)) { Sys.sleep(0.5) incProgress(1 / input$steps) } runif(1) }) }) output$result &lt;- renderText(round(data(), 2)) } # Create Shiny app ---- shinyApp(ui = ui, server = server) 5.7.3.2 Waiter 注意Waiter包需要创建一个R6对象，所有相关操作都要调用这个对象来设置。 这里就给几个示例，其余自行探索官网。 library(shiny) ui &lt;- fluidPage( waiter::use_waitress(), numericInput(&quot;steps&quot;, &quot;How many steps?&quot;, 10), actionButton(&quot;go&quot;, &quot;go&quot;), textOutput(&quot;result&quot;) ) server &lt;- function(input, output, session) { data &lt;- eventReactive(input$go, { waitress &lt;- waiter::Waitress$new(max = input$steps) on.exit(waitress$close()) for (i in seq_len(input$steps)) { Sys.sleep(0.5) waitress$inc(1) } runif(1) }) output$result &lt;- renderText(round(data(), 2)) } shinyApp(ui = ui, server = server) library(shiny) library(waiter) ui &lt;- fluidPage( autoWaiter(), actionButton( &quot;trigger&quot;, &quot;Render&quot; ), plotOutput(&quot;plot&quot;), plotOutput(&quot;plot2&quot;) ) server &lt;- function(input, output){ output$plot &lt;- renderPlot({ input$trigger Sys.sleep(3) plot(cars) }) output$plot2 &lt;- renderPlot({ input$trigger Sys.sleep(5) plot(runif(100)) }) } shinyApp(ui, server) library(shiny) ui &lt;- fluidPage( waiter::use_waiter(), actionButton(&quot;go&quot;, &quot;go&quot;), textOutput(&quot;result&quot;) ) server &lt;- function(input, output, session) { data &lt;- eventReactive(input$go, { waiter &lt;- waiter::Waiter$new() waiter$show() on.exit(waiter$hide()) Sys.sleep(sample(5, 1)) runif(1) }) output$result &lt;- renderText(round(data(), 2)) } shinyApp(ui = ui, server = server) 5.7.4 确认 为了防止用户意外做出错误的选择，需要再三确认用户的操作。 5.7.4.1 确认对话框 最简单的办法就是弹出对话框让用户再次确认自己的选择。 modalDialog()函数创建了一个对话框，根据footer参数来设置按钮选项。 library(shiny) modal_confirm &lt;- modalDialog( &quot;Are you sure you want to continue?&quot;, title = &quot;Deleting files&quot;, footer = tagList( actionButton(&quot;cancel&quot;, &quot;Cancel&quot;), actionButton(&quot;ok&quot;, &quot;Delete&quot;, class = &quot;btn btn-danger&quot;) ) ) ui &lt;- fluidPage( actionButton(&quot;delete&quot;, &quot;Delete all files?&quot;) ) server &lt;- function(input, output, session) { observeEvent(input$delete, { showModal(modal_confirm) }) observeEvent(input$ok, { showNotification(&quot;Files deleted&quot;) removeModal() }) observeEvent(input$cancel, { removeModal() }) } shinyApp(ui, server) 注意需要用showModal()来展现对话框，并用remobeModal()来及时移除对话框。 5.7.4.2 取消行为 用户反悔自己的行为需要有一个过程。非常典型的例子就是在网购平台下单后，发货前都能取消订单一样。 下面只展示，不细讲。 library(shiny) ui &lt;- fluidPage( textAreaInput(&quot;message&quot;, label = NULL, placeholder = &quot;What&#39;s happening?&quot;, rows = 3 ), actionButton(&quot;tweet&quot;, &quot;Tweet&quot;) ) runLater &lt;- function(action, seconds = 3) { observeEvent( invalidateLater(seconds * 1000), action, ignoreInit = TRUE, once = TRUE, ignoreNULL = FALSE, autoDestroy = FALSE ) } server &lt;- function(input, output, session) { waiting &lt;- NULL last_message &lt;- NULL observeEvent(input$tweet, { notification &lt;- glue::glue(&quot;Tweeted &#39;{input$message}&#39;&quot;) last_message &lt;&lt;- input$message updateTextAreaInput(session, &quot;message&quot;, value = &quot;&quot;) showNotification( notification, action = actionButton(&quot;undo&quot;, &quot;Undo?&quot;), duration = NULL, closeButton = FALSE, id = &quot;tweeted&quot;, type = &quot;warning&quot; ) waiting &lt;&lt;- runLater({ cat(&quot;Actually sending tweet...\\n&quot;) removeNotification(&quot;tweeted&quot;) }) }) observeEvent(input$undo, { waiting$destroy() showNotification(&quot;Tweet retracted&quot;, id = &quot;tweeted&quot;) updateTextAreaInput(session, &quot;message&quot;, value = last_message) }) } shinyApp(ui, server) "],["shiny_8.html", "5.8 shiny中的整洁式编程", " 5.8 shiny中的整洁式编程 5.8.1 Data-masking 由于shiny通过input来获取输入信息，其中不乏在数据框中需要用到的变量名。但由于不是直接输入对应的变量名，而是依靠input$variable的形式调用，所以会出现意料之外的结果。对此，需要区分环境中的变量与数据框中的变量这两种变量。前者需要用.env$var来声明，后者用.data$var声明。 我们想根据输入的变量名及最小值来筛选数据。第一种写法就会出现意料之外的结果，第二种写法才符合预期。 # 第一种 data &lt;- reactive(diamonds %&gt;% filter(input$var &gt; input$min)) # 第二种 data &lt;- reactive(diamonds %&gt;% filter(.data[[input$var]] &gt; .env$input$min)) 环境中有input，input中有min，故可写作.env$input$min；数据框中没有input，故.data$input$var的写法错误 5.8.2 Tidy-selection 同理，tidy风格下的select之类的函数都可以直接输入变量名来进行操作。因此需要进行转化。all_of()和any_of()适合把字符串向量转化为适合tidy风格的变量。 all_of()会严格要求字符串向量中的变量都在要提取的对象之中。没有的话就会报错。 any_of()比较随和，有就提取，没的话就算了。 指的是字符串向量中的变量名是否在数据框中 output$data &lt;- renderTable({ req(input$vars) mtcars %&gt;% select(all_of(input$vars)) }) output$count &lt;- renderTable({ req(input$vars) mtcars %&gt;% group_by(across(all_of(input$vars))) %&gt;% summarise(n = n(), .groups = &quot;drop&quot;) }) "],["shiny_9.html", "5.9 交互式Rmarkdown", " 5.9 交互式Rmarkdown 在rmarkdown中植入shiny需要以下两步： 在YAML头部添加runtime: shiny 将shiny的组件与渲染函数添加到代码块 例子如下 --- title: &quot;demo&quot; runtime: shiny output: html_document --- ## 示例 内容 ```{r, echo=FALSE} numericInput(&quot;m&quot;, &quot;Number of samples:&quot;, 2, min = 1, max = 100) ``` ```{r, echo=FALSE} renderPlot({ means &lt;- replicate(1e4, mean(runif(input$m))) hist(means, breaks = 20) }, res = 96) ``` "],["shiny_10.html", "5.10 数据仪表盘", " 5.10 数据仪表盘 shiny中制作仪表盘的有两个包：flexdashboard包和shinydashboard包。 flexdashboard shinydashboard R Markdown Shiny Super easy Not quite as easy Static or dynamic Dynamic CSS flexbox layout Bootstrap grid layout flexdashboard在rmarkdown中创建仪表盘，并且官网介绍非常清晰明了，上手简单，完全可以现学现用 shinydashboard依旧需要编写一个shiny应用程序 下面的内容着重介绍shinydashboard。 部分细节处可使用HTML和CSS来调整 5.10.1 整体框架 使用dashboardPage()去创建经典的数据看板——顶部标题行，左侧边栏，右侧主板。 dashboardPage( dashboardHeader(), dashboardSidebar(), dashboardBody() ) 当然，你也可以分开创建各个部分，在传递给dashboardPage() header &lt;- dashboardHeader() sidebar &lt;- dashboardSidebar() body &lt;- dashboardBody() dashboardPage(header, sidebar, body) 5.10.1.1 标题行 如果你不想设置标题行，则dashboardHeader(disable = TRUE)。 标题行除了可以有标题外，直接用dashboardHeader(title = \"My Dashboard\")赋值，还可以添加下拉菜单。 下拉菜单用dropdownMenu()创建，可以包含消息、通知和任务三种元素。 记得给这三个元素匹配合适的图标 这三种类型的下拉菜单都由dropdownMenu()函数创建，利用参数type指定菜单类型消息message、通知notifications、任务tasks，参数badgeStatus设置小气泡的颜色，然后再由各自的item项来创建具体的条目。 “小气泡”指的是类似手机app右上角显示消息数量的那个小气泡 status设置参加这里 messageItem() from参数表示消息来源，message参数表示消息内容，icon=icon()设置图标，time设置消息时间 icon设置参见这里，默认为“用户形状” notificationItem() text参数表示通知内容，icon设置图标，status设置通知的颜色 taskItem() text参数设置说明性文字，value参数设置任务进度，color设置进度条颜色，href设置超链接 示例如下： library(shiny) library(shinydashboard) header &lt;- dashboardHeader( title = &quot;下拉通知菜单&quot;, dropdownMenu( type = &quot;messages&quot;, badgeStatus = &quot;success&quot;, # 气泡颜色 messageItem( from = &quot;系统通知&quot;, message = &quot;数据更新已完成&quot;, time = &quot;10:30&quot; ), messageItem( from = &quot;用户反馈&quot;, message = &quot;发现新问题&quot;, icon = icon(&quot;file&quot;), time = &quot;11:45&quot; ) ), dropdownMenu( type = &quot;notifications&quot;, badgeStatus = &quot;danger&quot;, # 气泡颜色 notificationItem( text = &quot;您有新的粉丝&quot; ) ), dropdownMenu( type = &quot;tasks&quot;, taskItem( text = &quot;工作进度&quot;, value = 73, color = &quot;green&quot; ) ) ) ui &lt;- dashboardPage( header, dashboardSidebar(), dashboardBody()) server &lt;- function(input, output) { } shinyApp(ui, server) 当然，这些信息的更新应该是实时的，上面的示例只提供了一个静态的菜单，下面设置动态更新的菜单。 在UI的dashboardHeader()中，直接添加dropdownMenuOutput(\"ID\")，然后再在server处用renderMenu()渲染你要实时更新的菜单内容。 例如 output$messageMenu &lt;- renderMenu({ # Code to generate each of the messageItems here, in a list. This assumes # that messageData is a data frame with two columns, &#39;from&#39; and &#39;message&#39;. msgs &lt;- apply(messageData, 1, function(row) { messageItem(from = row[[&quot;from&quot;]], message = row[[&quot;message&quot;]]) }) # This is equivalent to calling: # dropdownMenu(type=&quot;messages&quot;, msgs[[1]], msgs[[2]], ...) dropdownMenu(type = &quot;messages&quot;, .list = msgs) }) 5.10.1.2 侧边栏 如果你不想显示侧边栏，则dashboardSidebar(disable = TRUE) 侧边栏隶属于sidebarMenu()，用menuItem()往里面增加条目。menuItem()的一般用法如下所示： menuItem( text, # 菜单项显示的文本（必填） tabName = NULL, # 关联的tab名称（对应tabItem的tabName） icon = NULL, # Font Awesome图标（如icon(&quot;dashboard&quot;)） badgeLabel = NULL, # 气泡标签（显示在菜单文本右侧） badgeColor = &quot;green&quot;, # 气泡颜色（&quot;green&quot;, &quot;red&quot;, &quot;blue&quot;等） href = NULL, # 外部链接URL（如果设置，会覆盖tabName） newtab = TRUE, # 是否在新标签页打开外部链接 selected = NULL, # 初始是否选中 expandedName = text, # 展开时显示的文本 startExpanded = FALSE, # 初始是否展开（用于带子菜单的情况） ... # 子菜单项（menuSubItem） ) 非常重要的一点，如果该条目没有子条目的话，那么它一定要与对应的tabItem匹配，这样才能将选项与对应的主板页面匹配起来。 在menuItem()中也可继续添加子条目menuSubItem，其用法如下所示： menuSubItem( text, tabName = NULL, href = NULL, newtab = TRUE, icon = shiny::icon(&quot;angle-double-right&quot;), selected = NULL ) 对应主板中的内容用tabItems()与tabItem()表示，并用tabName与侧边栏中的选项匹配。如 sidebar &lt;- dashboardSidebar( sidebarMenu( menuItem(&quot;Dashboard&quot;, tabName = &quot;dashboard&quot;, icon = icon(&quot;dashboard&quot;)), menuItem(&quot;Widgets&quot;, icon = icon(&quot;th&quot;), tabName = &quot;widgets&quot;, badgeLabel = &quot;new&quot;, badgeColor = &quot;green&quot;) ) ) body &lt;- dashboardBody( tabItems( tabItem(tabName = &quot;dashboard&quot;, h2(&quot;Dashboard tab content&quot;) ), tabItem(tabName = &quot;widgets&quot;, h2(&quot;Widgets tab content&quot;) ) ) ) 同样，侧边栏菜单及其条目也能动态生成，依靠renderMenu()、sidebarMenuOutput()、menuItemOutput()渲染并输出。 ui &lt;- dashboardPage( dashboardHeader(title = &quot;Dynamic sidebar&quot;), dashboardSidebar( sidebarMenuOutput(&quot;menu&quot;) ), dashboardBody() ) server &lt;- function(input, output) { output$menu &lt;- renderMenu({ sidebarMenu( menuItem(&quot;Menu item&quot;, icon = icon(&quot;calendar&quot;)) ) }) } shinyApp(ui, server) ui &lt;- dashboardPage( dashboardHeader(title = &quot;Dynamic sidebar&quot;), dashboardSidebar( sidebarMenu( menuItemOutput(&quot;menuitem&quot;) ) ), dashboardBody() ) server &lt;- function(input, output) { output$menuitem &lt;- renderMenu({ menuItem(&quot;Menu item&quot;, icon = icon(&quot;calendar&quot;)) }) } shinyApp(ui, server) 在侧边栏处，还允许添加shiny的输入组件，以及搜索栏sidebarSearchForm()。 搜索栏技术更加高级，暂且不提 5.10.1.3 主板 关于主板页面的内容，除了之前提到过的tabItems()以及tabItem()，更为理想的布局方式就是将页面划分为一个个方框，每个区域内放置图、表或输入组件。 1. 普通方框 box( title = &quot;Box Title&quot;, # 盒子标题 status = &quot;primary&quot;, # 边框颜色 bacjground = NULL, # 背景颜色 solidHeader = FALSE, # 是否使用实心头部 width = 6, # 宽度（基于Bootstrap网格系统） height = NULL, # 固定高度 collapsible = FALSE, # 是否可折叠 collapsed = FALSE, # 初始是否为折叠状态 closable = FALSE, # 是否可关闭 footer = NULL, # 底部内容 ..., # 盒子主体内容 id = NULL # 盒子ID（用于JS操作） ) 例如 dashboardBody( fluidRow( box( title = &quot;Histogram&quot;, status = &quot;primary&quot;, solidHeader = TRUE, collapsible = TRUE, plotOutput(&quot;plot3&quot;, height = 250) ), box( title = &quot;Inputs&quot;, status = &quot;warning&quot;, solidHeader = TRUE, &quot;Box content here&quot;, br(), &quot;More box content&quot;, sliderInput(&quot;slider&quot;, &quot;Slider input:&quot;, 1, 100, 50), textInput(&quot;text&quot;, &quot;Text input:&quot;) ) ) ) 背景色可参考这里 2. 选项卡方框 tabBox()与tabPanel()用来组织方框内各个元素的结构。 tabBox( ..., # tabPanel()元素 id = NULL, selected = NULL, title = NULL, width = 6, height = NULL, side = c(&quot;left&quot;, &quot;right&quot;) ) tabPanel( id = NULL title, # 选项卡标题 ..., # 选项卡内容 icon = NULL # Font Awesome图标（如 icon(&quot;chart-bar&quot;)) ) library(shiny) library(shinydashboard) body &lt;- dashboardBody( fluidRow( tabBox( title = &quot;First tabBox&quot;, # The id lets us use input$tabset1 on the server to find the current tab id = &quot;tabset1&quot;, height = &quot;250px&quot;, tabPanel(&quot;Tab1&quot;, &quot;First tab content&quot;), tabPanel(&quot;Tab2&quot;, &quot;Tab content 2&quot;) ), tabBox( side = &quot;right&quot;, height = &quot;250px&quot;, # side=&quot;right&quot;表示从右往左放 selected = &quot;Tab3&quot;, tabPanel(&quot;Tab1&quot;, &quot;Tab content 1&quot;), tabPanel(&quot;Tab2&quot;, &quot;Tab content 2&quot;), tabPanel(&quot;Tab3&quot;, &quot;Note that when side=right, the tab order is reversed.&quot;) ) ), fluidRow( tabBox( # Title can include an icon title = tagList(shiny::icon(&quot;gear&quot;), &quot;tabBox status&quot;), tabPanel(&quot;Tab1&quot;, &quot;Currently selected tab from first box:&quot;, verbatimTextOutput(&quot;tabset1Selected&quot;) ), tabPanel(&quot;Tab2&quot;, &quot;Tab content 2&quot;) ) ) ) shinyApp( ui = dashboardPage( dashboardHeader(title = &quot;tabBoxes&quot;), dashboardSidebar(), body ), server = function(input, output) { # The currently selected tab from the first box output$tabset1Selected &lt;- renderText({ input$tabset1 }) } ) 小技巧：可用tagList()来拼接图标和文本 3. 信息方框 类似一个方框里面，加个图标，加个说明性文本，加个指标。 infoBox( title, value = NULL, subtitle = NULL, icon = shiny::icon(&quot;bar-chart&quot;), color = &quot;aqua&quot;, width = 4, href = NULL, fill = FALSE # 是否填充方框 ) library(shiny) library(shinydashboard) ui &lt;- dashboardPage( dashboardHeader(title = &quot;Info boxes&quot;), dashboardSidebar(), dashboardBody( # infoBoxes with fill=FALSE fluidRow( # A static infoBox infoBox(&quot;New Orders&quot;, 10 * 2, icon = icon(&quot;credit-card&quot;)), # Dynamic infoBoxes infoBoxOutput(&quot;progressBox&quot;), infoBoxOutput(&quot;approvalBox&quot;) ), # infoBoxes with fill=TRUE fluidRow( infoBox(&quot;New Orders&quot;, 10 * 2, icon = icon(&quot;credit-card&quot;), fill = TRUE), infoBoxOutput(&quot;progressBox2&quot;), infoBoxOutput(&quot;approvalBox2&quot;) ), fluidRow( # Clicking this will increment the progress amount box(width = 4, actionButton(&quot;count&quot;, &quot;Increment progress&quot;)) ) ) ) server &lt;- function(input, output) { output$progressBox &lt;- renderInfoBox({ infoBox( &quot;Progress&quot;, paste0(25 + input$count, &quot;%&quot;), icon = icon(&quot;list&quot;), color = &quot;purple&quot; ) }) output$approvalBox &lt;- renderInfoBox({ infoBox( &quot;Approval&quot;, &quot;80%&quot;, icon = icon(&quot;thumbs-up&quot;, lib = &quot;glyphicon&quot;), color = &quot;yellow&quot; ) }) # Same as above, but with fill=TRUE output$progressBox2 &lt;- renderInfoBox({ infoBox( &quot;Progress&quot;, paste0(25 + input$count, &quot;%&quot;), icon = icon(&quot;list&quot;), color = &quot;purple&quot;, fill = TRUE ) }) output$approvalBox2 &lt;- renderInfoBox({ infoBox( &quot;Approval&quot;, &quot;80%&quot;, icon = icon(&quot;thumbs-up&quot;, lib = &quot;glyphicon&quot;), color = &quot;yellow&quot;, fill = TRUE ) }) } shinyApp(ui, server) 4. 数值方框 数值方框和信息方框类似，都是在小方块中有图标、有指标、有文本，都能设置成静态的或者动态的。 个人感觉，数值方框相较信息方框从视觉上凸显了“指标” library(shiny) library(shinydashboard) ui &lt;- dashboardPage( dashboardHeader(title = &quot;Value boxes&quot;), dashboardSidebar(), dashboardBody( fluidRow( # A static valueBox valueBox(10 * 2, &quot;New Orders&quot;, icon = icon(&quot;credit-card&quot;)), # Dynamic valueBoxes valueBoxOutput(&quot;progressBox&quot;), valueBoxOutput(&quot;approvalBox&quot;) ), fluidRow( # Clicking this will increment the progress amount box(width = 4, actionButton(&quot;count&quot;, &quot;Increment progress&quot;)) ) ) ) server &lt;- function(input, output) { output$progressBox &lt;- renderValueBox({ valueBox( paste0(25 + input$count, &quot;%&quot;), &quot;Progress&quot;, icon = icon(&quot;list&quot;), color = &quot;purple&quot; ) }) output$approvalBox &lt;- renderValueBox({ valueBox( &quot;80%&quot;, &quot;Approval&quot;, icon = icon(&quot;thumbs-up&quot;, lib = &quot;glyphicon&quot;), color = &quot;yellow&quot; ) }) } shinyApp(ui, server) 5. 布局 基于行的布局 使用fluidRow()来组织每一行的内容。注意宽度为12个单位，每个方框的默认宽度为6。 基于行的布局默认每行内容顶部对齐，因此底部不一定对齐，取决于各个元素的内容。在box()中可设置height来统一高度。 宽度width是基于bootstrap的12单位宽，而高度height的单位是像素 基于列的布局 在fluidRow()内部使用column()来划分出一列，column()内的box()将会从上到下排列。 由于column()中指定了宽度width，故box()的宽度得设置为width=NULL，统一使用column()的宽度。 fluidRow( column(width = 6, box( title = &quot;Box title&quot;, width = NULL, status = &quot;primary&quot;, &quot;Box content&quot; ), box( title = &quot;Title 1&quot;, width = NULL, solidHeader = TRUE, status = &quot;primary&quot;, &quot;Box content&quot; ), box( width = NULL, background = &quot;black&quot;, &quot;A box with a solid black background&quot; ) ), column(width = 6, box( status = &quot;warning&quot;, width = NULL, &quot;Box content&quot; ), box( title = &quot;Title 3&quot;, width = NULL, solidHeader = TRUE, status = &quot;warning&quot;, &quot;Box content&quot; ), box( title = &quot;Title 5&quot;, width = NULL, background = &quot;light-blue&quot;, &quot;A box with a solid light-blue background&quot; ) ) 行列混合布局 由于列布局是在fluidRow()中加入column()实现，因此，布局的基本单位就是行视角。基于此，就可以任意实现行与列混合布局。 5.10.2 外观 5.10.2.1 皮肤 根据dashboardPage()中的参数skin设置，如dashboardPage(skin=purple) 5.10.2.2 CSS样式 在shiny应用所在的目录中创建名为www的新文件夹，再在里面创建css文件，文件中的内容就是你自定义的css样式。 之后在dashboardBody处引用这个css文件即可。 示例如下： .main-header .logo { font-family: &quot;Georgia&quot;, Times, &quot;Times New Roman&quot;, serif; font-weight: bold; font-size: 24px; } dashboardPage( dashboardHeader(title = &quot;Custom font&quot;), dashboardSidebar(), dashboardBody( tags$head( tags$link(rel = &quot;stylesheet&quot;, type = &quot;text/css&quot;, href = &quot;custom.css&quot;) ) ) ) 注意在shiny所在的目录中新建www文件夹，该文件夹内包含custom.css文件 或者直接在shiny中输入html语言。 dashboardPage( dashboardHeader(title = &quot;Custom font&quot;), dashboardSidebar(), dashboardBody( tags$head(tags$style(HTML(&#39; .main-header .logo { font-family: &quot;Georgia&quot;, Times, &quot;Times New Roman&quot;, serif; font-weight: bold; font-size: 24px; } &#39;))) ) ) 其余方式详见此处 5.10.2.3 标题宽度 在dashboardHeader()中设置参数titleWidth来调整宽度，单位为像素。同时，也可设置标题处的背景色与标题行的背景色相同。 dashboardPage( dashboardHeader( title = &quot;Example of a long title that needs more space&quot;, titleWidth = 450 ), dashboardSidebar(), dashboardBody( # Also add some custom CSS to make the title background area the same # color as the rest of the header. tags$head(tags$style(HTML(&#39; .skin-blue .main-header .logo { background-color: #3c8dbc; } .skin-blue .main-header .logo:hover { background-color: #3c8dbc; } &#39;))) ) ) 5.10.2.4 侧边栏宽度 dashboardSidebar()中的参数width可调整宽度，单位为像素。 5.10.2.5 图标 不少函数中有参数icon，你可用icon()来传递对应的图标，如icon = icon('calendar')。 icon()默认使用FontAwesome的图标，也可用参数lib更改来源，使用Glyphicon的图标。 &quot;Calendar from Font-Awesome:&quot;, icon(&quot;calendar&quot;), &quot;Cog from Glyphicons:&quot;, icon(&quot;cog&quot;, lib = &quot;glyphicon&quot;) 5.10.2.6 配色 函数中涉及到参数status和color的，可以参考 图 5.1: Status 图 5.2: Colors 5.10.3 案例 数据生成部分： library(tidyverse) library(hms) # 用户信息 gen_customers &lt;- function(user_id_start, num, date){ # id_start表示从哪个id开始计数 # num表示要生成的用户数 # date表示今日日期 province &lt;- c(&quot;北京&quot;, &quot;天津&quot;, &quot;河北&quot;, &quot;山西&quot;, &quot;内蒙古&quot;, &quot;辽宁&quot;, &quot;吉林&quot;, &quot;黑龙江&quot;, &quot;上海&quot;, &quot;江苏&quot;, &quot;浙江&quot;, &quot;安徽&quot;, &quot;福建&quot;, &quot;江西&quot;, &quot;山东&quot;, &quot;河南&quot;, &quot;湖北&quot;, &quot;湖南&quot;, &quot;广东&quot;, &quot;广西&quot;, &quot;海南&quot;, &quot;重庆&quot;, &quot;四川&quot;, &quot;贵州&quot;, &quot;云南&quot;, &quot;西藏&quot;, &quot;陕西&quot;, &quot;甘肃&quot;, &quot;青海&quot;, &quot;宁夏&quot;, &quot;新疆&quot;) reg_times &lt;- hms::hms( seconds = sample(0:59, num, replace = TRUE), minutes = sample(0:59, num, replace = TRUE), hours = pmin(pmax(round(rnorm(num, mean = 14, sd = 5)), 0), 23) ) age_prob &lt;- runif(length(18:55)) age_prob &lt;- age_prob/sum(age_prob) gender_prob &lt;- runif(2) gender_prob &lt;- gender_prob/sum(gender_prob) province_prob &lt;- runif(31) province_prob &lt;- province_prob/sum(province_prob) customers &lt;- tibble( reg_date = rep(date, num), reg_time = reg_times, age = sample(18:55, num, replace = TRUE, prob = age_prob), gender = sample(c(&quot;男&quot;,&quot;女&quot;), num, replace = TRUE, prob = gender_prob), province = sample(province, num, replace = TRUE, prob = province_prob) ) %&gt;% arrange(reg_time) %&gt;% mutate(id = (user_id_start):(user_id_start+num-1)) %&gt;% relocate(id, .before = reg_date) return(customers) } # 商品信息 df_products &lt;- tibble( product_id = 1:20, category = rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;), times = c(8,3,4,3,2)), item = c(1:8, 1:3, 1:4, 1:3, 1:2), price = round(runif(20, 10, 500)), cost = round(price * runif(20, 0.4, 0.7)), original_stock = round(rnorm(20, 500, 5)), new_stock = original_stock ) # 订单信息 gen_order &lt;- function(order_id_start, user_id, date, df_customers){ # 生成每笔订单的购物信息 reg_date &lt;- df_customers[df_customers$id == user_id,][[&quot;reg_date&quot;]] if(reg_date != date){ order_time &lt;- hms::hms( seconds = sample(0:59, 1, replace = TRUE), minutes = sample(0:59, 1, replace = TRUE), hours = pmin(pmax(round(rnorm(1, mean = 14, sd = 5)), 0), 23) ) }else{ reg_time &lt;- df_customers[df_customers$id == user_id,][[&quot;reg_time&quot;]] sec &lt;- sample(0:(2*60*60), 1) # 2小时内下单 order_time &lt;- as_hms(reg_time + hms(seconds = min(sec, 86399))) } purchase_product &lt;- sample(1:20, sample(1:3, 1, prob = c(0.8,0.15,0.05))) purchase_num &lt;- sample(1:4, length(purchase_product), replace = TRUE, prob = c(0.7,0.15,0.1,0.05)) df_order &lt;- tibble( order_id = order_id_start, user_id = user_id, date = date, time = order_time, product = purchase_product, num = purchase_num ) return(df_order) } shiny部分： library(tidyverse) library(shiny) library(shinydashboard) library(shinyWidgets) library(bubbles) library(plotly) source(&#39;E:/R/shiny/demo/gen_data.R&#39;) header &lt;- dashboardHeader( title = &#39;Demo&#39;, dropdownMenuOutput(&#39;update_notification&#39;), tags$li( class = &quot;dropdown date-center&quot;, textOutput(&quot;current_date&quot;) ) ) sidebar &lt;- dashboardSidebar( sidebarMenu( menuItem(&quot;核 心 指 标&quot;, tabName = &quot;kpi&quot;, icon = icon(&quot;table&quot;)), menuItem(&quot;用 户 分 析&quot;, tabName = &quot;users&quot;, icon = icon(&quot;user&quot;)), menuItem(&#39;商 品 分 析&#39;, tabName = &#39;products&#39;, icon = icon(&quot;cart-shopping&quot;)), menuItem(&quot;设 置&quot;, tabName = &quot;setting&quot;, icon = icon(&quot;gear&quot;), menuSubItem(actionButton(&quot;manual_update&quot;, &quot;手动更新&quot;, icon = icon(&quot;sync&quot;))), menuSubItem( materialSwitch( inputId = &quot;auto_update&quot;, label = &#39;自动更新&#39;, value = FALSE, status = &#39;success&#39;, inline = TRUE ) )) ) ) body &lt;- dashboardBody( tags$head( tags$style(HTML(&quot; /* 日期居中样式 */ .date-center { position: absolute !important; left: 0 !important; right: 0 !important; top: 0 !important; text-align: center !important; padding-top: 10px !important; pointer-events: none; /* 防止遮挡其他元素 */ } /* 隐藏默认标题 */ .main-header .navbar-custom-menu, .main-header .sidebar-toggle { z-index: 1; /* 确保其他元素在前 */ } /* 文本样式 */ #current_date { color: white; font-weight: bold; font-size: 24px; } &quot;)) ), tabItems( # 核心指标 tabItem( tabName = &#39;kpi&#39;, fluidRow( valueBoxOutput(&#39;gmv&#39;, width = 3), valueBoxOutput(&#39;volume&#39;, width = 3), valueBoxOutput(&#39;users&#39;, width = 3), valueBoxOutput(&#39;reg_users&#39;, width = 3) ), fluidRow( column(width = 4, offset = 2, sliderInput(&quot;date_range&quot;, &quot;日期范围:&quot;, min = Sys.Date(), max = Sys.Date(), value = c(Sys.Date(), Sys.Date()), timeFormat = &quot;%Y-%m-%d&quot;)), column(width = 4, selectInput(&#39;selected_idx&#39;, label = &#39;指标&#39;, choices = c(&#39;历史销售额&#39;,&#39;历史销量&#39;,&#39;历史新增用户&#39;), selected = &#39;历史销售额&#39;)) ), fluidRow( box(status = &quot;primary&quot;, plotlyOutput(&#39;kpi_plot&#39;), width = 12) ) ), tabItem( tabName = &#39;users&#39;, fluidRow( # column里的元素（这里是box）的宽度12是相对该column而言的，而非整个屏幕 column(width = 4, box( status = &#39;primary&#39;, width = 12, plotlyOutput(&#39;user_frequency_plot&#39;) ) ), column(width = 4, box( status = &#39;primary&#39;, width = 12, div( textOutput(&#39;bin_width&#39;), style = &quot;font-size: 15px;&quot;), plotlyOutput(&#39;user_mean_amount_plot&#39;) ) ), column(width = 4, box( status = &#39;primary&#39;, width = 12, plotlyOutput(&#39;user_total_amount_plot&#39;) )) ), fluidRow() ), tabItem( tabName = &#39;products&#39;, fluidRow( column(width=6, box( title = &#39;今日销售商品构成&#39;, status = &#39;primary&#39;, width = 12, plotlyOutput(&#39;product_num_plot&#39;) ), box( title = &#39;商品库存&#39;, status = &#39;primary&#39;, width = 12, plotlyOutput(&#39;product_stock_plot&#39;) ) ), column(width=6, box( title = &#39;今日商品销售金额&#39;, status = &#39;primary&#39;, width = 12, bubblesOutput(&#39;product_amount_plot&#39;, height = &#39;800px&#39;) ) ) ) ) ) ) ui &lt;- dashboardPage(header, sidebar, body) server &lt;- function(input, output) { # 存储所有用户数据 df_customers &lt;- reactiveVal(tibble()) # 存储订单数据 df_orders &lt;- reactiveVal(tibble()) # 商品信息 df_products &lt;- reactiveVal(df_products) # 库存检测 stock_observe &lt;- reactiveVal(TRUE) # kpi指标 idx_gmv &lt;- reactiveVal(0) idx_volume &lt;- reactiveVal(0) idx_users &lt;- reactiveVal(0) idx_reg_users &lt;- reactiveVal(0) df_kpi &lt;- reactiveVal(tibble()) # 存储更新时间 update_time &lt;- reactiveVal(character(0)) # 当前日期（随时间动态更新） current_date &lt;- reactiveVal(Sys.Date()) # 响应式获取用户ID与订单ID next_user_id &lt;- reactive({ if(nrow(df_customers()) != 0) { max(df_customers()$id) + 1 } else { 1 } }) next_order_id &lt;- reactive({ if(nrow(df_orders()) != 0) { max(df_orders()$order_id) + 1 } else { 1 } }) # 更新数据 update_data &lt;- function() { isolate({ # 获取当前用户数据 current_users &lt;- df_customers() # 获取下一个可用用户ID next_user_id_val &lt;- next_user_id() # 获取当前日期 current_date_val &lt;- current_date() # 获取当前订单数据 current_orders &lt;- df_orders() # 获取下一个订单ID next_order_id_val &lt;- next_order_id() # 获取产品数据 products_data &lt;- df_products() # 获取kpi数据框 current_kpi &lt;- df_kpi() }) # 随机生成用户数量 num &lt;- sample(10:50, 1) # 生成新用户 new_customers &lt;- gen_customers( user_id_start = next_user_id_val, num = num, date = current_date_val ) update_customers &lt;- bind_rows(current_users, new_customers) # 生成新订单 consumers_num &lt;- round(rnorm(1, mean=nrow(update_customers)*0.3, sd=nrow(update_customers)/20)) consumers_id &lt;- sample(1:nrow(update_customers), pmax(consumers_num, 1)) %&gt;% as.list() order_id &lt;- as.list(next_order_id_val:(next_order_id_val+length(consumers_id)-1)) new_orders &lt;- map2(order_id, consumers_id, ~gen_order(.x, .y, current_date_val, update_customers)) %&gt;% do.call(rbind, .) new_orders$amount &lt;- products_data$price[new_orders$product] * new_orders$num update_orders &lt;- bind_rows(current_orders, new_orders) # 销售额 gmv &lt;- sum(new_orders$amount) # 销量 volume &lt;- sum(new_orders$num) # 总用户数 users &lt;- nrow(update_customers) # 新增用户数 reg_users &lt;- nrow(new_customers) update_kpi &lt;- tibble( idx = c(&#39;销售额&#39;,&#39;销量&#39;,&#39;新增用户数&#39;), date = current_date_val, val = c(gmv, volume, reg_users) ) update_kpi &lt;- rbind(current_kpi, update_kpi) # 修改库存 change_num &lt;- new_orders %&gt;% group_by(product) %&gt;% summarise(n=sum(num)) products_data$new_stock[change_num$product] &lt;- products_data$new_stock[change_num$product] - change_num$n new_stock_observe &lt;- any(products_data$new_stock &lt;=100) isolate({ # 更新用户数据集 df_customers(update_customers) # 更新订单数据集 df_orders(update_orders) # 更新库存 df_products(products_data) stock_observe(!new_stock_observe) # 更新KPI idx_gmv(gmv) idx_volume(volume) idx_users(users) idx_reg_users(reg_users) df_kpi(update_kpi) # 更新日期 current_date(current_date() + 1) # 记录更新时间 new_time &lt;- format(Sys.time(), &quot;%H:%M:%S&quot;) update_time(new_time) output$update_notification &lt;- renderMenu({ dropdownMenu( type = &quot;notifications&quot;, notificationItem( icon = icon(&quot;rotate&quot;), text = paste(&quot;最后更新:&quot;, update_time()), status = &quot;success&quot; ) ) }) }) } # 自动更新 observe({ # 设置定时器 on.exit(invalidateLater(2000), add = TRUE) if(input$auto_update){ if(isolate(stock_observe())){ update_data() # 更新滑块的最大值和结束日期 isolate({ updateSliderInput( inputId = &quot;date_range&quot;, max = current_date(), value = c(input$date_range[1], current_date()) ) }) }else{ showNotification(&#39;库存不足，停止更新！&#39;, duration = NULL, closeButton = TRUE) } } }) # 手动更新 observeEvent(input$manual_update, { if(isolate(stock_observe())){ update_data() # 更新滑块的最大值和结束日期 updateSliderInput( inputId = &quot;date_range&quot;, max = max(input$date_range[2], current_date()), value = c(input$date_range[1], current_date()) ) }else{ showNotification(&#39;库存不足，停止更新！&#39;, duration = NULL, closeButton = TRUE) } }) output$gmv &lt;- renderValueBox({ valueBox( value = idx_gmv(), subtitle = &#39;今日销售额&#39;, icon = icon(&#39;yen-sign&#39;), color = &#39;fuchsia&#39; ) }) output$volume &lt;- renderValueBox({ valueBox( value = idx_volume(), subtitle = &#39;今日销量&#39;, icon = icon(&#39;coins&#39;), color = &#39;orange&#39; ) }) output$users &lt;- renderValueBox({ valueBox( value = idx_users(), subtitle = &#39;总用户数&#39;, icon = icon(&#39;users&#39;), color = &#39;aqua&#39; ) }) output$reg_users &lt;- renderValueBox({ valueBox( value = idx_reg_users(), subtitle = &#39;今日注册用户数&#39;, icon = icon(&#39;registered&#39;), color = &#39;purple&#39; ) }) output$current_date &lt;- renderText({ format(current_date(), &quot;%Y年%m月%d日&quot;) }) output$kpi_plot &lt;- renderPlotly({ if(nrow(df_kpi())&gt;0){ selected_idx &lt;- switch(input$selected_idx, &#39;历史销售额&#39; = &#39;销售额&#39;, &#39;历史销量&#39; = &#39;销量&#39;, &#39;历史新增用户&#39; = &#39;新增用户数&#39;) df &lt;- df_kpi() %&gt;% filter(idx == selected_idx, date&gt;=input$date_range[1] &amp; date &lt;= input$date_range[2]) %&gt;% arrange(date) p &lt;- ggplot(df)+ geom_line(aes(x=date, y=val, color = idx, group = idx))+ geom_point(aes(x=date, y=val), color = &#39;black&#39;)+ theme_bw()+ labs(x=&#39;日期&#39;, y=&#39;&#39;)+ scale_x_date(date_labels = &#39;%Y-%m-%d&#39;)+ theme(legend.position = &#39;none&#39;) ggplotly(p) %&gt;% layout( hoverlabel = list( bgcolor = &quot;white&quot;, font = list(size = 12, color = &quot;black&quot;) ) ) } }) output$user_frequency_plot &lt;- renderPlotly({ if(nrow(df_orders())&gt;0){ df &lt;- df_orders() df &lt;- df %&gt;% group_by(user_id) %&gt;% summarise(num=length(unique(order_id)), mean_amount = mean(amount), total_amount = sum(amount)) %&gt;% dplyr::ungroup() p &lt;- ggplot(df)+ geom_bar(aes(x=num), fill = &#39;#1E90FF&#39;, color = &#39;black&#39;)+ theme_bw()+ labs(x=&#39;消费次数&#39;, y=&#39;&#39;)+ scale_x_continuous(breaks=scales::breaks_width(1)) ggplotly(p) %&gt;% layout( hoverlabel = list( bgcolor = &quot;white&quot;, font = list(size = 12, color = &quot;black&quot;) ) ) } }) output$bin_width &lt;- renderText({ if(nrow(df_orders())&gt;0){ df &lt;- df_orders() df &lt;- df %&gt;% group_by(user_id) %&gt;% summarise(num=length(unique(order_id)), mean_amount = mean(amount), total_amount = sum(amount)) %&gt;% dplyr::ungroup() bin_width &lt;- round(3.49 * sd(df$mean_amount) / nrow(df)^(1/3)) paste0(&#39;bin宽度：&#39;, bin_width) } }) output$user_mean_amount_plot &lt;- renderPlotly({ if(nrow(df_orders())&gt;0){ df &lt;- df_orders() df &lt;- df %&gt;% group_by(user_id) %&gt;% summarise(num=length(unique(order_id)), mean_amount = mean(amount), total_amount = sum(amount)) %&gt;% dplyr::ungroup() bin_width &lt;- round(3.49 * sd(df$mean_amount) / nrow(df)^(1/3)) p &lt;- ggplot(df)+ geom_histogram(aes(x=mean_amount), fill = &#39;#1E90FF&#39;, color = &#39;black&#39;, binwidth = bin_width )+ theme_bw()+ labs(x=&#39;平均消费金额&#39;, y=&#39;&#39;) ggplotly(p) %&gt;% layout( hoverlabel = list( bgcolor = &quot;white&quot;, font = list(size = 12, color = &quot;black&quot;) ) ) } }) output$user_total_amount_plot &lt;- renderPlotly({ if(nrow(df_orders())&gt;0){ df &lt;- df_orders() df &lt;- df %&gt;% group_by(user_id) %&gt;% summarise(num=length(unique(order_id)), mean_amount = mean(amount), total_amount = sum(amount)) %&gt;% dplyr::ungroup() %&gt;% slice_max(total_amount, n=10) %&gt;% mutate(user_id = factor(user_id)) %&gt;% mutate(user_id = fct_reorder(user_id, total_amount)) hover_text &lt;- paste(&quot;ID:&quot;, df$user_id, &quot;&lt;br&gt;&quot;, &quot;total_amount:&quot;, scales::comma(df$total_amount), &quot;&lt;br&gt;&quot;) # 创建基础ggplot对象 p &lt;- ggplot(df) + geom_bar( aes(x = total_amount, y = user_id), fill = &#39;#1E90FF&#39;, color = &#39;black&#39;, stat = &#39;identity&#39; ) + theme_bw() + labs(x = &#39;总消费金额&#39;, y = &#39;用户id&#39;) # 在ggplotly中使用自定义悬停文本 ggplotly(p, tooltip = &quot;none&quot;) %&gt;% # 禁用默认提示 style( text = hover_text, # 设置自定义悬停文本 hoverinfo = &quot;text&quot;, # 仅显示文本 traces = 1 # 应用到第一个轨迹（条形图） ) %&gt;% layout( hoverlabel = list( bgcolor = &quot;white&quot;, font = list(size = 12, color = &quot;black&quot;) ) ) } }) output$product_num_plot &lt;- renderPlotly({ if(nrow(df_orders())&gt;0){ df &lt;- df_orders() df$name &lt;- paste0(df_products()$category, df_products()$item)[df$product] plot_ly(df, labels=~name, values=~num, type=&#39;pie&#39;) } }) output$product_amount_plot &lt;- renderBubbles({ if(nrow(df_orders())&gt;0){ df &lt;- df_orders() df$name &lt;- paste0(df_products()$category, df_products()$item)[df$product] df &lt;- df %&gt;% dplyr::group_by(name) %&gt;% summarise(total_amount = sum(amount)) %&gt;% dplyr::ungroup() bubbles(df$total_amount, df$name, tooltip = paste0(df$name, &#39;:&#39;,df$total_amount), height = &#39;100%&#39;) } }) output$product_stock_plot &lt;- renderPlotly({ df &lt;- df_products() df$name &lt;- paste0(df$category, df$item) p &lt;- ggplot(df)+ geom_segment(aes(x=new_stock, xend=original_stock, y=name, yend=name))+ geom_point(aes(x=new_stock, y=name), color = &#39;red&#39;)+ geom_point(aes(x=original_stock, y=name), color = &#39;blue&#39;)+ theme_bw()+ labs(x=&#39;库存&#39;, y=&#39;&#39;) ggplotly(p) }) } shinyApp(ui, server) Tips: 必要时可以用HTML来增添细节。 fluidRow()中的column()具有独立的空间，其内部的元素的宽度都是相对column()而言的（即对于内部元素而言column的宽度就是12），而非整个屏幕。 建议将用到的反应式变量集中在一块，方便同一调度。 在observe()内部，会监测任一反应式变量（包括动态ui组件）的变化，如果有更新，则会重新执行代码块。注意，如果存在重复调度的情况，则会陷入无限循环，典型表现就是生成shiny应用时屏幕泛白。为了避免此类情况，若在observe()内部需要重复调用反应式变量，建议用isolate()将其隔离，转化为本地变量后，再用isolate()更新反应式变量。 "],["shiny_11.html", "5.11 分享shiny", " 5.11 分享shiny 分享你的shiny应用有多种方式，你可以将其部署到shiny.io、Posit Connect等平台。 下面介绍如何将你的shiny应用部署到shiny.io中。 进入shiny.io官网 点击Sign Up，创建账户 根据页面提示安装rsconnect包，注册账号信息 install.packages(&#39;rsconnect&#39;) 需要点击Show Secret得到完整的信息，之后在运行代码 部署你的shiny应用 rsconnect::deployApp(&#39;path/to/your/app&#39;) 该路径指向文件夹，这个文件夹中要包含你的shiny应用及相关文件。如果你的shiny应用需要调用这些文件，那么请使用相对路径 你可以在Account/Settings中修改你的账户名。 "],["langchain.html", "6 Langchain ", " 6 Langchain "],["langchain_x.html", "6.1 案例", " 6.1 案例 6.1.1 NL2SQL 如果可以的话，还应该加一个反馈机制：当第一次写出来的SQL语句在执行时发生错误后，不应该停止，而是将错误信息与错误的SQL语句喂给大模型，让大模型修正SQL语句，直至正确。 import os import pandas as pd from dotenv import load_dotenv from dataclasses import dataclass from typing import List, Dict, Any from langchain.agents import create_agent from langchain_ollama import ChatOllama from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate from langchain_community.utilities import SQLDatabase from langchain.agents.structured_output import ToolStrategy # 大模型 llm = ChatOllama( model=&quot;qwen3:4b&quot;, base_url=&quot;http://localhost:11434&quot;, temperature=0 ) # 连接数据库 load_dotenv() MYSQL_HOST = os.getenv(&quot;MYSQL_HOST&quot;, &quot;127.0.0.1&quot;) MYSQL_PORT = os.getenv(&quot;MYSQL_PORT&quot;, &quot;3306&quot;) MYSQL_USER = os.getenv(&quot;MYSQL_USER&quot;) MYSQL_PASSWORD = os.getenv(&quot;MYSQL_PASSWORD&quot;) MYSQL_DB = os.getenv(&quot;MYSQL_DB&quot;) DB_URI = ( f&quot;mysql+pymysql://{MYSQL_USER}:{MYSQL_PASSWORD}@{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DB}?charset=utf8mb4&quot; ) db = SQLDatabase.from_uri( DB_URI, include_tables=[&quot;df_customers&quot;, &quot;df_orders&quot;] # 白名单 ) table_info = db.get_table_info() # 获取数据库中表结构 # 定义提示词模板 system_template = &quot;&quot;&quot; 你是MySQL查询专家，严格遵守以下规则： 1. 仅执行查询操作，禁止增、删、改这些危险操作 2. 严格根据表结构进行查询，表结构信息：{table_info} 3. 调用工具前必须验证字段是否存在，生成SQL语句后自检语法 4. 当生成完SQL语句后，需要执行SQL语句，从数据库中提取相应的数据 5. 如果涉及多表连接、聚合等复杂操作，必要时可以创建临时表、使用子查询等方法 &quot;&quot;&quot; system_prompt = SystemMessagePromptTemplate.from_template(system_template) user_prompt = HumanMessagePromptTemplate.from_template(&quot;{question}&quot;) prompt = ChatPromptTemplate.from_messages([ system_prompt, user_prompt ]) # 定义工具 def generate_sql(question: str) -&gt; str: &quot;&quot;&quot;根据自然语言问题生成一条 MySQL SQL语句。&quot;&quot;&quot; msg = prompt.format_messages( table_info = table_info, question=question ) sql = llm.invoke(msg).content.strip() return sql def run_sql(sql: str) -&gt; dict: &quot;&quot;&quot;在数据库中执行SQL语句，提取相应的数据&quot;&quot;&quot; with db._engine.connect() as con: df = pd.read_sql(sql, con) df = df.to_dict(orient=&quot;records&quot;) return df # 格式化输出 @dataclass class FinalAnswer: &quot;&quot;&quot;输出结果&quot;&quot;&quot; sql: str df: List[Dict[str, Any]] explanation: str # 定义智能体 agent = create_agent( model=llm, tools=[generate_sql, run_sql], response_format=ToolStrategy(FinalAnswer) ) def sql_query_agent(question): out = agent.invoke( {&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: question}]} ) sql = out[&quot;structured_response&quot;].sql df = pd.DataFrame(out[&quot;structured_response&quot;].df) explanation = out[&quot;structured_response&quot;].explanation return sql, df, explanation # 流式输出 # for step in agent.stream( # {&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;查询ID为2的用户的个人信息及其总下单次数，总下单次数列用“order_num”表示&quot;}]}, # stream_mode=&quot;values&quot;, # ): # step[&quot;messages&quot;][-1].pretty_print() if __name__ == &quot;__main__&quot;: question = &quot;查询ID为2的用户的个人信息及其总下单次数，总下单次数列用“order_num”表示&quot; sql, df, _ = sql_query_agent(question) "],["reg.html", "7 应用回归分析", " 7 应用回归分析 回归模型的分类如下所示： 回归模型的建模步骤如下所示： "],["reg_1.html", "7.1 引言", " 7.1 引言 7.1.1 变量间的相关关系 函数关系：变量间存在的确定性数量对应关系 \\[ y=f(x_1, ..., x_p) \\] 相关关系：变量间客观存在的非确定性数量对应关系 \\[ y=f(x_1, ..., x_p, \\varepsilon) \\] 相关分析 用相关系数描述变量间相关关系的强度。以变量之间是否相 关、相关的方向和密切程度等为研究内容，不区分自变量和因变量，不 关心相关关系的表现形态。 回归分析 对具有相关关系的变量，根据其相关关系的具体形态，选择 合适的回归模型描述自变量对因变量的影响方式。 相关分析与回归分析的联系 相关分析是回归分析的前提；回归分析是对因果关系（说是这么说，但不能忽视“伪回归”现象）的探讨 相关分析和回归分析的区别 X与Y的地位是否平等；有无因果关系；相关分析揭示线性相关程度，回归分析给出具体的回归方程。 7.1.2 回归模型的一般形式 \\[ y=f(x_1, ..., x_p, \\varepsilon) \\] 其中y为因变量（响应变量、被解释变量），x为自变量（预测变量、解释变量），\\(\\varepsilon\\)为模型误差（随机扰动项）。 \\(\\varepsilon\\)包含的内容： 被忽略的解释变量 变量数值的观测误差 模型设定误差 其他随机因素的影响 "],["reg_2.html", "7.2 假定", " 7.2 假定 零均值：\\(E(\\varepsilon)=0\\) 同方差：\\(Var(\\varepsilon)=\\sigma^2\\) 无自相关：\\(Cov(\\varepsilon_i,\\varepsilon_j)=0\\) 无内生性：\\(Cov(X_i,\\varepsilon)=0\\) 随机扰动项\\(\\varepsilon\\)服从正态分布 自变量为非随机变量，且无多重共线性 模型是正确设定的，具有线性关系 其中第1、2、3条为Gauss-Markov条件。 1~5条和后续的内容有关，留到后面再讲。 第6条，为什么自变量为非随机变量？我们的目标是探讨自变量与因变量之间的因果关系。在线性模型中，我们已经在因变量的生成过程中引入随机扰动项来代表随机因素的影响。如果再把生成自变量的过程也引入随机因素，则不好把握自变量与因变量之间的因果关系。在实际应用中，我们往往关注在给定自变量的时候，因变量是如何变化的。通过假设自变量为非随机变量，可以简化模型，使得数学推导和计算更加简洁。如果解释变量是随机的，那么模型的推断将变得非常复杂，因为需要考虑解释变量的概率分布，这将大大增加分析的难度。同时，若自变量为随机变量则可能会有内生性问题。 第7条，如果真实模型不具有线性关系，那么我们的模型设定就是有偏误的（做此假定只是为了让自己用得放心，谁又能知道真实模型是什么样子的呢）。 "],["reg_3.html", "7.3 线性回归模型", " 7.3 线性回归模型 7.3.1 一元线性回归模型 \\[ \\left\\{ \\begin{array}{c} y=\\beta_0+\\beta_1x+\\varepsilon\\\\ E(\\varepsilon|x)=0\\\\ Var(\\varepsilon|x)=\\sigma^2 \\end{array} \\right. \\tag{7.1} \\] 等价于 \\[ \\left\\{ \\begin{array}{c} E(y|x)=\\beta_0+\\beta_1x\\\\ Var(y|x)=\\sigma^2 \\end{array} \\right. \\tag{7.2} \\] 习惯上将\\(E(y|x)\\)简记为\\(E(y)\\)，并称\\(E(y|x)=\\beta_0+\\beta_1x\\)为总体回归方程。 其中待估参数为\\(\\beta_0\\)、\\(\\beta_1\\)和\\(\\sigma^2\\)。 结合假定，可知\\(y\\sim~N(\\beta_0+\\beta_1x,\\sigma^2)\\)。 为什么考虑y的条件期望？ 由于\\(\\varepsilon\\)的存在，我们无法直接估计出参数\\(\\beta_0\\)和\\(\\beta_1\\)。结合零均值的假定，我们可以对模型左右两边取期望来消掉\\(\\varepsilon\\)的影响。同时需要注意的是，该期望是条件期望，我们更关注当x取固定值时y的均值。 如果从最优化的角度进行思考，假设对y的任意预测为\\(f(x)\\)，y的条件期望为\\(g(x)=E(y|x)\\)，则g(x)是y的最佳预测。 \\[ \\begin{align} E(y-f(x))^2&amp;=E(y-g(x)+g(x)-f(x))^2\\\\ &amp;=E(y-g(x))^2+E(g(x)-f(x))^2+2E[(y-g(x))(g(x)-f(x))]\\\\ &amp;=E(y-g(x))^2+E(g(x)-f(x))^2\\\\ &amp;\\geq E(y-g(x))^2 \\end{align} \\tag{7.3} \\] 拓展：分位数回归 一般的线性回归都是关注y条件均值，但有些时候我们可以对y的分位数进行回归，即分位数回归。分位数回归相较于均值回归能够获取更多的关于y的分布的信息，例如在保险行业，保险公司可以通过分位数回归来理解不同风险水平下的潜在损失。 上述都是针对总体的理论模型，而对于样本数据\\((x_i,y_i)\\)，则有: \\[ y_i=\\hat y_i + \\hat \\varepsilon=\\hat \\beta_0+ \\hat \\beta_1x_i+e_i \\tag{7.4} \\] \\[ \\hat y_i=\\hat \\beta_0+ \\hat \\beta_1x_i \\tag{7.5} \\] 其中式(7.4)为样本回归模型，式(7.5)为样本回归方程（也称经验回归方程），\\(\\hat y\\)和\\(e\\)（残差）分别是对\\(E(y|x)\\)和\\(\\varepsilon\\)的估计。 无论总体还是样本，带随机扰动项或者残差的叫“回归模型”，不带的叫“回归方程”或“回归函数”。 7.3.2 多元线性回归模型 \\[ \\left\\{ \\begin{array}{c} y=\\beta_0+\\beta_1x_1+...+\\beta_px_p+\\varepsilon\\\\ E(\\varepsilon|x)=0\\\\ Var(\\varepsilon|x)=\\sigma^2 \\end{array} \\right. \\tag{7.6} \\] 等价于 \\[ \\left\\{ \\begin{array}{c} E(y|x)=\\beta_0+\\beta_1x+...+\\beta_px_p\\\\ Var(y|x)=\\sigma^2 \\end{array} \\right. \\tag{7.7} \\] 其中待估参数为\\(\\beta_0\\)、\\(\\beta_1\\)、…、\\(\\beta_p\\)和\\(\\sigma^2\\)。 矩阵表达式为： \\[ \\left\\{ \\begin{array}{ll} Y=X\\beta+\\varepsilon\\\\ E(\\varepsilon)=0\\\\ Var(\\varepsilon)=\\sigma^2I_n \\end{array} \\right. \\tag{7.8} \\] 其中 \\[ Y= \\begin{pmatrix} y_1\\\\ y_2\\\\ \\vdots\\\\ y_n \\end{pmatrix} \\,, X= \\begin{pmatrix} 1 &amp; x_{11} \\cdots x_{1p}\\\\ 1 &amp; x_{21} \\cdots x_{2p}\\\\ \\vdots\\\\ 1 &amp; x_{n1} \\cdots x_{np} \\end{pmatrix} \\,, \\beta= \\begin{pmatrix} \\beta_1\\\\ \\beta_2\\\\ \\vdots\\\\ \\beta_n \\end{pmatrix} \\varepsilon= \\begin{pmatrix} \\varepsilon_1\\\\ \\varepsilon_2\\\\ \\vdots\\\\ \\varepsilon_n \\end{pmatrix} \\tag{7.9} \\] 此时\\(Y \\sim N(X\\beta,\\sigma^2I_n)\\) "],["reg_4.html", "7.4 参数估计", " 7.4 参数估计 7.4.1 最小二乘估计 7.4.1.1 一元场合 对于离差平方和\\(Q(\\beta_0,\\beta_1)\\)，最小二乘法考虑寻找合适的\\(\\hat \\beta_0\\)与\\(\\hat \\beta_1\\)使得残差平方和\\(Q(\\hat \\beta_0,\\hat \\beta_1)\\)最小。 \\[ \\begin{align} Q(\\beta_0,\\beta_1)&amp;=\\sum^n_{i=1}(y_i-\\beta_0-\\beta_1x_i)^2=\\sum^n_{i=1}\\varepsilon^2\\\\ \\Rightarrow Q(\\hat \\beta_0,\\hat \\beta_1)&amp;=\\underset{\\hat \\beta_0, \\hat \\beta_1} {\\arg\\min} \\sum^n_{i=1}(y_i-\\hat \\beta_0-\\hat \\beta_1x_i)^2\\\\ &amp;=\\underset{\\hat \\beta_0, \\hat \\beta_1} {\\arg\\min} \\sum^n_{i=1}(y_i-\\hat y_i)^2\\\\ &amp;=\\underset{\\hat \\beta_0, \\hat \\beta_1} {\\arg\\min} \\sum^n_{i=1}e_i^2 \\end{align} \\tag{7.10} \\] 分别对\\(\\beta_0\\)和\\(\\beta_1\\)求偏导，并使其为0： \\[ \\left\\{ \\begin{array}{ll} \\frac{\\partial Q}{\\partial \\beta_0} \\mid _{\\beta_0=\\hat \\beta_0, \\beta_1=\\hat \\beta_1} &amp;= -2 \\sum\\limits_{i=1}^n (y_i-\\hat \\beta_0-\\hat \\beta_1x_i)=0 \\\\ \\frac{\\partial Q}{\\partial \\beta_1} \\mid _{\\beta_0=\\hat \\beta_0, \\beta_1=\\hat \\beta_1} &amp;= -2 \\sum\\limits_{i=1}^n (y_i-\\hat \\beta_0-\\hat \\beta_1x_i)x_i=0 \\end{array} \\right. \\tag{7.11} \\] 式(7.11)等价于： \\[ \\left\\{ \\begin{array}{ll} \\sum\\limits_{i=1}^n e_i=0 \\\\ \\sum\\limits_{i=1}^n x_ie_i=0 \\end{array} \\right. \\tag{7.12} \\] 这个关系式非常重要 将求和式展开，并稍加整理，可得： \\[ \\left\\{ \\begin{array}{ll} \\bar y=\\hat \\beta_0+\\hat \\beta_1 \\bar x \\\\ \\sum\\limits_{i=1}^n y_ix_i=n\\bar x \\hat \\beta_0+\\hat \\beta_1\\sum\\limits_{i=1}^n x_i^2 \\end{array} \\right. \\tag{7.13} \\] 易得\\(\\hat \\beta_0=\\bar y-\\hat \\beta_1 \\bar x\\)，将其代入可得\\(\\hat \\beta_1\\)： \\[ \\begin{align} n\\bar x \\hat \\beta_0+\\hat \\beta_1\\sum_{i=1}^n x_i^2 &amp;= \\sum_{i=1}^n y_ix_i\\\\ n\\bar x (\\bar y-\\hat \\beta_1 \\bar x) + \\hat \\beta_1\\sum_{i=1}^n x_i^2 &amp;= \\sum_{i=1}^n y_ix_i\\\\ \\hat \\beta_1(\\sum_{i=1}^n x_i^2 - n \\bar x^2) &amp;= \\sum_{i=1}^n y_ix_i - n\\bar x \\bar y\\\\ \\hat \\beta_1 &amp;= \\frac{\\sum\\limits_{i=1}^n(x_i-\\bar x)y_i}{\\sum\\limits_{i=1}^n(x_i-\\bar x)^2}\\\\ \\hat \\beta_1 &amp;= \\frac{\\sum\\limits_{i=1}^n(x_i-\\bar x)(y_i-\\bar y)}{\\sum\\limits_{i=1}^n(x_i-\\bar x)^2} \\end{align} \\tag{7.14} \\] 注意有以下关系，后面还会用到 \\(\\sum\\limits_{i=1}^n(x_i-\\bar x)=0\\) \\(\\sum\\limits_{i=1}^n(x_i-\\bar x)x_i=\\sum\\limits_{i=1}^n(x_i-\\bar x)(x_i-\\bar x)=\\sum\\limits_{i=1}^n(x_i-\\bar x)^2\\) \\(\\sum\\limits_{i=1}^n(x_i-\\bar x)(y_i-\\bar y)\\)=\\(\\sum\\limits_{i=1}^n(x_i-\\bar x)y_i\\) 记\\(L_{xx}=\\sum\\limits_{i=1}^n(x_i-\\bar x)^2\\)、\\(L_{yy}=\\sum\\limits_{i=1}^n(y_i-\\bar y)^2\\)、\\(L_{xy}=\\sum\\limits_{i=1}^n(x_i-\\bar x)(y_i-\\bar y)\\)，则最小二乘估计为： \\[ \\left\\{ \\begin{array}{ll} \\hat \\beta_0=\\bar y - \\hat \\beta_1 \\bar x \\\\ \\hat \\beta_1=\\frac{L_{xy}}{L_{xx}} \\end{array} \\right. \\tag{7.15} \\] 而对于\\(\\sigma^2\\)，很自然地会用残差\\(e\\)进行估计，常用无偏估计量\\(\\hat \\sigma^2\\)进行估计： \\[ \\hat \\sigma^2=\\frac{\\sum\\limits_{i=1}^n e_i^2}{n-2} \\tag{7.16} \\] 在多元回归部分会给出更为详细的证明。 7.4.1.2 多元场合 对于离差平方和\\(Q(\\beta)\\)有 \\[ \\begin{aligned} Q(\\beta) &amp;= (Y-X \\beta)&#39;(Y-X \\beta)\\\\ &amp;=Y&#39;Y- \\beta&#39;X&#39;Y-Y&#39;X \\beta+\\beta&#39;X&#39;X \\beta\\\\ &amp;=Y&#39;Y-2\\beta&#39;X&#39;Y+\\beta&#39;X&#39;X \\beta \\end{aligned} \\tag{7.17} \\] 注意\\(\\beta&#39;X&#39;Y\\)和\\(Y&#39;X \\beta\\)都是标量。 对\\(\\beta\\)求偏导可得： \\[ {\\partial Q \\over \\partial \\beta} \\mid_{\\beta=\\hat \\beta}=-2X&#39;Y+2X&#39;X\\hat \\beta=0\\\\ \\begin{aligned} \\Rightarrow X&#39;X\\hat \\beta &amp;= X&#39;Y\\\\ \\hat \\beta&amp;=(X&#39;X)^{-1}X&#39;Y \\end{aligned} \\tag{7.18} \\] 一阶导条件等价于\\(X&#39;(Y-X\\hat \\beta)=X&#39;e=0\\)，其中\\(e\\)为残差，牢记\\(X&#39;e=0\\) 为了确保有解，要求自变量之间无多重共线性，即矩阵X列满秩，故矩阵X’X可逆。 故最小二乘估计为\\(\\hat \\beta=(X&#39;X)^{-1}Y\\)。 对于拟合值\\(\\hat Y\\)，有： \\[ \\hat Y = X \\hat \\beta = X(X&#39;X)^{-1}X&#39;Y=HY \\tag{7.19} \\] 其中\\(H=X(X&#39;X)^{-1}X&#39;\\)为n阶对称幂等矩阵，即\\(H=H&#39;\\)和\\(H=H^2\\)。\\(H\\)也称为投影矩阵。 实对称矩阵的特征根非0即1，故tr(H)=rank(H)=p+1 则残差向量\\(e\\)为： \\[ \\begin{aligned} e &amp;= Y-\\hat Y\\\\ &amp;=Y-HY\\\\ &amp;=(I-H)Y\\\\ &amp;=(I-H)(X\\beta+\\varepsilon)\\\\ &amp;=X\\beta-HX\\beta+(I-H)\\varepsilon\\\\ &amp;=(I-H)\\varepsilon \\end{aligned} \\tag{7.20} \\] 之后对残差平方和\\(SSE=e&#39;e=\\varepsilon&#39;(I-H)\\varepsilon\\)取期望： tr(AB)=tr(BA) I和H为n阶矩阵；X为p+1阶矩阵 \\[ \\begin{aligned} E(SSE)&amp;=E(\\varepsilon&#39;(I-H)\\varepsilon)\\\\ &amp;=E[tr(\\varepsilon&#39;(I-H)\\varepsilon)]\\\\ &amp;=E[tr((I-H)\\varepsilon\\varepsilon&#39;)]\\\\ &amp;=tr((I-H)E(\\varepsilon\\varepsilon&#39;))\\\\ &amp;=\\sigma^2 tr(I-H)\\\\ &amp;=\\sigma^2 [n-tr(H)]\\\\ &amp;=\\sigma^2 [n-tr(X(X&#39;X)^{-1}X&#39;)]\\\\ &amp;=\\sigma^2 [n-tr((X&#39;X)^{-1}X&#39;X)]\\\\ &amp;=\\sigma^2 [n-p-1]\\\\ \\end{aligned} \\tag{7.21} \\] 故\\(\\sigma^2\\)的无偏估计为\\(\\hat \\sigma^2={SSE \\over n-p-1}\\) 7.4.2 极大似然估计 7.4.2.1 一元场合 在\\(y\\sim~N(\\beta_0+\\beta_1x,\\sigma^2)\\)的假定下，写出对数似然函数： \\[ \\ln (L)=-{n \\over 2} \\ln (2\\pi \\sigma^2) - {1 \\over 2\\sigma^2} \\sum_{i=1}^n [y_i-(\\beta_0+\\beta_1x_i)]^2 \\tag{7.22} \\] 分别对\\(\\beta_0\\)、\\(\\beta_1\\)、\\(\\sigma^2\\)求偏导，可得对应的估计量。其中\\(\\beta_0\\)、\\(\\beta_1\\)与最小二乘估计的结果一致，但\\(\\sigma^2\\)的估计量为\\(\\hat \\sigma^2={\\sum\\limits_{i=1}^n e_i^2 \\over n}\\)，是有偏估计量。 7.4.2.2 多元场合 注意到有\\(Y \\sim N(X\\beta, \\sigma^2I_n)\\)。故对数似然函数为： \\[ \\ln L=-{n \\over 2}\\ln (2\\pi)-{n \\over 2}\\ln (\\sigma^2)-{1 \\over 2\\sigma^2}(Y-X\\beta)&#39;(Y-X\\beta) \\tag{7.23} \\] 要使对数似然函数取得最大值，则需最小化\\((Y-X\\beta)&#39;(Y-X\\beta)\\)，与式(7.17)一致，故\\(\\hat \\beta_{MLE}\\)结果与最小二乘估计一致。而\\(\\sigma^2\\)的估计量为\\(\\hat \\sigma^2={(Y-X\\beta)&#39;(Y-X\\beta) \\over n}\\)，同一元场合。 7.4.3 矩估计 7.4.3.1 一元场合 在前提假定中规定了\\(E(\\varepsilon)=0\\)及\\(Cov(X_i,\\varepsilon)=E(X_i\\varepsilon)=0\\)，注意到残差\\(e\\)是对\\(\\varepsilon\\)的估计，则用样本矩估计总体矩有： \\[ \\left\\{ \\begin{array}{ll} {1 \\over n} \\sum\\limits_{i=1}^n (y_i-\\hat \\beta_0-\\hat \\beta_1 x_i)=0 \\\\ {1 \\over n} \\sum\\limits_{i=1}^n (y_i-\\hat \\beta_0-\\hat \\beta_1 x_i)x_i=0 \\end{array} \\right. \\tag{7.24} \\] 与式(7.8)一致，则估计结果与最小二乘估计相同。 7.4.3.2 多元场合 在多元场合，注意到前提假定\\(E(\\varepsilon)=0\\)和\\(Cov(X_i,\\varepsilon)=E(X_i\\varepsilon)=0\\)，对应的样本矩条件为： \\[ {1 \\over n}X&#39;(Y-X\\hat \\beta)=0\\\\ \\Rightarrow \\hat \\beta =(X&#39;X)^{-1}X&#39;Y \\tag{7.25} \\] 可得矩估计的结果和最小二乘估计相同。 无论一元还是多元，最小二乘估计、极大似然估计和矩估计都用到了零均值、无内生性、无多重共线性（多元场合）的前提假定，其中极大似然估计额外运用了正态分布的假定。可以发现，估计的核心都是\\(X&#39;(Y-X\\hat \\beta)=0\\)，或者说是\\(X&#39;e=0\\)。 注意X’的第一行都是1，用来满足\\(E(\\varepsilon)=0\\)的条件。其余行为不同自变量的观测值，用来满足\\(Cov(X_i,\\varepsilon)=E(X_i\\varepsilon)=0\\)的条件。 7.4.4 几何视角 B站：最小二乘与投影矩阵 "],["reg_5.html", "7.5 最小二乘估计的性质", " 7.5 最小二乘估计的性质 根据高斯-马尔科夫定理，在满足假定的前提下，最小二乘估计为最优线性无偏估计(best linear unbiased estimator，BLUE)。再探讨性质之前，请先回忆一元及多元场合的最小二乘估计值，式(7.15)和式(7.18)。 7.5.1 线性 7.5.1.1 一元场合 式(7.15)给出了最小二乘估计，对其稍加整理即可发现\\(\\beta\\)是\\(y\\)的线性组合。 \\[ \\begin{aligned} \\hat{\\beta_1} &amp;= {L_{xy} \\over L_{xx}} \\\\ &amp;= {\\sum\\limits_{i=1}^n (x_i-\\bar x)(y_i - \\bar y) \\over L_{xx}} \\\\ &amp;= {\\sum\\limits_{i=1}^n (x_i-\\bar x)y_i \\over L_{xx}} \\\\ &amp;= \\sum_{i=1}^n {(x_i-\\bar x) \\over L_{xx}} y_i \\\\ &amp;= \\sum_{i=1}^n k_i y_i \\end{aligned} \\tag{7.26} \\] \\(L_{xx}\\)相当于一个常数，可以放进去或提出来。 \\[ \\begin{aligned} \\hat \\beta_0 &amp;= \\bar y - \\hat \\beta_1 \\bar x \\\\ &amp;= \\sum\\limits_{i=1}^n {1 \\over n}y_i - \\sum\\limits_{i=1}^n {(x_i-\\bar x)\\bar x \\over L_{xx}} y_i \\\\ &amp;= \\sum\\limits_{i=1}^n [{1 \\over n} - {(x_i-\\bar x)\\bar x \\over L_{xx}}]y_i \\end{aligned} \\tag{7.27} \\] 对于拟合值\\(\\hat y_i\\)有 \\[ \\hat y_i=\\hat \\beta_0+\\hat \\beta_1x_i=\\sum_{j=1}^n [\\frac{1}{n}+\\frac{(x_i-\\bar x)(x_j-\\bar x)}{L_{xx}}]y_j=\\sum_{j=1}^n h_{ij}y_j \\tag{7.28} \\] \\(h_{ij}=h_{ji}\\) 对于给定的新值\\(x_0\\)，对应的预测值\\(\\hat y_0\\)有 \\[ \\hat y_0=\\hat \\beta_0+\\hat \\beta_1x_0=\\sum_{j=1}^n [\\frac{1}{n}+\\frac{(x_0-\\bar x)(x_j-\\bar x)}{L_{xx}}]y_j=\\sum_{j=1}^n h_{0j}y_j \\tag{7.29} \\] 7.5.1.2 多元场合 对于最小二乘估计\\(\\hat \\beta\\)有 \\[ \\begin{aligned} \\hat \\beta &amp;= (X&#39;X)^{-1}X&#39;Y \\\\ &amp;= (X&#39;X)^{-1}X&#39;(X\\beta+\\varepsilon) \\\\ &amp;= \\beta+(X&#39;X)^{-1}X&#39;\\varepsilon \\end{aligned} \\tag{7.30} \\] 注意到\\(\\hat \\beta\\)不仅是\\(y\\)的线性组合，还是\\(\\varepsilon\\)的线性组合。 对于拟合值向量\\(\\hat Y\\)，参照式(7.19)有\\(\\hat Y = HY\\)。 对于预测值\\(\\hat y_0\\)，有 \\[ \\hat y_0 = x_0&#39;\\hat \\beta=x_0&#39;(X&#39;X)^{-1}X&#39;Y \\tag{7.31} \\] 对于残差向量\\(e\\)，参照式(7.20)，有\\(e=(I-H)Y=(I-H)\\varepsilon\\)。 7.5.2 无偏性 7.5.2.1 一元场合 \\[ \\begin{aligned} E(\\hat \\beta_1)&amp;= \\sum_{i=1}^n {(x_i-\\bar x) \\over L_{xx}} E(y_i) \\\\ &amp;= \\sum_{i=1}^n {(x_i-\\bar x) \\over L_{xx}}(\\beta_0+\\beta_1x_i) \\\\ &amp;= \\sum_{i=1}^n {(x_i-\\bar x)x_i \\over L_{xx}}\\beta_1 \\\\ &amp;= {L_{xx} \\over L_{xx}}\\beta_1 \\\\ &amp;= \\beta_1 \\end{aligned} \\tag{7.32} \\] \\[ \\begin{aligned} E(\\hat \\beta_0)&amp;=E(\\bar y)-E(\\hat \\beta_1)\\bar x \\\\ &amp;= {\\sum\\limits_{i=1}^n E(y_i) \\over n}-\\beta_1 \\bar x \\\\ &amp;= {\\sum\\limits_{i=1}^n (\\beta_0 + \\beta_1 x) \\over n}-\\beta_1 \\bar x \\\\ &amp;= \\beta_0 + \\beta_1 \\bar x -\\beta_1 \\bar x \\\\ &amp;= \\beta_0 \\end{aligned} \\tag{7.33} \\] 7.5.2.2 多元场合 \\[ \\begin{aligned} E(\\hat \\beta)&amp;=E((X&#39;X)^{-1}X&#39;Y) \\\\ &amp;= (X&#39;X)^{-1}X&#39;E(Y) \\\\ &amp;= (X&#39;X)^{-1}X&#39;X\\beta \\\\ &amp;= \\beta \\end{aligned} \\tag{7.34} \\] 7.5.3 有效性 7.5.3.1 一元场合 不妨令\\(\\tilde \\beta_1=\\sum\\limits_{i=1}^n c_iy_i\\)也是\\(\\beta_1\\)的无偏估计。 \\[ \\begin{aligned} E(\\tilde \\beta_1)&amp;=E(\\sum\\limits_{i=1}^n c_iy_i) \\\\ &amp;= \\sum\\limits_{i=1}^n c_iE(y_i) \\\\ &amp;= \\sum\\limits_{i=1}^n c_i(\\beta_0+\\beta_1x_i) \\\\ &amp;= \\beta_0 \\sum\\limits_{i=1}^n c_i + \\beta_1 \\sum\\limits_{i=1}^n c_ix_i \\\\ &amp;= \\beta_1 \\end{aligned} \\tag{7.35} \\] 根据无偏性，可知\\(\\tilde \\beta_1\\)满足\\(\\sum\\limits_{i=1}^n c_i=0\\)和\\(\\sum\\limits_{i=1}^n c_ix_i=1\\)。 \\[ \\begin{aligned} Var(\\tilde \\beta_1)&amp;=\\sum\\limits_{i=1}^n c_i^2Var(y_i) \\\\ &amp;= \\sum\\limits_{i=1}^n c_i^2 \\sigma^2 \\\\ &amp;= \\sum\\limits_{i=1}^n (c_i-k_i+k_i)^2 \\sigma^2 \\\\ &amp;= \\sigma^2[\\sum\\limits_{i=1}^n(c_i-k_i)^2+\\sum\\limits_{i=1}^nk_i^2+\\sum\\limits_{i=1}^n2(c_i-k_i)k_i] \\\\ &amp;= \\sum\\limits_{i=1}^n(c_i-k_i)^2 \\sigma^2 + Var(\\hat \\beta_1) \\\\ Var(\\tilde \\beta_1) &amp;\\geq Var(\\hat \\beta_1) \\end{aligned} \\tag{7.36} \\] 注意到\\(\\sum\\limits_{i=1}^n c_i=0\\)和\\(\\sum\\limits_{i=1}^n c_ix_i=1\\)，其中 \\[ \\begin{aligned} \\sum\\limits_{i=1}^n2(c_i-k_i)k_i &amp;= 2\\sum\\limits_{i=1}^n c_ik_i-2\\sum\\limits_{i=1}^n k_i^2 \\\\ &amp;=2\\sum\\limits_{i=1}^n c_i{(x_i-\\bar x) \\over \\sum\\limits_{i=1}^n (x_i-\\bar x)^2}-2\\sum\\limits_{i=1}^n ({(x_i-\\bar x) \\over \\sum\\limits_{i=1}^n (x_i-\\bar x)^2})^2 \\\\ &amp;= 2{1 \\over \\sum\\limits_{i=1}^n (x_i-\\bar x)^2}-2{1 \\over \\sum\\limits_{i=1}^n (x_i-\\bar x)^2} \\\\ &amp;= 0 \\end{aligned} \\tag{7.37} \\] 关于\\(\\hat \\beta_0\\)的证明也是类似的，略。 7.5.3.2 多元场合 不妨令\\(\\tilde \\beta=[(X&#39;X)^{-1}X&#39;+A]Y\\)是\\(\\beta\\)的无偏估计。 \\[ \\begin{aligned} E(\\tilde \\beta)&amp;=[(X&#39;X)^{-1}X&#39;+A]E(Y) \\\\ &amp;= [(X&#39;X)^{-1}X&#39;+A]X\\beta \\\\ &amp;= \\beta +AX\\beta \\\\ &amp;= \\beta \\end{aligned} \\tag{7.38} \\] 根据无偏性，可知\\(AX=0\\)。 \\[ \\begin{aligned} Cov(\\hat \\beta)&amp;=[(X&#39;X)^{-1}X&#39;]Cov(Y)[(X&#39;X)^{-1}X&#39;]&#39; \\\\ &amp;= [(X&#39;X)^{-1}X&#39;]\\sigma^2I_n[(X&#39;X)^{-1}X&#39;]&#39; \\\\ &amp;= \\sigma^2 (X&#39;X)^{-1} \\\\ Cov(\\tilde \\beta)&amp;=[(X&#39;X)^{-1}X&#39;+A]Cov(Y)[(X&#39;X)^{-1}X&#39;+A]&#39; \\\\ &amp;= [(X&#39;X)^{-1}X&#39;+A]\\sigma^2I_n[(X&#39;X)^{-1}X&#39;+A]&#39; \\\\ &amp;= \\sigma^2 [(X&#39;X)^{-1}+AA&#39;] \\end{aligned} \\tag{7.39} \\] 注意到\\(AX=0\\) 由于\\(AA&#39;\\)是半正定矩阵（非负定），则\\(Cov(\\tilde \\beta) \\geq Cov(\\hat \\beta)\\)。 7.5.4 方差 7.5.4.1 一元场合 \\(Var(\\hat{\\beta_1})\\) \\[ \\begin{aligned} Var(\\hat \\beta_1)&amp;=Var(\\sum_{i=1}^n {(x_i-\\bar x) \\over L_{xx}} y_i) \\\\ &amp;= \\sum_{i=1}^n {(x_i-\\bar x)^2 \\over L_{xx}^2}\\sigma^2 \\\\ &amp;= {L_{xx} \\over L_{xx}^2}\\sigma^2 \\\\ &amp;= {\\sigma^2 \\over L_{xx}} \\end{aligned} \\tag{7.40} \\] \\(Var(\\hat{\\beta_0})\\) \\[ \\begin{aligned} Var(\\hat \\beta_0)&amp;=Var(\\bar y - \\hat \\beta_1 \\bar x) \\\\ &amp;= Var(\\bar y)+\\bar x^2Var(\\hat \\beta)-2Cov(\\bar y,\\hat \\beta_1 \\bar x) \\\\ &amp;= {\\sigma^2 \\over n}+\\bar x^2{\\sigma^2 \\over L_{xx}} \\\\ &amp;= [{1 \\over n}+{\\bar x^2 \\over L_{xx}}]\\sigma^2 \\end{aligned} \\tag{7.41} \\] 其中 \\[ \\begin{aligned} Cov(\\bar y,\\hat \\beta_1 \\bar x)&amp;={\\bar x^2 \\over n}Cov(\\sum_{i=1}^n y_i,\\sum_{i=1}^n k_iy_i) \\\\ &amp;= {\\bar x^2 \\over n} \\sum_{i=1}^n k_iCov(y_i,y_i) \\\\ &amp;= {\\bar x^2 \\sigma^2 \\over n} \\sum_{i=1}^n k_i \\\\ &amp;= {\\bar x^2 \\sigma^2 \\over n} \\sum_{i=1}^n {x_i-\\bar x \\over L_{xx}} \\\\ &amp;= 0 \\end{aligned} \\tag{7.42} \\] 注意有\\(Cov(\\varepsilon_i,\\varepsilon_j)=0\\)，则\\(Cov(y_i,y_j)=0\\) \\(Cov(\\hat \\beta_0,\\hat \\beta_1)\\) \\[ \\begin{aligned} Cov(\\hat \\beta_0,\\hat \\beta_1) &amp;= Cov(\\bar y - \\hat \\beta_1 \\bar x,\\hat \\beta_1) \\\\ &amp;= Cov(\\bar y,\\hat \\beta_1)-\\bar xCov(\\hat \\beta_1,\\hat \\beta_1) \\\\ &amp;= 0-\\bar x {\\sigma^2 \\over L_{xx}} \\\\ &amp;= -{\\bar x \\over L_{xx}}\\sigma^2 \\end{aligned} \\tag{7.43} \\] 7.5.4.2 多元场合 \\[ \\begin{aligned} Cov(\\hat \\beta) &amp;= E[(\\hat \\beta-E(\\hat \\beta))(\\hat \\beta-E(\\hat \\beta))&#39;] \\\\ &amp;= E[(\\hat \\beta-\\beta)(\\hat \\beta-\\beta)&#39;] \\\\ &amp;= E[(X&#39;X)^{-1}X&#39;\\varepsilon \\varepsilon &#39; X(X&#39;X)^{-1}] \\\\ &amp;= (X&#39;X)^{-1}X&#39;E(\\varepsilon \\varepsilon &#39;) X(X&#39;X)^{-1} \\\\ &amp;= (X&#39;X)^{-1}X&#39;\\sigma^2I_n X(X&#39;X)^{-1} \\\\ &amp;= \\sigma^2(X&#39;X)^{-1} \\end{aligned} \\tag{7.44} \\] 7.5.5 正态性 7.5.5.1 一元场合 根据式(7.26)和式(7.27)、式(7.32)和式(7.33)、式(7.40)和式(7.41)可知，\\(\\hat \\beta_1\\)和\\(\\hat \\beta_0\\)服从正态分布。 \\[ \\begin{gather} \\hat \\beta_1 \\sim N(\\beta_1, \\frac{\\sigma^2}{L_{xx}}) \\\\ \\hat \\beta_0 \\sim N(\\beta_0, [\\frac{1}{n}+\\frac{\\bar x^2}{L_{xx}}]\\sigma^2) \\end{gather} \\tag{7.45} \\] \\(\\hat \\beta_1\\)的正态性源自y的正态性，而y的正态性又源自\\(\\varepsilon\\)的正态性 因此，凡是能被\\(y\\)线性表示的都具有正态性，详见多元场合 7.5.5.2 多元场合 凡是能被\\(Y\\)线性表示的都具有正态性。 \\[ \\begin{gather} \\hat \\beta \\sim N(\\beta, \\sigma^2(X&#39;X)^{-1}) \\\\ \\hat Y = HY \\sim N(X\\beta, \\sigma^2H) \\\\ \\hat y_0 = x_0&#39;(X&#39;X)^{-1}X&#39;Y \\sim N(x_0&#39;\\beta, \\sigma^2x_0&#39;(X&#39;X)^{-1}x_0) \\\\ e=(I-H)Y \\sim N(0, \\sigma^2(I-H)) \\end{gather} \\tag{7.46} \\] 7.5.6 残差 7.5.6.1 一元场合 线性表示 根据式(7.28)，对于残差\\(e_i\\)有 \\[ e_i=y_i-\\hat y_i=y_i-\\sum_{j=1}^n h_{ij}y_j \\tag{7.47} \\] \\(E(e_i)\\) \\[ E(e_i)=E(y_i-\\hat y_i)=(\\beta_0+\\beta_1 x_i)-(\\beta_0+\\beta_1 x_i)=0 \\tag{7.48} \\] \\(Cov(e_i,e_j)\\) 当\\(i \\neq j\\)时： \\[ \\begin{aligned} Cov(e_i,e_j)&amp;=Cov(y_i-\\sum\\limits_{k=1}^nh_{ik}y_k \\, , \\, y_j-\\sum\\limits_{l=1}^nh_{jl}y_l) \\\\ &amp;= -Cov(y_i \\, , \\, h_{ji}y_i)-Cov(y_j \\, , \\, h_{ij}y_j)+\\sum\\limits_{k=1}^n h_{ik}h_{jk}Cov(y_k \\, , \\, y_k) \\\\ &amp;= -h_{ji}\\sigma^2-h_{ij}\\sigma^2+h_{ij}\\sigma^2 \\\\ &amp;= -h_{ij}\\sigma^2 \\end{aligned} \\tag{7.49} \\] 其中 \\[ \\begin{aligned} \\sum\\limits_{k=1}^n h_{ik}h_{jk}&amp;=\\sum\\limits_{k=1}^n [{1 \\over n^2} + {(x_k-\\bar{x})(x_j-\\bar{x}+x_i-\\bar{x}) \\over nL_{xx}}+{(x_i-\\bar{x})(x_j-\\bar{x})(x_k-\\bar{x})^2 \\over L_{xx}^2}] \\\\ &amp;= {1 \\over n} + {(x_i-\\bar{x})(x_j-\\bar{x}) \\over L_{xx}} \\\\ &amp;= h_{ij} \\end{aligned} \\tag{7.50} \\] 当\\(i = j\\)时： \\[ \\begin{aligned} Cov(e_i \\, , \\, e_i)&amp;=Var(e_i) \\\\ &amp;= Var(y_i-\\sum\\limits_{j=1}^nh_{ij}y_j) \\\\ &amp;= Var(y_i)+Var(\\sum\\limits_{j=1}^nh_{ij}y_j)-2Cov(y_i\\, , \\, \\sum\\limits_{j=1}^nh_{ij}y_j) \\\\ &amp;= \\sigma^2 + \\sigma^2 \\sum\\limits_{j=1}^n h_{ij}^2-2h_{ii}\\sigma^2 \\\\ &amp;= \\sigma^2+h_{ii}\\sigma^2-2h_{ii}\\sigma^2 \\\\ &amp;= (1-h_{ii})\\sigma^2 \\end{aligned} \\tag{7.51} \\] 其中 \\[ \\begin{aligned} \\sum\\limits_{j=1}^n h_{ij}^2 &amp;= \\sum\\limits_{j=1}^n [{1 \\over n^2}+{(x_i-\\bar{x})^2(x_j-\\bar{x})^2 \\over L_{xx}^2}+{2(x_i-\\bar{x})(x_j-\\bar{x}) \\over nL_{xx}}] \\\\ &amp;= {1 \\over n}+{(x_i-\\bar{x})^2 \\over L_{xx}} \\\\ &amp;= h_{ii} \\end{aligned} \\tag{7.52} \\] 故 \\[ Cov(e_i\\, , \\, e_j)= \\begin{cases} (1-h_{ii})\\sigma^2, &amp;i=j \\\\ -h_{ij}\\sigma^2, &amp;i \\neq j \\end{cases} \\tag{7.53} \\] 特别的，称\\(h_{ii}=\\frac{1}{n}+\\frac{(x_i-\\bar{x})^2}{L_{xx}}\\)为杠杆值。 杠杆值度量了自变量空间中第\\(i\\)个观测点偏离样本中心的程度。当杠杆值越大时，对应的\\(Var(e_i)\\)越小，在几何上表现为较远的观测点会把回归线尽可能地拉到自身周边，从而降低自身的残差值。对应的观测点也称之为高杠杆点 \\(Cov(e_i, \\hat y_j)\\) 对任意的\\(i\\)和\\(j\\)，由式(7.28)和式(7.47)可得 \\[ \\begin{aligned} Cov(e_i, \\hat y_j)&amp;=Cov(y_i-\\sum_{k=1}^n h_{ik}y_k, \\sum_{k=1}^n h_{jk}y_k) \\\\ &amp;=Cov(y_i,\\sum_{k=1}^n h_{jk}y_k)-Cov(\\sum_{k=1}^n h_{ik}y_k,\\sum_{k=1}^n h_{jk}y_k) \\\\ &amp;= h_{ji}\\sigma^2-\\sigma^2\\sum_{k=1}^n h_{ik}h_{jk} \\\\ &amp;= h_{ji}\\sigma^2-\\sigma^2 h_{ij} \\\\ &amp;= 0 \\end{aligned} \\tag{7.54} \\] \\(Cov(e_i, \\hat y_0)\\) 由式(7.29)可得 \\[ \\begin{aligned} Cov(e_i,\\hat y_0)&amp;= Cov(y_i-\\sum_{j=1}^n h_{ij}y_j, \\sum_{j=1}^n h_{0j}y_j) \\\\ &amp;= Cov(y_i, \\sum_{j=1}^n h_{0j}y_j)-Cov(\\sum_{j=1}^n h_{ij}y_j,\\sum_{j=1}^n h_{0j}y_j) \\\\ &amp;= h_{0i}\\sigma^2-\\sigma^2 \\sum_{j=1}^n h_{ij}h_{0j} \\\\ &amp;= h_{0i}\\sigma^2-\\sigma^2 h_{i0} \\\\ &amp;= 0 \\end{aligned} \\tag{7.55} \\] \\(Cov(e_i, \\hat \\beta_0)\\) 由式(7.27)可得 \\[ \\begin{aligned} Cov(e_i, \\hat \\beta_0)&amp;=Cov(y_i-\\sum_{j=1}^n h_{ij}y_j, \\sum_{j=1}^n [{1 \\over n} - {(x_j-\\bar x)\\bar x \\over L_{xx}}]y_j) \\\\ &amp;= Cov(y_i, \\sum_{j=1}^n [{1 \\over n} - {(x_j-\\bar x)\\bar x \\over L_{xx}}]y_j)-Cov(\\sum_{j=1}^n h_{ij}y_j,\\sum_{j=1}^n [{1 \\over n} - {(x_j-\\bar x)\\bar x \\over L_{xx}}]y_j) \\\\ &amp;= [{1 \\over n} - {(x_i-\\bar x)\\bar x \\over L_{xx}}]\\sigma^2 - \\sigma^2 \\sum_{j=1}^n h_{ij}({1 \\over n} - {(x_j-\\bar x)\\bar x \\over L_{xx}}) \\\\ &amp;= [{1 \\over n} - {(x_i-\\bar x)\\bar x \\over L_{xx}}]\\sigma^2-[{1 \\over n} - {(x_i-\\bar x)\\bar x \\over L_{xx}}]\\sigma^2 \\\\ &amp;= 0 \\end{aligned} \\tag{7.56} \\] \\(Cov(e_i, \\hat \\beta_1)\\) 由式(7.26)可得 \\[ \\begin{aligned} Cov(e_i, \\hat \\beta_1) &amp;= Cov(y_i-\\sum_{j=1}^n h_{ij}y_j, \\sum_{j=1}^n \\frac{(x_j-\\bar x)}{L_{xx}}y_j) \\\\ &amp;= Cov(y_i, \\sum_{j=1}^n \\frac{(x_j-\\bar x)}{L_{xx}}y_j) - Cov(\\sum_{j=1}^n h_{ij}y_j,\\sum_{j=1}^n \\frac{(x_j-\\bar x)}{L_{xx}}y_j) \\\\ &amp;= \\frac{(x_i-\\bar x)}{L_{xx}} \\sigma^2 - \\frac{(x_i-\\bar x)}{L_{xx}} \\sigma^2 \\\\ &amp;= 0 \\end{aligned} \\tag{7.57} \\] \\(\\sum_{i=1}^n e_i = \\sum_{i=1}^n x_ie_i=0\\) 参见式(7.12) 7.5.6.2 多元场合 线性表示 式(7.20)给出了残差的表达式为\\(e=Y-HY=(I-H)Y=(I-H)\\varepsilon\\) \\(E(e)\\) \\[ E(e)=E[(I-H)\\varepsilon]=0 \\tag{7.58} \\] \\(Cov(e)\\) \\[ \\begin{aligned} Cov(e) &amp;= Cov((I-H)Y) \\\\ &amp;= Cov((I-H)\\varepsilon) \\\\ &amp;= (I-H)E(\\varepsilon\\varepsilon&#39;)(I-H)&#39; \\\\ &amp;= \\sigma^2 (I-H) \\end{aligned} \\tag{7.59} \\] \\(Cov(e,\\hat Y)\\) \\[ \\begin{aligned} Cov(e,\\hat Y)&amp;=Cov((I-H)Y,HY) \\\\ &amp;=(I-H)Cov(Y)H&#39; \\\\ &amp;=(I-H)Cov(X\\beta+\\varepsilon)H \\\\ &amp;=\\sigma^2(I-H)H \\\\ &amp;=\\sigma^2(H-H^2) \\\\ &amp;=0 \\end{aligned} \\tag{7.60} \\] \\(Cov(e,\\hat y_0)\\) \\[ \\begin{aligned} Cov(e,\\hat y_0)&amp;=Cov((I-H)Y,x_0&#39;(X&#39;X)^{-1}X&#39;Y) \\\\ &amp;=\\sigma^2(I-H)X(X&#39;X)^{-1}x_0 \\\\ &amp;=\\sigma^2(X(X&#39;X)^{-1}-X(X&#39;X)^{-1}X&#39;X(X&#39;X)^{-1})x_0 \\\\ &amp;=0 \\end{aligned} \\tag{7.61} \\] \\(Cov(e,\\hat \\beta)\\) \\[ \\begin{aligned} Cov(e,\\hat \\beta)&amp;=Cov((I-H)Y,(X&#39;X)^{-1}X&#39;Y) \\\\ &amp;= \\sigma^2(I-H)X(X&#39;X)^{-1} \\\\ &amp;= \\sigma^2[X(X&#39;X)^{-1}-X(X&#39;X)^{-1}X&#39;X(X&#39;X)^{-1}] \\\\ &amp;= 0 \\end{aligned} \\tag{7.62} \\] 由于\\(e\\)和\\(\\hat \\beta\\)都是\\(Y\\)的线性组合，因此都服从正态分布，故协方差为0表示\\(e\\)和\\(\\hat \\beta\\)之间独立，同样的也有SSE或\\(\\hat \\sigma^2\\)与\\(\\hat \\beta\\)独立 \\(X&#39;e=0\\) 参见式(7.18) 在正规方程组中，我们得到的一阶导条件为\\(X&#39;e=0\\)。从几何视角来看，残差向量\\(e\\)正交于X张成的列空间，因此凡是X列空间中的向量均与\\(e\\)不相关 "],["reg_6.html", "7.6 显著性检验", " 7.6 显著性检验 由于显著性检验依赖于最小二乘估计的分布，在前述内容中已经说明最小二乘估计服从正态分布，因此该部分内容严重依赖于随机扰动项的正态性、同方差、无自相关假定。 同时，在显著性检验中涉及t检验和F检验，这就依赖如下条件 \\[ \\frac{SSE}{\\sigma^2} \\sim \\chi^2(n-p-1) \\tag{7.63} \\] 证： \\[ \\begin{aligned} \\frac{SSE}{\\sigma^2}&amp;=\\frac{e&#39;e}{\\sigma^2} \\\\ &amp;=\\frac{\\varepsilon&#39;(I-H)\\varepsilon}{\\sigma^2} \\\\ &amp;=\\frac{\\varepsilon&#39;}{\\sigma}(I-H)\\frac{\\varepsilon}{\\sigma} \\end{aligned} \\tag{7.64} \\] 已知\\(\\varepsilon \\sim N(0, \\sigma^2I)\\)，则\\(\\frac{\\varepsilon}{\\sigma} \\sim N(0,I)\\)。 由于式(7.64)为二次型，且矩阵\\((I-H)\\)为秩为\\(n-p-1\\)的对称幂等矩阵，故存在某种正交变换使得式(7.64)的二次型化为相互独立的变量平方和，也就是卡方分布，其中自由度就是矩阵\\((I-H)\\)的秩。 同时，根据式(7.62)可知\\(\\hat \\beta\\)与\\(SSE/\\sigma^2\\)独立。 7.6.1 区间估计 7.6.1.1 一元场合 根据式(7.45)已知\\(\\hat \\beta_1\\)的分布，由于\\(\\sigma^2\\)未知，因此采用式(7.16)的\\(\\hat \\sigma^2\\)进行替代，进而构造t统计量进行区间估计。 \\[ \\begin{gather} t=\\frac{\\hat \\beta_1-\\beta_1}{\\sqrt{\\hat \\sigma^2/L_{xx}}} \\sim t(n-2) \\\\ P\\begin{pmatrix}\\begin{vmatrix}\\frac{\\hat \\beta_1-\\beta_1}{\\sqrt{\\hat \\sigma^2/L_{xx}}}\\end{vmatrix} &lt; t_{\\alpha/2}(n-2)\\end{pmatrix}=1-\\alpha \\\\ \\begin{pmatrix} \\hat \\beta_1-t_{\\alpha/2}(n-2) \\sqrt{\\frac{\\hat \\sigma^2}{L_{xx}}}, \\; \\hat \\beta_1+t_{\\alpha/2}(n-2) \\sqrt{\\frac{\\hat \\sigma^2}{L_{xx}}}\\end{pmatrix} \\end{gather} \\tag{7.65} \\] 我们在乎自变量是否能解释因变量的变动，因此\\(\\hat \\beta_0\\)的区间估计，包括下面的显著性检验都不对\\(\\hat \\beta_0\\)进行讨论 7.6.1.2 多元场合 回顾式(7.46)，可知\\(\\hat \\beta_j \\sim N(\\beta_j, \\sigma^2c_{jj})\\)，其中\\(c_{jj}\\)表示\\((X&#39;X)^{-1}\\)的第\\(j+1\\)个主对角线元素，故有 \\[ \\begin{gather} t=\\frac{\\hat \\beta_j-\\beta_j}{\\sqrt{\\hat \\sigma^2c_{jj}}} \\sim t(n-p-1) \\\\ P\\begin{pmatrix}\\begin{vmatrix}\\frac{\\hat \\beta_j-\\beta_j}{\\sqrt{\\hat \\sigma^2c_{jj}}}\\end{vmatrix} &lt; t_{\\alpha/2}(n-p-1)\\end{pmatrix}=1-\\alpha \\\\ \\begin{pmatrix} \\hat \\beta_j-t_{\\alpha/2}(n-p-1) \\sqrt{\\hat \\sigma^2 c_{jj}}, \\; \\hat \\beta_j+t_{\\alpha/2}(n-p-1) \\sqrt{\\hat \\sigma^2 c_{jj}}\\end{pmatrix} \\end{gather} \\tag{7.66} \\] 挖坑，回归系数向量的置信域（置信椭球） 7.6.2 t检验 7.6.2.1 一元场合 t检验用于检验单个回归系数是否显著。 对于假设检验问题 \\[ H_0:\\beta_1=0 \\quad vs \\quad H_1:\\beta_1 \\neq 0 \\] 在原假设下有\\(\\hat \\beta_1 \\sim N(0, \\sigma^2/L_{xx})\\)，同样用式(7.16)的\\(\\hat \\sigma^2\\)替代\\(\\sigma^2\\)，进而构造t统计量进行显著性检验。 \\[ t=\\frac{\\hat \\beta_1}{\\sqrt{\\hat \\sigma^2/L_{xx}}} \\tag{7.67} \\] 在原假设下\\(t \\sim t(n-2)\\)，当\\(|t| \\geq t_{\\alpha/2}(n-2)\\)时拒绝原假设。 7.6.2.2 多元场合 对于假设检验问题 \\[ H_0:\\beta_j=0 \\quad vs \\quad H_1:\\beta_j \\neq 0 \\] 在原假设下有\\(\\hat \\beta_j \\sim N(0,\\sigma^2 c_{jj})\\)，故构造检验统计量 \\[ t_j=\\frac{\\hat \\beta_j}{\\sqrt{\\hat \\sigma^2 c_{jj}}} \\tag{7.68} \\] 在原假设下\\(t_j \\sim t(n-p-1)\\)，当\\(|t_j| \\geq t_{\\alpha/2}(n-p-1)\\)时拒绝原假设。 考虑更一般的假设检验问题 \\[ H_0:c&#39;\\beta=0 \\quad vs \\quad H_1:c&#39;\\beta \\neq 0 \\] 有\\(c&#39;\\hat \\beta \\sim N(c&#39;\\beta, \\sigma^2c&#39;(X&#39;X)^{-1}c)\\)，故 \\[ t=\\frac{c&#39;\\hat \\beta}{\\sqrt{\\hat \\sigma^2c&#39;(X&#39;X)^{-1}c}} \\tag{7.69} \\] 原假设下有\\(t \\sim t(n-p-1)\\)，当\\(|t| \\geq t(n-p-1)\\)时拒绝原假设 7.6.3 F检验 7.6.3.1 一元场合 F检验用于检验整个回归方程是否显著，也就是说检验因变量是否与至少一个自变量存在线性关系。特别的，一元场合只有一个自变量，因此F检验也就相当于检验\\(\\beta_1\\)是否为0。 对于假设检验问题 \\[ H_0:\\beta_1=0 \\quad vs \\quad H_1:\\beta_1 \\neq 0 \\] 构造F统计量 \\[ F=\\frac{SSR/1}{SSE/(n-2)} \\tag{7.70} \\] 其中\\(SST=\\sum_{i=1}^n(y_i-\\bar y)^2, \\; SSR=\\sum_{i=1}^n(\\hat y_i -\\bar y)^2, \\; SSE=\\sum_{i=1}^n(y_i-\\hat y_i)^2\\)。 在原假设下\\(F \\sim F(1,n-2)\\)，当\\(F \\geq F_\\alpha(1,n-2)\\)时，拒绝原假设。 注意到，在一元线性回归中，F统计量与t统计量有如下关系式 \\[ t^2=\\begin{pmatrix}\\frac{\\hat \\beta_1}{\\sqrt{\\hat \\sigma^2/L_{xx}}}\\end{pmatrix}^2=\\frac{\\hat \\beta_1^2L_{xx}}{SSE/(n-2))}=\\frac{SSR}{SSE/(n-2)}=F \\tag{7.71} \\] 其中 \\[ \\begin{aligned} SSR&amp;=\\sum_{i=1}^n (\\hat y_i - \\bar y)^2 \\\\ &amp;= \\sum_{i=1}^n (\\hat \\beta_0 + \\hat \\beta_1x_i-\\bar y)^2 \\\\ &amp;= \\sum_{i=1}^n (\\bar y - \\hat \\beta_1 \\bar x + \\hat \\beta_1x_i-\\bar y)^2 \\\\ &amp;= \\sum_{i=1}^n \\hat \\beta_1^2(x_i-\\bar x)^2 \\\\ &amp;= \\hat \\beta_1^2 L_{xx} \\end{aligned} \\tag{7.72} \\] 平方和分解式 \\[ \\begin{aligned} SST&amp;=\\sum_{i=1}^n(y_i-\\bar y)^2 \\\\ &amp;= \\sum_{i=1}^n(y_i-\\hat y_i+\\hat y_i-\\bar y)^2 \\\\ &amp;=\\sum_{i=1}^n(\\hat y_i - \\bar y)^ + \\sum_{i=1}^n (y_i-\\hat y_i)^2 + 2\\sum_{i=1}^n(y_i-\\hat y_i)(\\hat y_i - \\bar y) \\\\ &amp;= SSR+SSE+2\\sum_{i=1}^n(y_i-\\hat y_i)\\hat y_i - 2\\bar y \\sum_{i=1}^n (y_i-\\hat y_i) \\\\ &amp;= SSR+SSE+2\\sum_{i=1}^n e_i(\\hat \\beta_0+\\hat \\beta_1x_i)-2\\bar y \\sum_{i=1}^n e_i \\\\ &amp;= SSR+SSE +2\\hat \\beta_0 \\sum_{i=1}^n e_i+2\\hat \\beta_1 \\sum_{i=1}^n e_ix_i \\\\ &amp;= SSR+SSE \\end{aligned} \\tag{7.73} \\] 注意式(7.12)表明\\(\\sum_{i=1}^n e_i=0, \\; \\sum_{i=1}^n e_ix_i=0\\)。 关于式(7.70)的原理，可参考1999年王松桂《线性统计模型：线性回归与方差分析》中的定理4.1.1，该定理包括 (a) \\(RSS/\\sigma^2 \\sim \\chi^2_{n-p}\\) (b) 若约束条件\\(A\\beta=b\\)成立，则\\((RSS_H-RSS/\\sigma^2) \\sim \\chi^2_m\\) (c) \\(RSS\\)与\\(RSS_H-RSS\\)相互独立 (d) 当约束条件\\(A\\beta=b\\)成立，则 \\[ F_H = \\frac{(RSS_H-RSS)/m}{RSS/(n-p)} \\sim F_{m,n-p} \\] 其中\\(RSS_H\\)表示受约束的最小二乘估计对应的残差平方和。 7.6.3.2 多元场合 整个回归方程的显著性检验同样采用F检验进行。 对于假设检验问题 \\[ H_0: \\beta_1=...=\\beta_p=0 \\quad vs \\quad H_1: \\exists \\beta_i \\neq 0, \\; i\\in \\{1,...,p\\} \\] 构造F统计量 \\[ F=\\frac{SSR/p}{SSE/(n-p-1)} \\tag{7.74} \\] 在原假设下，\\(F\\sim F(p,n-p-1)\\)，当\\(F \\geq F_\\alpha(p,n-p-1)\\)时即可拒绝原假设。 下面对F检验进行推广。 考虑部分回归系数的显著性检验问题，不妨令\\(\\beta_2\\)为\\(\\beta\\)中假设系数为0的那部分系数，对应的自变量有\\(p^*\\)个，记为\\(X_2\\)。剩余的系数和自变量个数为\\(\\beta_1\\)和\\(p-p^*\\)个，自变量记为\\(X_1\\)。 更一般的线性假设问题及证明可参考1999年王松桂《线性统计模型：线性回归与方差分析》中的4.1节，其中的线性假设为\\(A\\beta=b\\) 对于假设检验问题 \\[ H_0:\\beta_2=0 \\quad vs \\quad H_1:\\beta_2 \\neq 0 \\] 对于同一样本，无约束回归与有约束回归对应的\\(SST\\)都是一致的。而在约束条件\\(\\beta_2=0\\)下，对应的残差平方和\\(SSE^*\\)必定大于等于无约束条件下的残差平方和\\(SSE\\)，即\\(SSE^* \\geq SSE\\)。注意到有\\(SSR-SSR^*=SSE^*-SSE\\)，结合式(7.63)，在原假设下有 \\[ F=\\frac{(SSE^*-SSE)/(p-p^*)}{SSE/(n-p-1)} \\sim F(p-p^*,n-p-1) \\tag{7.75} \\] \\(SSE^{\\ast} / \\sigma^2 \\sim \\chi^2(n-p^{\\ast}-1)\\) \\(SSE/\\sigma^2 \\sim \\chi^2(n-p-1)\\) \\((SSE^\\ast-SSE) / \\sigma^2 \\sim \\chi^2(p-p^\\ast)\\) 因此，该检验统计量通过度量\\(SSE^*-SSE\\)的差异大小来检验约束条件是否显著存在。若约束条件真的存在，则\\(SSE^*-SSE\\)之间的差异自然就小；若约束条件不存在，则\\(SSE^*-SSE\\)之间的差异自然就大。 多元场合的平方和分解式的表达 \\[ \\begin{gather} SST=\\sum_{i=1}^n(y_i-\\bar y)^2=\\sum_{i=1}^n [(1-\\frac{1}{n})y_i -\\frac{1}{n}\\sum_{j \\neq i}y_j]^2=Y&#39;(I-\\frac{1}{n}1_n1_n&#39;)Y \\\\ SSE=\\sum_{i=1}^n(y_i-\\hat y_i)^2=Y&#39;(I-H)Y \\\\ SSR=SST-SSE=Y&#39;(H-\\frac{1}{n}1_n1_n&#39;)Y \\end{gather} \\tag{7.76} \\] 其中\\(1_n\\)表示长度为n且元素均为1的向量。 7.6.4 偏F检验 在多元场合中，根据式(7.75)的启示，可以假设某一自变量对应的回归系数为0，根据约束前后残差平方和的差异大小来判断该自变量的重要性，称此检验为偏F检验。 假设检验问题为 \\(H_0:\\beta_j=0 \\quad vs \\quad H_1:\\beta_j \\neq 0\\) 则检验统计量为 \\[ F_j=\\frac{(SSE_{(-j)}-SSE)/1}{SSE/(n-p-1)} \\tag{7.77} \\] \\(SSE_{(-j)}=Y&#39;(I-H_0)Y, \\; H_0=X_0(X_0&#39;X_0)^{-1}X_0&#39;\\)，其中\\(X_0\\)表示剔除变量\\(x_j\\)后的设计矩阵 其中\\(SSE_{(-i)}\\)表示去掉第\\(i\\)个自变量后所拟合模型的残差平方和。在原假设下，由\\(F_i \\sim F(1,n-p-1)\\)。当\\(F \\geq F_\\alpha(1,n-p-1)\\)时拒绝原假设。 若约束前后残差平方和变化过大，说明该自变量较为重要，此时\\(F_i\\)的值会较大，倾向于拒绝原假设。 \\(\\beta_j\\)的t检验统计量与偏F检验统计量有如下关系 \\[ t_j^2=F_j \\tag{7.78} \\] 证：挖坑 7.6.5 样本决定系数 样本决定系数定义如下 \\[ R^2=\\frac{SSR}{SST}=\\frac{\\sum_{i=1}^n (\\hat y_i-\\bar y)^2}{\\sum_{i=1}^n (y_i-\\bar y)^2} \\tag{7.79} \\] 也称拟合优度、判定系数、确定系数。 \\(R^2\\)反映了因变量的变异(SST)中可以由自变量解释(SSR)的比例. 关于\\(R^2\\)这里推荐阅读统计之都的文章《为什么我不是R方的粉丝》 7.6.5.1 一元场合 在一元线性回归中，\\(R^2\\)与样本相关系数具有如下关系 \\[ R^2=\\frac{SSR}{SST}=\\frac{\\hat \\beta_1^2 L_{xx}}{L_{yy}}=\\frac{L_{xy}^2}{L_{xx}L_{yy}}=r^2 \\tag{7.80} \\] 7.6.5.2 多元场合 在多元场合中，样本决定系数\\(R^2\\)与\\(Cor(\\hat Y, Y)\\)具有如下关系 \\[ \\begin{aligned} Cor(\\hat Y, Y)&amp;=\\frac{(\\hat Y - 1_n\\bar y)&#39;(Y-1_n\\bar y)}{\\sqrt{SSR \\times SST}} \\\\ &amp;= \\frac{(\\hat Y - 1_n\\bar y)&#39;(\\hat Y + e -1_n\\bar y)}{\\sqrt{SSR \\times SST}} \\\\ &amp;= \\frac{(\\hat Y - 1_n\\bar y)&#39;(\\hat Y-1_n\\bar y)+(\\hat Y - 1_n\\bar y)&#39;e}{\\sqrt{SSR \\times SST}} \\\\ &amp;= \\frac{SSR+0}{\\sqrt{SSR \\times SST}} \\\\ &amp;= \\sqrt{\\frac{SSR}{SST}} \\\\ &amp;= \\sqrt{R^2} \\end{aligned} \\tag{7.81} \\] \\(e\\)与\\(\\hat Y\\)正交，且\\(\\sum_{i=1}^n e_i=0\\) 定义样本复相关系数为 \\[ R=\\sqrt{R^2}=\\sqrt{\\frac{SSR}{SST}} \\tag{7.82} \\] 反映了因变量与一组自变量间的相关性 定义调整的\\(R^2\\)为 \\[ R_{adj}^2 = 1-\\frac{SSE/(n-p-1)}{SST/(n-1)}=1-\\frac{n-1}{n-p-1}(1-R^2) \\tag{7.82} \\] 普通\\(R^2\\)会随着自变量的增加而单调增加，而调整的\\(R^2\\)相较于普通\\(R^2\\)多了对自变量个数的惩罚，因此可用于不同自变量个数下不同模型之间拟合效果的比较。 "],["reg_7.html", "7.7 预测", " 7.7 预测 7.7.1 预测因变量新值的均值 7.7.1.1 一元场合 回顾式(7.2)，注意我们的线性回归模型是对\\(E(y|x)\\)，简记为\\(E(y)\\)，即对因变量的条件均值进行回归。因此，给定自变量\\(x_0\\)，对\\(E(y)\\)的一个自然的点估计就是 \\[ \\hat E(y_0)=\\hat y_0=\\hat \\beta_0 + \\hat \\beta_1 x_0 \\tag{7.83} \\] 根据式(7.29)，可知 \\[ \\hat y_0 = \\hat \\beta_0 + \\hat \\beta_1x_0 \\sim N(\\beta_0+\\beta_1 x_0, (\\frac{1}{n}+\\frac{(x_0-\\bar x)^2}{L_{xx}})\\sigma^2) \\tag{7.84} \\] 构造枢轴量 \\[ \\frac{\\hat y_0 - E(y_0)}{\\sqrt{\\hat \\sigma^2(\\frac{1}{n}+\\frac{(x_0-\\bar x)^2}{L_{xx}})}} \\sim t(n-2) \\tag{7.85} \\] 故\\(E(y_0)\\)的区间估计为\\(\\hat y_0 \\pm t_{\\alpha/2}(n-2)\\sqrt{\\hat \\sigma^2(\\frac{1}{n}+\\frac{(x_0-\\bar x)^2}{L_{xx}})}\\)。 7.7.1.2 多元场合 同样，一个自然的点估计就是\\(\\hat y_0\\)。 在正态假设下，有 \\[ \\begin{gather} \\hat y_0 = x_0&#39;(X&#39;X)^{-1}X&#39;Y \\sim N(x_0&#39;\\beta, \\sigma^2x_0&#39;(X&#39;X)^{-1}x_0) \\\\ \\hat y_0-E(y_0) \\sim N(0, \\sigma^2x_0&#39;(X&#39;X)^{-1}x_0) \\\\ t=\\frac{\\hat y_0 -E(y_0)}{\\sqrt{\\hat \\sigma^2x_0&#39;(X&#39;X)^{-1}x_0}} \\sim t(n-p-1) \\\\ \\hat y_0 \\pm t_{\\alpha/2}(n-p-1)\\sqrt{\\hat \\sigma^2x_0&#39;(X&#39;X)^{-1}x_0} \\end{gather} \\tag{7.86} \\] 7.7.2 预测因变量的新值 7.7.2.1 一元场合 因变量的新值为\\(y_0\\)，相较于因变量的均值\\(E(y_0)\\)，我们需要考虑随机扰动项的影响，即\\(y_0=E(y_0)+\\varepsilon\\)。 对\\(y_0\\)的点估计依旧是\\(\\hat y_0\\)。 对\\(y_0\\)的区间估计则先构造出枢轴量，有 \\[ y_0 - \\hat y_0 \\sim N(0,(1+\\frac{1}{n}+\\frac{(x_0-\\bar x)^2}{L_{xx}})\\sigma^2) \\tag{7.87} \\] 其中方差里的“1”就是纳入了随机扰动项的影响。同样用\\(\\hat \\sigma^2\\)来估计\\(\\sigma^2\\)，根据t分布得到区间估计为\\(\\hat y_0 \\pm t_{\\alpha/2}(n-2)\\sqrt{\\hat \\sigma^2(1+\\frac{1}{n}+\\frac{(x_0-\\bar x)^2}{L_{xx}})}\\)。 7.7.2.2 多元场合 同样，点估计为\\(\\hat y_0\\)。 在正态性假设下，有 \\[ \\begin{gather} \\hat y_0 = x_0&#39;(X&#39;X)^{-1}X&#39;Y \\sim N(x_0&#39;\\beta, \\sigma^2x_0&#39;(X&#39;X)^{-1}x_0) \\\\ y_0-\\hat y_0 \\sim N(0, \\sigma^2(1+x_0&#39;(X&#39;X)^{-1}x_0)) \\\\ t=\\frac{y_0-\\hat y_0}{\\sqrt{\\hat \\sigma^2(1+x_0&#39;(X&#39;X)^{-1}x_0})} \\sim t(n-p-1) \\\\ \\hat y_0 \\pm t_{\\alpha/2}(n-p-1)\\sqrt{\\hat \\sigma^2(1+x_0&#39;(X&#39;X)^{-1}x_0)} \\end{gather} \\tag{7.88} \\] "],["reg_8.html", "7.8 回归系数的解释", " 7.8 回归系数的解释 对于多元线性回归模型 \\[ \\left\\{ \\begin{array}{c} E(y|x)=\\beta_0+\\beta_1x+...+\\beta_px_p\\\\ Var(y|x)=\\sigma^2 \\end{array} \\right. \\] 截距项\\(\\beta_0\\)反映了当自变量均取0时因变量的期望。 而对于自变量的回归系数，理论上来说，\\(\\beta_i\\)表示当固定其他自变量不变时，\\(x_i\\)每增加一个单位，因变量的期望能够变化\\(\\beta_i\\)个单位。实际上，自变量之间往往具有相关性，可能无法做到固定某些自变量的值而改变其他自变量的值。也就是说，自变量之间所提供的信息是有重叠的部分。 考虑自变量\\(X_i\\)的影响，记其余自变量对应的设计矩阵为\\(X_0\\)，对应的帽子矩阵为\\(H_0=X_0(X_0&#39;X_0)^{-1}X_0&#39;\\)，则 \\[ \\begin{aligned} Y&amp;=X_i\\beta_i+X_0\\beta_0+\\varepsilon \\\\ (I-H_0)Y&amp;=(I-H_0)X_i\\beta_i+(I-H_0)X_0\\beta_0+(I-H_0)\\varepsilon \\\\ e_{Y|X_0}&amp;=e_{X_i|X_0}\\beta_i+(I-H_0)\\varepsilon \\end{aligned} \\tag{7.89} \\] 一般的多元线性回归中\\(e=(I-H)Y\\)，即\\((I-H)\\)的作用是为了得到残差，而\\(H\\)则决定了是对谁而言的残差。像这里的\\(H_0\\)是对\\(X_0\\)而言的，也就是经过\\(X_0\\)调整后的残差 其中\\(e_{a|b}\\)表示a对b回归得到的残差，即a中不能由b线性解释的部分，称为“经过b调整后的a”。 上式表明原始多元线性回归中的\\(\\beta_i\\)与经过\\(X_0\\)调整过后的\\(Y\\)对经过\\(X_0\\)调整过后的\\(X_i\\)回归得到的回归系数是一致的。 特别的，经过\\(X_0\\)调整过后的\\(Y\\)对经过\\(X_0\\)调整过后的\\(X_i\\)回归的最小二乘估计为 \\[ \\hat \\beta_i = (X_i&#39;(I-H_0)X_i)^{-1}X_i&#39;(I-H)Y \\tag{7.90} \\] 注意有\\((I-H_0)=(I-H_0)^2\\) 既然原始多元线性回归中的\\(\\beta_i\\)与经过\\(X_0\\)调整过后的\\(Y\\)对经过\\(X_0\\)调整过后的\\(X_i\\)回归得到的回归系数是一致的，那么\\(\\beta_i\\)也反映了经过其余自变量线性调整后\\(x_i\\)对\\(y\\)额外的贡献，也称\\(\\beta_i\\)为偏回归系数。 于是，称\\(e_{Y|X_0}\\)与\\(e_{X_i|X_0}\\)的散点图为偏回归图或附加变量图。 对该图拟合最小二乘回归线，其斜率就是\\(\\hat \\beta_i\\) 若附加变量图中的线性关系越强，说明新增变量\\(x_i\\)对已包含其余变量的回归方程增加的贡献就越大 "],["reg_9.html", "7.9 中心化与标准化", " 7.9 中心化与标准化 各个变量的量纲不同，会导致原始设计矩阵的数值差异较大，基于该设计矩阵得到的最小二乘估计不具有可比性。 7.9.1 中心化 中心化处理，即变量减去其均值。中心化的意义能够将未知参数的个数降低1，并在一定程度上降低舍入误差。 记\\(X=\\begin{pmatrix}1_n &amp; \\tilde X\\end{pmatrix}, \\; \\beta = \\begin{pmatrix}\\beta_0 \\\\ \\tilde \\beta \\end{pmatrix}, \\; \\gamma = \\begin{pmatrix}\\gamma_0 \\\\ \\tilde \\gamma \\end{pmatrix}, \\; \\alpha = \\begin{pmatrix}\\alpha_0 \\\\ \\tilde \\alpha \\end{pmatrix}, \\;\\bar X = \\begin{pmatrix} \\bar x_1 &amp; \\cdots &amp; \\bar x_p \\end{pmatrix}&#39;\\)。 \\(X\\)的第一列均为1，\\(\\tilde X\\)才是纯粹的自变量矩阵，注意区分 则 \\[ \\begin{gather} \\tilde X_c = \\begin{pmatrix}I-\\frac{1}{n}1_n1_n&#39; \\end{pmatrix}\\tilde X \\\\ Y_c = Y-1_n\\bar y \\end{gather} \\tag{7.91} \\] \\(1_n\\)表示长度为n且元素均为1的列向量 其中\\((I-\\frac{1}{n}1_n1_n&#39;)\\)为中心化矩阵，下标\\(c\\)表示经过中心化处理后的矩阵或向量。 对此得到如下样本回归模型及相应的最小二乘估计 原始模型 \\[ \\begin{gather} E(Y)=1_n\\beta_0+\\tilde X \\tilde \\beta \\\\ \\\\ \\begin{pmatrix} \\hat \\beta_0 \\\\ \\hat{\\tilde \\beta} \\end{pmatrix}=\\begin{pmatrix} \\bar y - \\bar X&#39;\\hat{\\tilde \\beta} \\\\ (\\tilde X_c&#39;\\tilde X_c)^{-1}\\tilde X_c&#39; Y \\end{pmatrix} \\end{gather} \\tag{7.91} \\] 对X进行中心化处理 \\[ \\begin{gather} E(Y)=1_n\\gamma_0+\\tilde X_c \\tilde \\gamma \\\\ \\\\ \\begin{pmatrix} \\hat \\gamma_0 \\\\ \\hat{\\tilde \\gamma} \\end{pmatrix}=\\begin{pmatrix} \\bar y \\\\ (\\tilde X_c&#39;\\tilde X_c)^{-1}\\tilde X_c&#39; Y \\end{pmatrix}=\\begin{pmatrix} \\bar y \\\\ \\hat{\\tilde \\beta} \\end{pmatrix} \\end{gather} \\tag{7.92} \\] 对Y和X进行中心化处理 \\[ \\begin{gather} E(Y_c)=1_n\\alpha_0+\\tilde X_c \\tilde \\alpha \\\\ \\\\ \\begin{pmatrix} \\hat \\alpha_0 \\\\ \\hat{\\tilde \\alpha} \\end{pmatrix}=\\begin{pmatrix} 0 \\\\ (\\tilde X_c&#39;\\tilde X_c)^{-1}\\tilde X_c&#39; Y \\end{pmatrix}=\\begin{pmatrix} 0 \\\\ \\hat{\\tilde \\beta} \\end{pmatrix} \\end{gather} \\tag{7.93} \\] 小结： 仅对X进行中心化处理，则斜率项的估计不变，截距项的估计值变为\\(\\bar y\\)。 对Y和X进行中心化处理，则斜率项的估计不变，截距项的估计值变为0。 自变量和因变量任何形式的位移变化均不改变斜率项的估计值，继而也不改变线性回归模型的拟合优度。 7.9.2 标准化 变量减去其均值并除以其标准差即为标准化处理。标准化处理能够消除量纲不同和数量级差异所带来的影响。 沿用中心化的记号，并记\\(D_X=diag(sd(x_1),...,sd(x_p))\\)。 则 \\[ \\begin{gather} \\tilde X^*=\\tilde X_cD_X^{-1}=(I-\\frac{1}{n}1_n1_n&#39;)\\tilde X D_X^{-1} \\\\ Y^*=\\frac{Y_c}{sd(y)}=\\frac{1}{sd(y)}(I-\\frac{1}{n}1_n1_n&#39;)Y \\end{gather} \\tag{7.94} \\] 对此得到如下样本回归模型及相应的最小二乘估计 仅对X进行标准化处理 \\[ \\begin{gather} E(Y)=1_n\\delta_0+\\tilde X^*\\tilde \\delta \\\\ \\\\ \\begin{pmatrix} \\hat \\delta_0 \\\\ \\hat{\\tilde \\delta} \\end{pmatrix}= \\begin{pmatrix} \\bar y \\\\ (\\tilde{X^{\\ast}}&#39;\\tilde{X^\\ast})^{-1}\\tilde{X^{\\ast}}&#39;Y \\end{pmatrix} = \\begin{pmatrix} \\bar y \\\\ D_X\\hat{\\tilde \\beta} \\end{pmatrix} \\end{gather} \\tag{7.95} \\] 对Y和X进行标准化处理 \\[ \\begin{gather} E(Y^*)=1_n\\eta_0+\\tilde X^*\\tilde \\eta \\\\ \\\\ \\begin{pmatrix} \\hat \\eta_0 \\\\ \\hat{\\tilde \\eta} \\end{pmatrix}= \\begin{pmatrix} 0 \\\\ (\\tilde{X^{\\ast}}&#39;\\tilde{X^\\ast})^{-1}\\tilde{X^{\\ast}}&#39;Y^* \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{1}{sd(y)}D_X\\hat{\\tilde \\beta} \\end{pmatrix} \\end{gather} \\tag{7.96} \\] 小结： 仅对X进行标准化处理，由于标准化处理中包含了中心化处理，因此截距项为\\(\\bar y\\)，而斜率项则为原来的\\(sd(x_i)\\)倍。 \\(\\beta_i^\\ast \\frac{x_i}{sd(x_i)}=\\frac{\\beta_i^\\ast}{sd(x_i)}x_i=\\beta_i x_i\\)，故\\(\\beta_i^*=sd(x_i)\\beta_i\\) 对Y和X进行标准化处理，则截距项变为0，斜率项为原来的\\(\\frac{sd(x_i)}{sd(y)}\\)倍。 \\(\\frac{sd(y)}{sd(x_i)} \\beta_i^\\ast=\\beta_i\\)，故\\(\\beta_i^\\ast = \\frac{sd(x_i)}{sd(y)} \\beta_i\\) 标准化涉及到尺度变换和位移变化，因此既有中心化的特征（截距项），又有倍数关系（斜率项）。 "],["reg_10.html", "7.10 相关系数与偏相关系数", " 7.10 相关系数与偏相关系数 7.10.1 样本相关系数 定义两个变量间的相关系数 \\[ r=\\frac{\\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y)}{\\sqrt{\\sum_{i=1}^n (x_i-\\bar x)^2\\sum_{i=1}^n (y_i-\\bar y)^2}}=\\frac{L_{xy}}{\\sqrt{L_{xx}L_{yy}}} \\tag{7.97} \\] 样本相关系数反映了两个变量间线性关系的密切程度。特别的，样本相关系数为0并不意味着两个变量之间没有任何关系，只是没有线性相关关系。样本相关系数的大小与样本量有关，当样本量较小时，\\(|r|\\)容易接近1，当样本量较大时，\\(|r|\\)容易偏小。 7.10.1.1 样本相关系数的显著性检验 考虑两个变量间的一元线性回归模型。回顾式(7.80)与式(7.70)，可得 \\[ \\begin{aligned} F&amp;=\\frac{SSR/1}{SSE/(n-2)} \\\\ &amp;= (n-2)\\frac{SSR/SST}{SSE/SST} \\\\ &amp;= (n-2)\\frac{r^2}{1-r^2} \\\\ r^2&amp;=\\frac{F}{F+n-2} \\end{aligned} \\tag{7.98} \\] 故样本相关系数的显著性检验可通过一元场合的F检验进行，其中\\(F \\sim F(1,n-2)\\)，原假设为\\(\\rho=0\\)。 或者根据一元场合式(7.67)可得 \\[ \\begin{aligned} t&amp;=\\frac{\\hat \\beta_1}{\\sqrt{\\hat \\sigma^2/L_{xx}}} \\\\ &amp;= \\frac{\\hat \\beta_1\\sqrt{L_{xx}}}{\\sqrt{SSE/(n-2)}} \\\\ &amp;= \\frac{\\sqrt{n-2}\\frac{L_{xy}}{\\sqrt{L_{xx}L_{yy}}}}{\\sqrt{SSE/L_{yy}}} \\\\ &amp;= \\frac{\\sqrt{(n-2)}r}{\\sqrt{1-r^2}} \\end{aligned} \\tag{7.99} \\] 一元场合中还有式(7.71)的关系:\\(t^2=F\\) 此时可根据\\(t \\sim t(n-2)\\)的显著性检验，原假设为\\(\\rho=0\\)。 7.10.2 样本偏相关系数 简单样本相关系数度量了两个变量间的相关性。但在多元相关分析中，由于受到其他变量的影响，简单样本相关系数并不能反映两个变量间纯粹的相关性，需要控制其他变量的影响，对此引入样本偏相关系数。 自变量间的样本偏相关系数 在样本相关阵r中，记\\(\\Delta_{ij}\\)为r的第i行第i列元素的代数余子式，以\\(x_1\\)与\\(x_2\\)为例，定义样本偏相关系数为 \\[ r_{12;3,...p}=\\frac{-\\Delta_{12}}{\\sqrt{\\Delta_{11}\\Delta_{22}}} \\tag{7.100} \\] 因变量与自变量的样本偏相关系数 记除\\(x_1\\)之外的自变量为\\(x_{(-1)}\\)，\\(e_{x_1|x_{(-1)}}\\)和\\(e_{y|x_{(-1)}}\\)分别表示\\(x_1\\)和\\(y\\)对\\(x_{(-1)}\\)回归的残差，定义\\(y\\)与\\(x_1\\)的样本偏相关系数为 \\[ r_{y1;2,...p}=\\frac{Cov(e_{x_1|x_{(-1)}},e_{y|x_{(-1)}})}{\\sqrt{Var(e_{x_1|x_{(-1)}})Var(e_{y|x_{(-1)}})}}=Cor(e_{x_1|x_{(-1)}},e_{y|x_{(-1)}}) \\tag{7.101} \\] 控制其他变量的影响就是考虑这些变量回归后的残差 因变量与自变量的样本偏决定系数 定义\\(y\\)与\\(x_1\\)的样本偏决定系数为 \\[ r_{y1;2,...p}^2 = \\frac{SSE_{(-1)}-SSE}{SSE_{(-1)}} \\tag{7.102} \\] 若把\\(SSE_{(-1)}\\)看成\\(SST\\)，则形式同\\(R^2\\) 样本偏决定系数反映了引入该新自变量后，因变量剩余变差的相对减少了。 则\\(y\\)与\\(x_1\\)的样本偏相关系数也可为 \\[ r_{y1;2,...p} = \\sqrt{ \\frac{SSE_{(-1)}-SSE}{SSE_{(-1)}}} \\tag{7.103} \\] 正如样本相关系数与F统计量有关系，这里的样本偏相关系数（或者说样本偏决定系数）也与偏F统计量有关系。 同式(7.98)，若把\\(SSE_{(-1)}\\)看成\\(SST\\)，则同理有 \\(SST=\\sum_{i=1}^n (y_i-\\bar y)^2\\)又何尝不是均值模型的残差平方和呢？ \\[ \\begin{aligned} F_1&amp;=\\frac{(n-p-1)r_{y1;2,...p}^2}{1-r_{y1;2,...p}^2} \\\\ r_{y1;2,...p}^2 &amp;= \\frac{F_1}{F_1+n-p-1} \\end{aligned} \\tag{7.104} \\] "],["reg_11.html", "7.11 重要的定义和等式", " 7.11 重要的定义和等式 Gauss-Markov条件 \\[ \\begin{gather} E(\\varepsilon)=0 \\\\ Var(\\varepsilon_i)=\\sigma^2 \\\\ Cov(\\varepsilon_i, \\varepsilon_j)=0, \\; i \\neq j \\end{gather} \\] 一元回归中\\(\\hat \\beta_1\\)与样本相关系数\\(r\\)、回归平方和\\(SSR\\) \\[ \\begin{aligned} r&amp;=\\frac{L_{xy}}{\\sqrt{L_{xx}L_{yy}}} \\\\ &amp;= \\hat \\beta_1 \\sqrt{\\frac{L_{xx}}{L_{yy}}} \\\\ SSR&amp;=\\sum_{i=1}^n (\\hat y_i -\\bar y)^2 \\\\ &amp;=\\sum_{i=1}^n (\\hat \\beta_0+\\hat \\beta_1x_i-\\bar y)^2 \\\\ &amp;=\\sum_{i=1}^n (\\hat \\beta_1x_i+\\bar y -\\hat \\beta_1 \\bar x - \\bar y)^2 \\\\ &amp;= \\hat \\beta_1^2 \\sum_{i=1}^n (x_i-\\bar x)^2 \\\\ &amp;= \\hat \\beta_1^2 L_{xx} \\end{aligned} \\] 一元场合的线性系数\\(h_{ij}\\) \\[ \\begin{gather} h_{ij}=\\frac{1}{n}+\\frac{(x_i-\\bar x)(x_j -\\bar x)}{L_{xx}}=h_{ji} \\\\ \\hat y_i = \\sum_{j=1}^n h_{ij}y_j \\\\ \\hat y_0=\\sum_{j=1}^n h_{0j}y_j \\\\ e_i = y_i - \\sum_{j=1}^n h_{ij}y_j \\\\ \\sum_{j=1}^n h_{ij}^2 = h_{ii} \\\\ \\sum_{k=1}^n h_{ik}h_{jk} = h_{ij} \\end{gather} \\] 特别的，称\\(h_{ii}\\)为杠杆值，度量了自变量空间中第i个数据偏离数据中心的程度。 特别的，在证明最小二乘估计的性质时基本上都要将这些估计量转化为y的线性表达，无论一元还是多元。 方差分析表 列：方差来源|自由度|平方和|均方|F值|p值 一元线性回归中F统计量与t统计量的关系 \\[ t^2=\\begin{pmatrix}\\frac{\\hat \\beta_1}{\\sqrt{\\hat \\sigma^2/L_{xx}}}\\end{pmatrix}^2=\\frac{\\hat \\beta_1^2L_{xx}}{SSE/(n-2))}=\\frac{SSR}{SSE/(n-2)}=F \\] 其中 \\[ \\begin{aligned} SSR&amp;=\\sum_{i=1}^n (\\hat y_i - \\bar y)^2 \\\\ &amp;= \\sum_{i=1}^n (\\hat \\beta_0 + \\hat \\beta_1x_i-\\bar y)^2 \\\\ &amp;= \\sum_{i=1}^n (\\bar y - \\hat \\beta_1 \\bar x + \\hat \\beta_1x_i-\\bar y)^2 \\\\ &amp;= \\sum_{i=1}^n \\hat \\beta_1^2(x_i-\\bar x)^2 \\\\ &amp;= \\hat \\beta_1^2 L_{xx} \\end{aligned} \\] 一元线性回归中\\(R^2\\)与样本相关系数\\(r\\)的关系 \\[ R^2=\\frac{SSR}{SST}=\\frac{\\hat \\beta_1^2 L_{xx}}{L_{yy}}=\\frac{L_{xy}^2}{L_{xx}L_{yy}}=r^2 \\] 这也可以视作\\(\\hat \\beta_1\\)与样本相关系数的关系 帽子矩阵或投影矩阵 \\[ H=X(X&#39;X)^{-1}X&#39; \\] 矩阵\\(H\\)为对称幂等矩阵，即\\(H&#39;=H, \\; H^2=H\\)。\\(I-H\\)也是对称幂等矩阵。对称幂等矩阵的秩和迹相等。 帽子矩阵的元素就是前面提到的线性系数\\(h_{ij}\\)。 中心化矩阵 \\[ I-\\frac{1}{n}1_n1_n&#39; \\] 多元场合的平方和分解式 \\[ \\begin{gather} SST=\\sum_{i=1}^n(y_i-\\bar y)^2=\\sum_{i=1}^n [(1-\\frac{1}{n})y_i -\\frac{1}{n}\\sum_{j \\neq i}y_j]^2=Y&#39;(I-\\frac{1}{n}1_n1_n&#39;)Y \\\\ SSE=\\sum_{i=1}^n(y_i-\\hat y_i)^2=Y&#39;(I-H)Y \\\\ SSR=SST-SSE=Y&#39;(H-\\frac{1}{n}1_n1_n&#39;)Y \\end{gather} \\] 偏F检验统计量 \\[ F_j = \\frac{(SSE_{(-j)}-SSE)/1}{SSE/(n-p-1)} \\] t检验统计量与偏F统计量的关系 \\[ t_j^2=F_j \\] 以\\(SSE_{(-j)}\\)为中介，为\\(t_j\\)检验统计量与样本偏决定系数之间建立了联系。注意\\(SSE\\)可通过标准误求得 调整的\\(R^2\\) \\[ R_{adj}^2 = 1-\\frac{SSE/(n-p-1)}{SST/(n-1)}=1-\\frac{n-1}{n-p-1}(1-R^2) \\] 样本决定系数与\\(Cor(\\hat Y, Y)\\) \\[ \\begin{aligned} Cor(\\hat Y, Y)&amp;=\\frac{(\\hat Y - 1_n\\bar y)&#39;(Y-1_n\\bar y)}{\\sqrt{SSR \\times SST}} \\\\ &amp;= \\frac{(\\hat Y - 1_n\\bar y)&#39;(\\hat Y + e -1_n\\bar y)}{\\sqrt{SSR \\times SST}} \\\\ &amp;= \\frac{(\\hat Y - 1_n\\bar y)&#39;(\\hat Y-1_n\\bar y)+(\\hat Y - 1_n\\bar y)&#39;e}{\\sqrt{SSR \\times SST}} \\\\ &amp;= \\frac{SSR+0}{\\sqrt{SSR \\times SST}} \\\\ &amp;= \\sqrt{\\frac{SSR}{SST}} \\\\ &amp;= \\sqrt{R^2} \\\\ Cor(\\hat Y, Y)&amp;=\\frac{\\sum_{i=1}^n(\\hat y_i-\\bar y)(y_i-\\bar y)}{\\sqrt{\\sum_{i=1}^n(\\hat y_i-\\bar y)^2\\sum_{i=1}^n(y_i-\\bar y)^2}}=\\frac{L_{\\hat yy}}{\\sqrt{L_{\\hat y \\hat y}L_{yy}}} \\end{aligned} \\] 样本复相关系数 \\[ R=\\sqrt{R^2}=\\sqrt{\\frac{SSR}{SST}} \\] 原始多元线性回归中的\\(\\beta_i\\)与经过\\(X_0\\)调整过后的\\(Y\\)对经过\\(X_0\\)调整过后的\\(X_i\\)回归得到的回归系数是一致的 \\[ (I-H_0)Y=(I-H_0)X_i\\beta_i+(I-H_0)\\varepsilon \\] 相关系数的显著性检验 考虑一元线性回归中的t检验和F检验，根据\\(r^2=R^2=\\frac{SSR}{SST}\\)的关系式让t统计量和F统计量转化成对应的形式即可。 样本偏相关系数、样本偏决定系数 自变量间的样本偏相关系数 \\[ r_{12;3,...p}=\\frac{-\\Delta_{12}}{\\sqrt{\\Delta_{11}\\Delta_{22}}} \\] 因变量与自变量的样本偏相关系数 \\[ r_{y1;2,...p}=\\frac{Cov(e_{x_1|x_{(-1)}},e_{y|x_{(-1)}})}{\\sqrt{Var(e_{x_1|x_{(-1)}})Var(e_{y|x_{(-1)}})}}=Cor(e_{x_1|x_{(-1)}},e_{y|x_{(-1)}})=\\sqrt{\\frac{SSE_{(-1)}-SSE}{SSE_{(-1)}}} \\] 因变量与自变量的样本偏决定系数 \\[ r_{y1;2,...p}^2 = \\frac{SSE_{(-1)}-SSE}{SSE_{(-1)}} \\] 偏F统计量与样本偏决定系数 \\[ \\begin{aligned} F_1&amp;=\\frac{(n-p-1)r_{y1;2,...p}^2}{1-r_{y1;2,...p}^2} \\\\ r_{y1;2,...p}^2 &amp;= \\frac{F_1}{F_1+n-p-1} \\end{aligned} \\] "],["reg_12.html", "7.12 回归诊断", " 7.12 回归诊断 线性回归模型的估计、检验等操作依赖于假定。因此有必要去验证假定。 除此之外，还需对数据进行检验，看看是否存在异常点或强影响点。 7.12.1 残差分析 残差定义为\\(e_i=y_i-\\hat y_i\\)，反映了拟合效果的好坏，是随机扰动项的“观察值”，因此可根据残差的性状来判断随机扰动项假设的合理性。 7.12.1.1 不同形式的残差 普通残差 普通残差定义为\\(e_i=y_i-\\hat y_i\\)，具有\\(E(e_i)=0, \\; Var(e_i)=(1-h_{ii})\\sigma^2, \\; \\rho(e_i,e_j)=\\frac{-h_{ij}}{\\sqrt{(1-h_{ii})(1-h_{jj})}}\\)的性质。 注意\\(Var(e_i)\\)中包含着\\(h_{ii}\\)，\\(h_{ii}\\)为杠杆值，是帽子矩阵\\(H\\)的第i个对角线元素，反映了自变量空间中第i个数据偏离数据中心的程度。特别的，\\(h_{ii}\\)越大，\\(Var(e_i)\\)越小，这表明当某个数据点距离数据中心较远时，会有把拟合直线拖向自己的倾向，因而其残差也可能会较小，称这样的数据点为高杠杆点。 已知\\(tr(H)=\\sum_{i=1}^n h_{ii}=p+1\\)，一个判断高杠杆点的准则是将杠杆值超过两倍杠杆值平均值的数据点认为是高杠杆点。 学生化残差 定义学生化残差为 \\[ r_i=\\frac{e_i}{\\sqrt{\\widehat{Var}(e_i)}}=\\frac{e_i}{\\sqrt{(1-h_{ii})\\hat \\sigma^2}} \\tag{7.105} \\] \\(r_i\\)的性质有\\(E(r_i)=0, \\; Var(r_i)=1, \\; \\rho(r_i,r_j)=\\frac{-h_{ij}}{\\sqrt{(1-h_{ii})(1-h_{jj})}}\\)。 在实际应用中可近似认为\\(r_i\\)相互独立且服从标准正态分布。 学生化残差相较于普通残差解决了方差不等的问题，但仍会受到异常值的影响，会使\\(\\hat \\sigma^2\\)偏大，继而让\\(r_i\\)偏小，因此不太适合根据\\(|r_i|&gt;3\\)的准则来判断异常值。 删除残差 在计算残差\\(e_i\\)时，用不包含第i组观测点的数据\\(Y_{(i)}\\)和\\(X_{(i)}\\)进行回归得到回归方程，根据该回归方程对该组观测点进行预测得到\\(\\hat y_{(i)}\\)，则删除残差为\\(e_{(i)}=y_i-\\hat y_{(i)}\\)。由于删除残差没有用到第i组观测点，因此能够在一定程度上减轻异常点的影响。 特别的，可证 \\[ e_{(i)}=\\frac{e_i}{1-h_{ii}} \\tag{7.106} \\] 参考https://zhuanlan.zhihu.com/p/49276967 删除学生化残差 删除学生化残差定义为 \\[ r_{(i)}=\\frac{e_i}{\\sqrt{1-h_{ii}}\\hat \\sigma_{(i)}}=r_i(\\frac{n-p-2}{n-p-1-r_i^2})^{\\frac{1}{2}} \\tag{7.107} \\] 一般根据\\(|r_{(i)}|&gt;3\\)来判断异常值点。 7.12.1.2 残差图 可令残差（可取学生化残差）为纵坐标，以任何其他有关量为横坐标绘制散点图，常见的横坐标有因变量的拟合值、自变量、时间等。 根据残差图能够看出残差的分布形态，进而可以粗略地判断其是否满足相关性质。 7.12.1.3 正态性检验 对残差（可取学生化残差）进行正态性检验。 图方法 QQ图 绘制(理论分位数, 实际分位数)的散点，看散点是否处于45°线上。 PP图 绘制(理论累积概率, 实际累积概率)的散点，看散点是否处于45°线上。 假设检验 Kolmogorov-Smirnov检验 Shapiro-Wiks检验 注意，无论随机扰动项是否服从正态分布，最小二乘估计都是具有BLUE性质。但若不服从正态分布，后续的t检验、F检验和预测等都不能进行了（毕竟都是基于正态分布展开讨论的）。 7.12.2 异常点和强影响点 7.12.2.1 异常点 异常点是从因变量的维度讨论的异常数据。 基于数据删除模型的异常点检验 数据删除模型如下所示 \\[ \\left\\{ \\begin{array}{c} Y_{(i)}=X_{(i)}\\beta_{(i)}+\\varepsilon_{(i)} \\\\ E(\\varepsilon_{(i)})=0\\\\ Var(\\varepsilon_{(i)})=\\sigma^2 I_{n-1} \\end{array} \\right. \\tag{7.108} \\] 该方法即根据删除残差和删除学生化残差来判断是否为异常点。一般根据\\(|r_{(i)}|&gt;3\\)所对应的数据点判定为异常点。 数据删除模型又是新的模型形式呀 基于均值漂移模型的异常点检验 均值漂移模型如下所示 \\[ \\left\\{ \\begin{array}{c} Y=X\\beta+\\gamma d_i+\\varepsilon \\\\ E(\\varepsilon)=0\\\\ Var(\\varepsilon)=\\sigma^2 I_{n} \\end{array} \\right. \\tag{7.109} \\] 其中\\(d_i\\)表示第i个分量为1而其他分量均为0的n维列向量。该模型表示，如果第i个观测点明显偏高或者偏低，那么\\(d_i\\)的系数\\(\\gamma\\)应该是显著异于0的，而\\(\\gamma d_i\\)会影响到第i个观测点的截距项，因此称“均值漂移模型”。 在识别异常点的过程中，注意有掩盖效应和淹没效应。 掩盖效应：假定的异常点个数小于实际个数，有可能一个都找不到。 淹没效应：假定的异常点个数大于实际个数，有可能将正常点误判为异常点。 7.12.2.2 强影响点 异常点是从因变量的维度讨论的异常数据，高杠杆点是从自变量的角度讨论的异常数据。而综合二者后，称能够对统计推断造成较大影响的点为强影响点。 杠杆值\\(h_{ii}\\)大于两倍杠杆值均值\\(2\\frac{p+1}{n}\\)即可视为高杠杆点 识别方法： Cook距离 定义Cook距离 \\[ D_i=\\frac{(\\hat \\beta -\\hat \\beta_{(i)})&#39;X&#39;X(\\hat \\beta -\\hat \\beta_{(i)})}{(p+1)\\hat \\sigma^2} = \\frac{1}{p+1}(\\frac{h_{ii}}{1-h_{ii}})r_i^2 \\tag{7.110} \\] Cook距离度量了删除第i个数据点前后对回归系数估计值的变化情况。 一个粗略的判断准则为：当\\(D_i &lt; 0.5\\)，则认为不是强影响点，当\\(D_i&gt;1\\)，则认为是强影响点。 Welsch-Kuh统计量(DFFITS准则) Welsch-Kuh统计量定义为 \\[ WK_i=\\frac{\\hat y_i-\\hat y_{(i)}}{\\sqrt{\\hat \\sigma^2_{(i)}h_{ii}}}=\\sqrt{(\\frac{h_{ii}}{1-h_{ii}})r_{(i)}^2} \\tag{7.111} \\] DFFITS准则度量了删除第i个数据点前后该点处拟合值的变化情况。 判断准则为若\\(|WK_i|&gt;2\\sqrt{\\frac{p+1}{n-p-1}}\\)则视为强影响点。 Hadi统计量 Hadi统计量定义为 \\[ H_i=\\frac{h_{ii}}{1-h_{ii}}+\\frac{p+1}{1-h_{ii}}\\cdot \\frac{d_i^2}{1-d_i^2} \\tag{7.112} \\] 其中\\(d_i=\\frac{e_i}{SSE}\\)称为正规化残差。 称以\\(\\frac{p+1}{1-h_{ii}}\\cdot \\frac{d_i^2}{1-d_i^2}\\)为横坐标，以\\(\\frac{h_{ii}}{1-h_{ii}}\\)为纵坐标的散点图为“位势-残差图”。 7.12.3 异方差 若\\(Var(\\varepsilon_i)=\\sigma^2_i\\)，即不同扰动项有不同的方差，则称之为“异方差”问题。 7.12.3.1 原因 遗漏重要变量 重要变量对因变量的影响被归结到随机扰动项中，而这些影响具有差异性，从而导致异方差。 模型设定误差 包括模型形式和变量选择，例如本应包含自变量的二次项但未包含，也会导致异方差问题。 数据的测量误差 在截面数据中个体间的差异较大 存在异常点 7.12.3.2 后果 最小二乘估计仍是无偏的，但不是最小方差线性无偏估计 无偏性没用到随机扰动项的同方差假定，因此仍具有无偏性。但求估计量的方差时需要用到同方差假定，因而不具有有效性。 最小二乘估计的方差估计量是有偏的 既然方差估计量是有偏的，那么凡是用到\\(\\hat \\sigma^2\\)的地方（显著性检验、预测）都会失效。 例如负的偏差会低估参数估计量的真实方差，这会导致对应的t统计量偏大，从而错误地拒绝了原假设。正的偏差会高估参数估计量的真实方差，会产生相反的结果。 7.12.3.3 识别 残差图 根据残差图观察残差的分布形态。 Spearman等级相关系数法 求得普通最小二乘下的残差，根据\\(x_i\\)与\\(|e_i|\\)的等级（秩）差来构造等级相关系数，对等级相关系数进行显著性检验，若拒绝原假设则说明自变量和\\(|e_i|\\)之间存在系统关系，也就说明存在异方差。 Goldfeld-Quandt检验 检验是否存在递增或递减的异方差情形。 Breusch-Pagan检验 \\(e_i^2\\)对所有自变量进行回归，看看残差平方是否和某个自变量有关系。 White检验 \\(e_i^2\\)对所有自变量、自变量平方及变量间的交互项进行回归，看看残差平方是否和某一项有关系。 7.12.3.4 补救 加权最小二乘法 加权最小二乘法通过为数据加权，来消除异方差性。对方差较大的观测赋予较小的权重，以牺牲大方差项的拟合效果为代价，改善小方差项的拟合效果。这个方法关键是要确定合适的权重，实际中可尝试采用残差平方的倒数最为权重。 采用异方差稳健标准误 既然异方差问题会影响\\(\\hat \\sigma^2\\)的估计，那么就直接采用更为稳健的标准误替代\\(\\hat \\sigma^2\\)。 Box-Cox变换 对因变量采取如下变换（因变量为正） \\[ y^{(\\lambda)}=\\begin{cases} \\frac{y^\\lambda-1}{\\lambda}, &amp;\\lambda \\neq 0 \\\\ \\ln y , &amp;\\lambda=0 \\end{cases} \\tag{7.113} \\] 可根据极大似然估计法确定\\(\\lambda\\)。Box-Cox变换能够在一定程度上改善数据的非正态性、异方差性、自相关性。但除了对数变换（表示百分比变动）外其余变换都缺乏解释性。 7.12.4 自相关 若\\(Cov(\\varepsilon_i, \\varepsilon_j) \\neq 0\\)，则称之为“自相关”问题。 7.12.4.1 原因 遗漏重要变量 重要变量对因变量的影响被归结到随机扰动项中，而这些影响是前后相关联的，从而导致自相关。 模型设定误差 经济变量的滞后性会给序列带来自相关性 随机误差项本身的自相关 如地震不仅影响当期，其造成的影响还会持续一段时间。 因对数据加工整理而导致扰动项之间产生自相关性 如把月度数据合并为季度数据、对缺失值进行插值。 7.12.4.2 后果 最小二乘估计仍是无偏的，但不是最小方差线性无偏估计 无偏性没用到随机扰动项的无自相关假定，因此仍具有无偏性。但求估计量的方差时需要用到无自相关假定，因而不具有有效性。 最小二乘估计的方差被低估 同样会造成显著性检验、预测等操作失效。 7.12.4.3 识别 图示法 绘制\\(e_t\\)与\\(e_{t-1}\\)、\\(t\\)的散点图，看看前后是否有某种特定的趋势。 游程检验 对残差的符号变化情况进行游程检验，看看是否是随机变化的。 Durbin-Watson检验 DW检验只能检验一阶自相关情形，且有一些前提条件（如模型中必须包含截距项、解释变量中不包含Y的滞后项等）。 Breusch-Godfrey检验 让残差对所有自变量及残差的滞后项跑个回归，看看残差滞后项的回归系数是否显著。 纯随机性检验 利用时间序列中的Q统计量进行纯随机性检验。 7.12.4.4 补救 广义差分法 采用异方差自相关一致标准误(HAC) 直接用HAC替换\\(\\hat \\sigma^2\\)。 Box-Cox变换 7.12.5 多重共线性 若设计矩阵\\(X\\)的各个列向量之间是线性相关的，则称之为“完全多重共线性”。若是近似线性相关的，则称之为“多重共线性”。注意，多重共线性是一个程度轻重的问题。 7.12.5.1 原因 经济变量之间的内在联系是产生多重共线性的根本原因 经济变量之间存在共同的变化趋势 模型中存在滞后项 7.12.5.2 后果 若是完全多重共线性，则\\(X&#39;X\\)不可逆，无法得到最小二乘估计 最小二乘估计仍是线性无偏的，但多重共线性会导致各估计量的方差较大 同样会造成显著性检验、预测等操作失效。 回归系数的估计量的符号跟实际不符，估计量的含义变得不明确 7.12.5.3 识别 经验判断 \\(R^2\\)很高，F统计量值很大，但各个回归系数显著的较少 回归系数的符号与预期相反 解释变量之间两两高度相关 若模型中增加或减少一个自变量，回归系数的估计值产生较大的变化 条件数 考虑标准化后的矩阵\\(X&#39;X\\)，设其特征根为\\(\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_p \\geq0\\)，则定义条件数为 \\[ CI_i=\\sqrt{\\frac{\\lambda_1}{\\lambda_i}} \\tag{7.114} \\] 当条件数大于等于10时就有相对较为严重的多重共线性。 也可以不取根号，相对应的阈值也要改变 方差膨胀因子 定义方差膨胀因子为 \\[ VIF_i = \\frac{1}{1-R_i^2} \\tag{7.115} \\] 其中\\(R_i^2\\)为自变量\\(x_i\\)对其余自变量回归得到的样本决定系数。若\\(VIF_i \\geq 10\\)，则认为\\(x_i\\)与其他自变量之间存在较强的多重共线性问题。 7.12.5.4 补救 变量筛选与处理 可以剔除一些不重要的变量，或者对变量进行变形，如化总体指标为人均指标。 采取适当的参数估计方法 如岭回归、主成分回归、偏最小二乘回归。 "],["reg_13.html", "7.13 变量选择与正则化", " 7.13 变量选择与正则化 7.13.1 冗余与遗漏 对于某一实际问题涉及到的因变量Y，我们搜寻了m个可能与其相关的自变量。称包含这m个自变量的回归模型为全模型。若只从这m个自变量中选取p个自变量进行回归，则称对应的模型为选模型。 若真实模型是选模型，而用了全模型进行回归，则在模型中引入了不必要的变量，即变量冗余。 若真实模型是全模型，而用了选模型进行回归，则在模型中遗漏了关键变量，即变量遗漏。 7.13.2 变量选择的传统方法 7.13.2.1 自变量选择准则 可根据如下准则进行模型选择。 调整的\\(R^2\\) \\(C_p\\)准则 \\[ C_p=\\frac{1}{n}(SSE_p+2p\\frac{SSE_m}{n-m-1}) \\tag{7.116} \\] 其中\\(SSE_m\\)表示全模型的残差平方和，\\(SSE_p\\)表示选模型的残差平方和。 这只是\\(C_p\\)准则的一种定义，还有另一种定义 AIC \\[ AIC=-2\\ln L(\\hat \\theta;X)+2p \\tag{7.117} \\] 其中\\(L(\\cdot)\\)表示模型的似然函数，\\(\\hat \\theta\\)表示参数\\(\\theta\\)的极大似然估计（在多元线性回归模型中就是\\(\\hat \\beta\\)和\\(\\hat \\sigma^2\\)），\\(X\\)表示样本。 BIC \\[ BIC=-2\\ln L(\\hat \\theta;X)+p \\ln n \\tag{7.118} \\] BIC准则相较于AIC准则增强了对变量个数的惩罚，并新增了对样本数的惩罚。 7.13.2.2 变量选择方法 最优子集法 对自变量的所有组合（共\\(2^m-1\\)种组合）分别拟合回归方程，根据自变量选择准则从中挑选最优模型。 费时 向前回归法 考虑偏F统计量，模型的变量从少到多，每次将偏F统计量最大的且显著的那个变量纳入到模型中，直到没有可引入的变量为止。 当然也可选择其他自变量选择准则 向后回归法 考虑偏F统计量，模型的变量从多到少，每次将偏F统计量最小的且不显著的那个变量从模型中剔除，直到没有剔除的变量为止。 向前向后法 向前法或者向后法都是“只进不出”或者“只出不进”，没有考虑变量间的联合效应。而向前向后法综合了这两种方法，每引入一个自变量时对所有已纳入到模型中的自变量进行逐个检验，考察是否要剔除变量，直至既无显著的自变量引入模型，也无不显著的自变量从回归模型中剔除为止。 7.13.3 变量选择的正则化方法 在模型估计时纳入正则项（惩罚项），不同的惩罚项有不同功能与作用。 详见变量选择与惩罚函数。 "],["ms.html", "8 应用多元统计 ", " 8 应用多元统计 "],["ms_1.html", "8.1 矩阵运算", " 8.1 矩阵运算 8.1.1 Kronecker积 设\\(A=(a_{ij})_{m \\times n}, \\, B=(b_{ij})_{p \\times q}\\)，则称矩阵\\(C=(a_{ij}B)_{mp \\times nq}\\)为矩阵A和矩阵B的Kronecker积，记为\\(C=A\\otimes B\\)。 性质： \\((A_1 \\otimes B_1)(A_2 \\otimes B_2)=(A_1A_2)\\otimes(B_1B_2)\\) 证： 不妨记\\(A_1=(a_{ij}^{(1)})=\\begin{pmatrix} a_{1 \\cdot }^{(1)} \\\\ a_{2 \\cdot}^{(1)} \\\\ \\vdots \\\\ a_{m \\cdot}^{(1)} \\end{pmatrix},\\, A_2=(a_{ij}^{(2)})=\\begin{pmatrix} a_{\\cdot 1}^{(2)} &amp; a_{\\cdot 2}^{(2)} &amp; \\cdots &amp; a_{\\cdot n}^{(2)} \\end{pmatrix}\\)，则 \\[ \\begin{aligned} (A_1 \\otimes B_1)(A_2 \\otimes B_2)&amp;=(a_{ij}^{(1)}B_1)(a_{ij}^{(2)}B_2) \\\\ &amp;=(a_{i \\cdot}^{(1)}a_{\\cdot j}^{(2)}B_1B_2) \\\\ &amp;= (a_{i \\cdot}^{(1)}a_{\\cdot j}^{(2)})\\otimes (B_1B_2) \\\\ &amp;=(A_1A_2)\\otimes (B_1B_2) \\end{aligned} \\tag{8.1} \\] 这里进行了简写，注意只要矩阵运算中出现小写字母，代表该矩阵的元素 \\((A\\otimes B)&#39;=A&#39; \\otimes B&#39;\\) 证： \\[ \\begin{aligned} (A\\otimes B)&#39;&amp;=(a_{ij}B)&#39; \\\\ &amp;= (a_{ji}B&#39;) \\\\ &amp;= A&#39; \\otimes B&#39; \\end{aligned} \\tag{8.2} \\] \\((A\\otimes B)^{-1}=A^{-1} \\otimes B^{-1}\\) 证： \\[ \\begin{aligned} (A \\otimes B) \\cdot (A^{-1} \\otimes B^{-1})&amp;=(AA^{-1}) \\otimes (BB^{-1})\\\\ &amp;=I \\otimes I\\\\ &amp;=I\\\\ \\Rightarrow (A \\otimes B)^{-1} &amp;=A^{-1} \\otimes B^{-1} \\end{aligned} \\tag{8.3} \\] 若A，B均为方阵，则\\(tr(A\\otimes B)=tr(A) \\cdot tr(B)\\) 证： \\[ \\begin{aligned} tr(A\\otimes B)&amp;=tr(a_{ij}B) \\\\ &amp;= \\sum_{i}\\sum_{j}a_{ii}b_{jj} \\\\ &amp;= \\sum_{i}a_{ii}\\sum_{j}b_{jj} \\\\ &amp;= tr(A) \\cdot tr(B) \\end{aligned} \\tag{8.4} \\] \\(||A \\otimes B||=||A|| \\cdot ||B||\\) 证： \\[ \\begin{aligned} ||A \\otimes B||=... \\end{aligned} \\tag{8.4} \\] \\(rank(A \\otimes B)=rank(A) \\cdot rank(B)\\) 证： 不妨记矩阵\\(A_{m\\times n}\\)前p行为极大线性无关组，矩阵\\(B_{h\\times k}\\)前q行为极大线性无关组，对\\(A\\otimes B\\)进行行化简，如下所示： \\[ \\begin{aligned} A \\otimes B = &amp;\\begin{pmatrix} a_{11}B &amp; a_{12}B &amp; \\cdots &amp; a_{1n}B \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ a_{m1}B &amp; a_{m2}B &amp; \\cdots &amp; a_{mn}B \\end{pmatrix}\\\\ \\overset {a} \\Rightarrow &amp;\\begin{pmatrix} a_{11}B &amp; a_{12}B &amp; \\cdots &amp; a_{1n}B \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ a_{p1}B &amp; a_{p2}B &amp; \\cdots &amp; a_{pn}B \\\\ 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\end{pmatrix}\\\\ \\overset {b} \\Rightarrow &amp;\\begin{pmatrix} a_{11}b_{11} &amp; \\cdots &amp; a_{11}b_{1k} &amp; \\cdots &amp; a_{1n}b_{11} &amp; \\cdots &amp; a_{1n}b_{1k}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ a_{11}b_{q1} &amp; \\cdots &amp; a_{11}b_{qk} &amp; \\cdots &amp; a_{1n}b_{q1} &amp; \\cdots &amp; a_{1n}b_{qk} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ a_{p1}b_{11} &amp; \\cdots &amp; a_{p1}b_{1k} &amp; \\cdots &amp; a_{pn}b_{11} &amp; \\cdots &amp; a_{pn}b_{1k}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ a_{p1}b_{q1} &amp; \\cdots &amp; a_{p1}b_{qk} &amp; \\cdots &amp; a_{pn}b_{q1} &amp; \\cdots &amp; a_{pn}b_{qk} \\\\ 0 &amp; \\cdots &amp; 0 &amp; \\cdots &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; 0 &amp; \\cdots &amp; 0 &amp; \\cdots &amp; 0 \\end{pmatrix} \\end{aligned} \\tag{8.5} \\] 8.1.2 拉直 设\\(A=(a_1,...,a_n)\\)是一个\\(m \\times n\\)矩阵，其中\\(a_i=(a_{1i},...,a_{mi})&#39;\\)。将矩阵A按列向量\\(a_1,...,a_n\\)依次排成一个\\(mn \\times 1\\)的向量，即\\(vec(A)=\\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{pmatrix}\\)，称\\(vec(A)\\)为矩阵A的按列拉直运算，同理，记\\(rvec(A)\\)为矩阵A的按行拉直运算。显然，有\\(rvec(A)=(vec(A&#39;))&#39;\\)。 \\(tr(AB)=(vec(A&#39;))&#39;vec(B)\\) 证： \\[ \\begin{aligned} tr(AB)&amp;= \\sum_i a_{i \\cdot}b_{\\cdot i} \\\\ &amp;= (a_{i \\cdot})(b_{\\cdot i}) \\\\ &amp;= rvec(A)vec(B) \\\\ &amp;= (vec(A&#39;))&#39;vec(B) \\end{aligned} \\tag{8.6} \\] 第二行表示元素为A的行向量的行向量与元素为B的列向量的列向量的内积 \\(vec(ABC)=(C&#39; \\otimes A)vec(B)\\) 证： 令\\(C_{m \\times n}=(c_{ij})=(c_1,...,c_n), \\, B=(b_1,...,b_m)\\)，则 \\[ \\begin{aligned} (C&#39; \\otimes A)vec(B)&amp;= \\begin{pmatrix} c_{11}A &amp; c_{21}A &amp; \\cdots &amp; c_{m1}A \\\\ c_{12}A &amp; c_{22}A &amp; \\cdots &amp; c_{m2}A \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ c_{1n}A &amp; c_{2n}A &amp; \\cdots &amp; c_{mn}A \\end{pmatrix} \\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{pmatrix} \\\\ &amp;= \\begin{pmatrix} A\\sum (c_{i1} b_i) \\\\ A\\sum (c_{i2} b_i) \\\\ \\vdots \\\\ A\\sum (c_{in} b_i) \\end{pmatrix} \\\\ &amp;= \\begin{pmatrix} ABc_1 \\\\ ABc_2 \\\\ \\vdots \\\\ ABc_n \\end{pmatrix} \\\\ &amp;= vec(ABC) \\end{aligned} \\tag{8.6} \\] \\(tr(ABC)=(vec(A&#39;))&#39;(I \\otimes B)vec(C)\\) 证： 令\\(A=\\begin{pmatrix}a_{1 \\cdot} \\\\ a_{2 \\cdot} \\\\ \\vdots \\\\ a_{m \\cdot}\\end{pmatrix}\\) \\[ \\begin{aligned} tr(ABC)&amp;=tr(AB \\cdot C) \\\\ &amp;= [vec((AB)&#39;)]&#39;vec(C) \\\\ &amp;= rvec(AB)vec(C) \\\\ &amp;= \\begin{pmatrix}a_{1 \\cdot}B &amp; a_{2 \\cdot}B &amp; \\cdots &amp; a_{m \\cdot}B \\end{pmatrix} vec(C) \\\\ &amp;= \\begin{pmatrix}a_{1 \\cdot} &amp; a_{2 \\cdot} &amp; \\cdots &amp; a_{m \\cdot} \\end{pmatrix} diag\\{B, B , \\cdots ,B\\}vec(C) \\\\ &amp;=rvec(A)(I \\otimes B)vec(C) \\\\ &amp;= (vec(A&#39;))&#39;(I \\otimes B)vec(C) \\end{aligned} \\tag{8.7} \\] 8.1.3 减号逆与加号逆 8.1.3.1 减号逆 对于一个\\(m \\times n\\)的矩阵A，一切满足方程组\\(AXA=A\\)的矩阵X称为矩阵A的广义逆，记为\\(A^{-}\\)，也称减号逆。 减号逆不唯一 证： 令A是一个\\(m \\times n\\)矩阵，rank(A)=r, 若\\(A=P \\begin{pmatrix} I_r &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix} Q\\)，P和Q分别为m阶、n阶矩阵，则 \\[ A^{-} = Q^{-1} \\begin{pmatrix} I_r &amp; B \\\\ C &amp; D \\end{pmatrix} P^{-1} \\tag{8.8} \\] 其中B，C，D为适当阶数的任意矩阵。 对任意矩阵A，有\\(A&#39;A(A&#39;A)^{-}A&#39;=A&#39;,\\;A(A&#39;A)^{-}A&#39;A=A\\) 证： 令\\(A&#39;A(A&#39;A)^{-}A&#39;=X\\)，\\(A(A&#39;A)^{-}A&#39;A=Y\\)，则可得： \\[ \\begin{aligned} A&#39;A(A&#39;A)^{-}A&#39;&amp;= X \\\\ A&#39;A(A&#39;A)^{-}A&#39;A &amp;= XA \\\\ A&#39;A &amp;= XA \\\\ (A&#39;-X)A &amp;= 0 \\\\ A&#39;(A-X&#39;) &amp;= 0 \\\\ \\end{aligned} \\tag{8.9} \\] 故\\(X=A&#39;\\)必为该方程的解，即\\(A&#39;A(A&#39;A)^{-}A&#39;=A&#39;\\)。 \\[ \\begin{aligned} A(A&#39;A)^{-}A&#39;A &amp;= Y \\\\ A&#39;A(A&#39;A)^{-}A&#39;A &amp;= A&#39;Y \\\\ A&#39;A &amp;= A&#39;Y \\\\ A&#39;(A-Y) &amp;= 0 \\end{aligned} \\tag{8.10} \\] 故\\(Y=A\\)必为方程的解，即\\(A(A&#39;A)^{-}A&#39;A=Y\\)。 设相容线性方程组\\(Ax=b\\)，则 对任一广义逆\\(A^{-}, \\, x=A^{-}b\\)必为解 证： \\[ \\begin{aligned} Ax&amp;=b \\\\ AA^{-}Ax&amp;=AA^{-}b \\\\ Ax&amp;=AA^{-}b \\\\ A(x-A^{-}b)&amp;=0 \\\\ x&amp;=A^{-}b \\end{aligned} \\tag{8.11} \\] 齐次方程组\\(Ax=0\\)的通解为\\(x=(I-A^-A)z\\)，z为任意向量，\\(A^-\\)为任一固定的广义逆 证： \\[ \\begin{aligned} A &amp;= AA^{-}A \\\\ Az &amp;= AA^{-}Az \\\\ A(Iz-A^{-}Az) &amp;= 0 \\\\ A(I-A^{-}A)z &amp;= 0 \\end{aligned} \\tag{8.12} \\] \\(Ax=b\\)的通解为\\(x=A^-b+(I-A^-A)z\\)，z为任意向量，\\(A^-\\)为任一固定的广义逆 证： 由上述可得，\\(Ax=b\\)的通解为\\(Ax=b\\)的特解加上\\(Ax=0\\)的通解，即\\(x=A^{-}b+(I-A^{-}A)z\\) 8.1.3.2 加号逆 设A为任一矩阵，若矩阵X满足\\((1)AXA=A;\\;(2)XAX=X; \\; (3)(AX)&#39;=AX; \\; (4)(XA)&#39;=XA\\)，则称X为A的Moore-Penrose广义逆，记为\\(A^+\\)，也称加号逆或伪逆。 每个矩阵均存在加号逆且唯一 存在性，证： 设A是一个\\(m \\times n\\)矩阵，\\(rank(A)=r\\)，若A的奇异值分解\\(A=U\\begin{pmatrix} \\Lambda_r &amp; 0 \\\\0 &amp; 0 \\end{pmatrix}V&#39;=UDV&#39;\\)，U和V分别为m阶、n阶正交矩阵。令\\(X=V\\begin{pmatrix} \\Lambda_r^{-1} &amp; 0 \\\\0 &amp; 0 \\end{pmatrix}U&#39;=V\\tilde DU&#39;\\)，则 \\[ \\begin{aligned} &amp;AXA=(UDV&#39;)(V\\tilde D U&#39;)(UDV&#39;)=UDV&#39;=A \\\\ &amp;XAX= (V\\tilde D U&#39;)(UDV&#39;)(V\\tilde D U&#39;)=V\\tilde D U&#39;=X \\\\ &amp;(AX)&#39;=(UDV&#39;V\\tilde D U&#39;)&#39;=(UU&#39;)&#39;=UU&#39;=UDV&#39;V\\tilde D U&#39;=AX\\\\ &amp;(XA)&#39;=(V\\tilde D U&#39;UDV&#39;)&#39;=(VV&#39;)&#39;= VV&#39;=VDU&#39;U\\tilde D V&#39;=XA \\end{aligned} \\tag{8.13} \\] 即\\(X=A^+\\)。 唯一性，证： 令X，Y均满足伪逆的四个条件，即X，Y均为A的加号逆 \\[ \\begin{aligned} X&amp;=XAX\\\\ &amp;=X(AX)&#39; \\\\ &amp;=XX&#39;A&#39; \\\\ &amp;=XX&#39;(AYA)&#39; \\\\ &amp;=X(AX)&#39;(AY)&#39; \\\\ &amp;=XAX(AY) \\\\ &amp;=XAY \\\\ &amp;=(XA)&#39;Y \\\\ &amp;=A&#39;X&#39;Y \\\\ &amp;=A&#39;X&#39;(YAY) \\\\ &amp;=A&#39;X&#39;(YA)&#39;Y \\\\ &amp;=A&#39;X&#39;A&#39;Y&#39;Y \\\\ &amp;=(AXA)&#39;Y&#39;Y \\\\ &amp;=A&#39;Y&#39;Y \\\\ &amp;=(YA)&#39;Y \\\\ &amp;=YAY \\\\ &amp;=Y \\end{aligned} \\tag{8.14} \\] \\(A^+=A&#39;(AA&#39;)^+=(A&#39;A)^+A&#39;\\) 证： 令矩阵A的秩为r，则其奇异值分解为\\(A=U\\begin{pmatrix} \\Lambda_r &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix}V&#39;\\)，对应的有\\(AA&#39;=U\\begin{pmatrix} \\Lambda_r^2 &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix}U&#39;, \\; A&#39;A=V\\begin{pmatrix} \\Lambda_r^2 &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix}V&#39;\\)。 \\[ \\begin{aligned} A&#39;(AA&#39;)^+ &amp;= A&#39;U\\begin{pmatrix} \\Lambda_r^{-2} &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix}U&#39; \\\\ &amp;=V\\begin{pmatrix} \\Lambda_r &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix}U&#39;U\\begin{pmatrix} \\Lambda_r^{-2} &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix}U&#39; \\\\ &amp;=V\\begin{pmatrix} \\Lambda_r^{-1} &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix}U&#39; \\\\ &amp;=A^+ \\end{aligned} \\tag{8.15} \\] 同理，\\(A^+=(A&#39;A)^+A&#39;\\)。 8.1.4 分块矩阵 将矩阵\\(A_{n \\times p}\\)分成四块：\\(A=\\begin{pmatrix} A_{11} &amp; A_{12} \\\\ A_{21} &amp; A_{22} \\end{pmatrix}\\)，即为分块矩阵。 若A为方阵，\\(A_{11}\\)也为方阵，则 当\\(|A_{11}| \\neq 0\\)时，\\(|A|=|A_{11}||A_{22 \\cdot 1}|\\)，其中\\(A_{22 \\cdot 1}=A_{22}-A_{21}A_{11}^{-1}A_{12}\\) 证： 令分块矩阵左乘\\(\\begin{pmatrix} I &amp; 0 \\\\ -A_{21}A_{11}^{-1} &amp; I\\end{pmatrix}\\)，右乘\\(\\begin{pmatrix} I &amp; -A_{11}^{-1}A_{12} \\\\ 0 &amp; I\\end{pmatrix}\\)，可得 \\[ \\begin{pmatrix} I &amp; 0 \\\\ -A_{21}A_{11}^{-1} &amp; I\\end{pmatrix}\\begin{pmatrix} A_{11} &amp; A_{12} \\\\ A_{21} &amp; A_{22} \\end{pmatrix}\\begin{pmatrix} I &amp; -A_{11}^{-1}A_{12} \\\\ 0 &amp; I\\end{pmatrix}=\\begin{pmatrix} A_{11} &amp; 0 \\\\ 0 &amp; A_{22 \\cdot 1} \\end{pmatrix} \\tag{8.16} \\] 其中\\(A_{22 \\cdot 1}=A_{22}-A_{21}A_{11}^{-1}A_{12}\\)。等式左右两边取行列式即可。 当\\(|A_{22}| \\neq 0\\)时，\\(|A|=|A_{11 \\cdot 2}||A_{22}|\\)，其中\\(A_{11 \\cdot 2}=A_{11}-A_{12}A_{22}^{-1}A_{21}\\)。 证： 同理。 若A为可逆方阵，\\(A_{11}\\)和\\(A_{22}\\)均为方阵，则 当\\(|A_{11}|\\neq 0\\)时，\\(A^{-1}=\\begin{pmatrix} A_{11}^{-1} + A_{11}^{-1}A_{12}A_{22 \\cdot 1}^{-1}A_{21}A_{11}^{-1} &amp; -A_{11}^{-1}A_{12}A_{22 \\cdot 1}^{-1} \\\\ -A_{22 \\cdot 1}^{-1}A_{21}A_{11}^{-1} &amp; A_{22\\cdot 1}^{-1} \\end{pmatrix}\\)。 证： 由式(8.16)可知， \\[ \\begin{aligned} \\begin{pmatrix} A_{11} &amp; 0 \\\\ 0 &amp; A_{22 \\cdot 1} \\end{pmatrix}^{-1}&amp;=\\begin{pmatrix} I &amp; -A_{11}^{-1}A_{12} \\\\ 0 &amp; I\\end{pmatrix}^{-1}\\begin{pmatrix} A_{11} &amp; A_{12} \\\\ A_{21} &amp; A_{22} \\end{pmatrix}^{-1}\\begin{pmatrix} I &amp; 0 \\\\ -A_{21}A_{11}^{-1} &amp; I\\end{pmatrix}^{-1} \\\\ \\begin{pmatrix} A_{11} &amp; A_{12} \\\\ A_{21} &amp; A_{22} \\end{pmatrix}^{-1}&amp;=\\begin{pmatrix} I &amp; -A_{11}^{-1}A_{12} \\\\ 0 &amp; I\\end{pmatrix}\\begin{pmatrix} A_{11} &amp; 0 \\\\ 0 &amp; A_{22 \\cdot 1} \\end{pmatrix}^{-1}\\begin{pmatrix} I &amp; 0 \\\\ -A_{21}A_{11}^{-1} &amp; I\\end{pmatrix} \\\\ &amp;=\\begin{pmatrix} I &amp; -A_{11}^{-1}A_{12} \\\\ 0 &amp; I\\end{pmatrix}\\begin{pmatrix} A_{11}^{-1} &amp; 0 \\\\ 0 &amp; A_{22 \\cdot 1}^{-1} \\end{pmatrix}\\begin{pmatrix} I &amp; 0 \\\\ -A_{21}A_{11}^{-1} &amp; I\\end{pmatrix} \\\\ &amp;= \\begin{pmatrix} A_{11}^{-1} + A_{11}^{-1}A_{12}A_{22 \\cdot 1}^{-1}A_{21}A_{11}^{-1} &amp; -A_{11}^{-1}A_{12}A_{22 \\cdot 1}^{-1} \\\\ -A_{22 \\cdot 1}^{-1}A_{21}A_{11}^{-1} &amp; A_{22\\cdot 1}^{-1} \\end{pmatrix} \\end{aligned} \\tag{8.17} \\] 当\\(|A_{22}|\\neq 0\\)时，\\(A^{-1}=\\begin{pmatrix} A_{11 \\cdot 2}^{-1} &amp; -A_{11 \\cdot 2}^{-1}A_{12}A_{22}^{-1} \\\\ -A_{22}^{-1}A_{21}A_{11\\cdot 2}^{-1} &amp; A_{22\\cdot 1}^{-1}+ A_{22}^{-1}A_{21}A_{11 \\cdot 2}^{-1}A_{12}A_{22}^{-1} \\end{pmatrix}\\)。 证： 同理。 当\\(|A_{11}|\\neq 0, \\;|A_{22}|\\neq 0\\)时，\\(A^{-1}=\\begin{pmatrix} A_{11 \\cdot 2}^{-1} &amp; -A_{11}^{-1}A_{12}A_{22 \\cdot 1}^{-1} \\\\ -A_{22}^{-1}A_{21}A_{11\\cdot 2}^{-1} &amp; A_{22\\cdot 1}^{-1} \\end{pmatrix}\\)。 证： 同理。 "],["ms_2.html", "8.2 多元正态分布", " 8.2 多元正态分布 8.2.1 多元分布的基本运算性质 \\(E(tr(AX))=tr(E(AX))=tr(AE(X))\\) \\(Cov(AX,BY)=ACov(X,Y)B&#39;, \\, Cov(AX)=ACov(X)A&#39;\\) \\(E(X&#39;AX)=tr(A\\Sigma)+\\mu&#39;A\\mu\\) 证： \\[ \\begin{aligned} E(X&#39;AX)&amp;=E[tr(X&#39;AX)] \\\\ &amp;=E[tr(AXX&#39;)] \\\\ &amp;=tr(AE(XX&#39;)) \\\\ &amp;=tr(A(\\Sigma+\\mu\\mu&#39;)) \\\\ &amp;=tr(A\\Sigma)+tr(A\\mu\\mu&#39;) \\\\ &amp;=tr(A\\Sigma)+tr(\\mu&#39;A\\mu) \\\\ &amp;=tr(A\\Sigma)+\\mu&#39;A\\mu \\end{aligned} \\tag{8.18} \\] 其中\\(Cov(X)=\\Sigma=E(XX&#39;)-\\mu\\mu&#39;\\)。 若\\(X \\sim \\varphi_X(t), \\, Y=AX+A\\)，则\\(\\varphi_Y(t)=\\exp(it&#39;a)\\varphi_X(A&#39;t)\\) 证： \\[ \\begin{aligned} \\varphi_Y(t)&amp;=E(e^{it&#39;Y}) \\\\ &amp;= E(e^{it&#39;(AX+a)}) \\\\ &amp;= e^{it&#39;a}E(e^{it&#39;AX}) \\\\ &amp;= e^{it&#39;a}E(e^{i(A&#39;t)&#39;X}) \\\\ &amp;= e^{it&#39;a}\\varphi_X(A&#39;t) \\end{aligned} \\tag{8.19} \\] 若X，Y相互独立且维数相同，则\\(\\varphi_{X+Y}(t)=\\varphi_X(t)\\varphi_Y(t)\\)。 证： \\[ \\begin{aligned} \\varphi_{X+Y}(t)&amp;=E(e^{it&#39;(X+Y)}) \\\\ &amp;=E(e^{it&#39;X}e^{it&#39;Y}) \\\\ &amp;= E(e^{it&#39;X})E(e^{it&#39;Y}) \\\\ &amp;= \\varphi_X(t)\\varphi_Y(t) \\end{aligned} \\tag{8.20} \\] 一元正态分布\\(N(\\mu,\\sigma^2)\\)的特征函数 \\[ \\begin{aligned} E(e^{itX})&amp;=\\int_{-\\infty}^{\\infty}\\exp(itx)\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp[-\\frac{(x-\\mu)^2}{2\\sigma^2}]dx \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^{\\infty}\\exp[-\\frac{(x-\\mu)^2-2\\sigma^2itx}{2\\sigma^2}]dx \\\\ &amp;=\\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^{\\infty}\\exp[-\\frac{(x-\\sigma^2it-\\mu)^2+\\sigma^4t^2-2\\sigma^2itu}{2\\sigma^2}]dx \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp(\\frac{-\\sigma^2t^2}{2}+itu) \\int_{-\\infty}^{\\infty}\\exp[-\\frac{(x-\\sigma^2it-\\mu)^2}{2\\sigma^2}]dx \\\\ &amp;\\stackrel{y=(\\frac{x-\\sigma^2it-\\mu}{\\sigma})}\\Longrightarrow \\frac{1}{\\sqrt{2\\pi}} \\exp(\\frac{-\\sigma^2t^2}{2}+itu)\\int_{-\\infty}^{\\infty} \\exp[-\\frac{y^2}{2}]dy \\\\ &amp;=\\exp(\\frac{-\\sigma^2t^2}{2}+itu) \\end{aligned} \\tag{8.21} \\] 8.2.2 多元正态分布的定义 概率密度函数 \\[ f(x)=\\frac{1}{(2\\pi)^{\\frac{p}{2}}|\\Sigma|^{\\frac{1}{2}}}\\exp(-\\frac{1}{2}(x-\\mu)&#39;\\Sigma^{-1}(x-\\mu)) \\tag{8.22} \\] 该定义要求\\(\\Sigma &gt; 0\\)，记\\(X\\sim N_p(\\mu, \\Sigma)\\)。 特征函数 \\[ \\varphi_X(t)=\\exp(it&#39;\\mu-\\frac{1}{2}t&#39;\\Sigma t) \\tag{8.23} \\] 该定义要求\\(\\Sigma \\geq 0\\)，记\\(X\\sim N_p(\\mu, \\Sigma)\\)。 线性组合1 设\\(Y_1,...Y_q \\stackrel{iid}\\sim N(0,1)\\)，A时\\(p \\times q\\)常数矩阵，\\(\\mu\\)为\\(p \\times 1\\)常数向量，称q维随机向量\\(Y=(Y_1,...,Y_p)&#39;\\)的线性组合\\(X=AY+\\mu\\)的分布为p维正态分布，记记\\(X\\sim N_p(\\mu, \\Sigma)\\)，其中\\(\\Sigma=AA&#39;\\)。 线性组合2 若p维随机向量\\(X=(X_1,...,X_p)&#39;\\)的任意线性组合均服从一元正态分布，则称X为p维正态分布，记为\\(X\\sim N_p(\\mu, \\Sigma)\\)。 二元正态分布\\(N(\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2,\\rho)\\)的概率密度函数 \\[ f(x,y)=\\frac{1}{2\\pi\\sigma_1\\sigma_2\\sqrt{1-\\rho^2}}\\exp\\{-\\frac{[\\frac{(x-\\mu_1)^2}{\\sigma^2}-2\\rho\\frac{(x-\\mu_1)(y-\\mu_2)}{\\sigma_1\\sigma_2}+\\frac{(y-\\mu_2)^2}{\\sigma^2}]}{2(1-\\rho^2)}\\} \\tag{8.24} \\] 8.2.3 正态分布的条件分布和独立性 8.2.3.1 条件分布 设\\(X \\sim N_p(\\mu, \\Sigma), p \\geq 2\\)，将\\(X,\\,\\mu, \\, \\Sigma\\)进行相同的分块，即\\(X=\\begin{pmatrix} X^{(1)} \\\\ X^{(2)} \\end{pmatrix}, \\mu=\\begin{pmatrix} \\mu^{(1)} \\\\ \\mu^{(2)} \\end{pmatrix}, \\Sigma=\\begin{pmatrix} \\Sigma_{11} &amp; \\Sigma_{12} \\\\ \\Sigma_{21} &amp; \\Sigma_{22} \\end{pmatrix}&gt;0\\)，其中\\(X^{(1)}\\)为q维向量，\\(X^{(2)}\\)为p-q维向量。 给定\\(X^{(2)}=x^{(2)}\\)时，\\((X^{(1)}|X^{(2)}=x^{(2)}) \\sim N_q(\\mu_{1 \\cdot 2},\\Sigma_{11 \\cdot 2})\\)，其中\\(\\mu_{1 \\cdot 2}=\\mu^{(1)}+\\Sigma_{12}\\Sigma_{22}^{-1}(x^{(2)}-\\mu^{(2)}), \\, \\Sigma_{11 \\cdot 2}=\\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\\)。 证1： 由分块矩阵逆的性质可知 \\[ \\Sigma^{-1}=\\begin{pmatrix} \\Sigma_{11 \\cdot 2}^{-1} &amp; -\\Sigma_{11 \\cdot 2}^{-1}\\Sigma_{12}\\Sigma_{22}^{-1} \\\\ -\\Sigma_{22}^{-1}\\Sigma_{21}\\Sigma_{11 \\cdot 2}^{-1} &amp; \\Sigma_{22}^{-1}+\\Sigma_{22}^{-1}\\Sigma_{21}\\Sigma_{11 \\cdot 2}^{-1}\\Sigma_{12}\\Sigma_{22}^{-1} \\end{pmatrix} \\tag{8.25} \\] 由条件密度函数定义可得 \\[ \\begin{aligned} f(x^{(1)}|x^{(2)}) &amp;= \\frac{f(x^{(1)},x^{(2)})}{f_{X^{(2)}}(x^{(2)})} \\\\ &amp;=\\frac{(2\\pi)^{-\\frac{p}{2}}|\\Sigma|^{-\\frac{1}{2}} \\exp(-\\frac{1}{2}(x-\\mu)&#39;\\Sigma^{-1}(x-\\mu))}{(2\\pi)^{-\\frac{p-q}{2}}|\\Sigma_{22}|^{-\\frac{1}{2}} \\exp(-\\frac{1}{2}(x^{(2)}-\\mu^{(2)})&#39;\\Sigma_{22}^{-1}(x^{(2)}-\\mu^{(2)}))} \\end{aligned} \\tag{8.25} \\] 根据\\((x-\\mu)&#39;=\\begin{pmatrix} x^{(1)}-\\mu^{(1)} \\\\ x^{(2)}-\\mu^{(2)} \\end{pmatrix}&#39;\\)，及式(8.25)，可得 \\[ \\begin{aligned} &amp;\\quad (x-\\mu)&#39;\\Sigma^{-1}(x-\\mu)-(x^{(2)}-\\mu^{(2)})&#39;\\Sigma_{22}^{-1}(x^{(2)}-\\mu^{(2)}) \\\\ &amp;=(x^{(1)}-\\mu^{(1)})&#39;\\Sigma_{11 \\cdot 2}^{-1}(x^{(1)}-\\mu^{(1)})-(x^{(2)}-\\mu^{(2)})&#39;(\\Sigma_{22}^{-1}\\Sigma_{21}\\Sigma_{11 \\cdot 2}^{-1})(x^{(1)}-\\mu^{(1)}) \\\\ &amp;-(x^{(1)}-\\mu^{(1)})&#39;\\Sigma_{11 \\cdot 2}^{-1}\\Sigma_{12}\\Sigma_{22}^{-1}(x^{(2)}-\\mu^{(2)}) \\\\ &amp;+ (x^{(2)}-\\mu^{(2)})&#39;(\\Sigma_{22}^{-1}+\\Sigma_{22}^{-1}\\Sigma_{21}\\Sigma_{11 \\cdot 2}^{-1}\\Sigma_{12}\\Sigma_{22}^{-1})(x^{(2)}-\\mu^{(2)}) \\\\ &amp;- (x^{(2)}-\\mu^{(2)})&#39;\\Sigma_{22}^{-1}(x^{(2)}-\\mu^{(2)}) \\\\ &amp;= (x^{(1)}-\\mu^{(1)})&#39;\\Sigma_{11 \\cdot 2}^{-1}(x^{(1)}-\\mu^{(1)}-\\Sigma_{12}\\Sigma_{22}^{-1}(x^{(2)}-\\mu^{(2)})) \\\\ &amp;-(x^{(2)}-\\mu^{(2)})&#39;\\Sigma_{22}^{-1}\\Sigma_{21}\\Sigma_{11 \\cdot 2}^{-1}(x^{(1)}-\\mu^{(1)}-\\Sigma_{12}\\Sigma_{22}^{-1}(x^{(2)}-\\mu^{(2)})) \\\\ &amp;= ((x^{(1)}-\\mu^{(1)})&#39;-(x^{(2)}-\\mu^{(2)})&#39;\\Sigma_{22}^{-1}\\Sigma_{21})\\Sigma_{11\\cdot 2}^{-1}(x^{(1)}-\\mu^{(1)}-\\Sigma_{12}\\Sigma_{22}^{-1}(x^{(2)}-\\mu^{(2)})) \\\\ &amp;\\stackrel{\\mu_{1 \\cdot 2}=\\mu^{(1)}+\\Sigma_{12}\\Sigma_{22}^{-1}(x^{(2)}-\\mu^{(2)})}\\Rightarrow (x^{(1)}-\\mu_{1\\cdot 2})&#39;\\Sigma_{11 \\cdot 2}^{-1}(x^{(1)}-\\mu_{1\\cdot 2}) \\end{aligned} \\tag{8.26} \\] 则 \\[ \\begin{aligned} f(x^{(1)}|x^{(2)})&amp;=(2\\pi)^{-\\frac{q}{2}}|\\Sigma_{11\\cdot 2}|^{-\\frac{1}{2}} \\exp(-\\frac{1}{2}(x^{(1)}-\\mu_{1\\cdot 2})&#39;\\Sigma_{11 \\cdot 2}^{-1}(x^{(1)}-\\mu_{1\\cdot 2})) \\end{aligned} \\tag{8.27} \\] 故\\((X^{(1)}|X^{(2)}=x^{(2)}) \\sim N_q(\\mu_{1 \\cdot 2},\\Sigma_{11 \\cdot 2})\\)。 证2： 令\\(Y^{(1)}=X^{(1)}-\\Sigma_{12}\\Sigma_{22}^{-1}X_2^{(2)}\\)，\\(Y^{(2)}=X^{(2)}\\)，则有 \\[ Y= \\begin{pmatrix} Y^{(1)} \\\\ Y^{(2)} \\end{pmatrix}= \\begin{pmatrix} I &amp; -\\Sigma_{12}\\Sigma_{22}^{-1} \\\\ 0 &amp; I \\end{pmatrix} \\begin{pmatrix} X^{(1)} \\\\ X^{(2)} \\end{pmatrix}= AX \\tag{8.28} \\] 因为\\(X \\sim N_p(\\mu, \\Sigma)\\)，所以\\(Y \\sim N_p(A\\mu, A\\Sigma A&#39;)\\)。 其中 \\[ A\\mu = \\begin{pmatrix} \\mu_1-\\Sigma_{12}\\Sigma_{22}^{-1}\\mu_2 \\\\ \\mu_2 \\end{pmatrix} \\tag{8.29} \\] \\[ \\begin{aligned} A\\Sigma A&#39; &amp;= \\begin{pmatrix} I &amp; -\\Sigma_{12}\\Sigma_{22}^{-1} \\\\ 0 &amp; I \\end{pmatrix} \\begin{pmatrix} \\Sigma_{11} &amp; \\Sigma_{12} \\\\ \\Sigma_{21} &amp; \\Sigma_{22} \\end{pmatrix} \\begin{pmatrix} I &amp; 0 \\\\ -\\Sigma_{22}^{-1}\\Sigma_{12} &amp; I \\end{pmatrix} \\\\ &amp;= \\begin{pmatrix} \\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21} &amp; 0 \\\\ 0 &amp; \\Sigma_{22} \\end{pmatrix} \\\\ &amp;= \\begin{pmatrix} \\Sigma_{11 \\cdot 2}&amp; 0 \\\\ 0 &amp; \\Sigma_{22} \\end{pmatrix} \\end{aligned} \\tag{8.30} \\] 故\\(Y^{(1)}\\)与\\(Y^{(2)}\\)独立。 已知\\(Y^{(1)} \\sim N_q(\\mu_1-\\Sigma_{12}\\Sigma_{22}^{-1}\\mu_2,\\Sigma_{11 \\cdot 2})\\)，当给定\\(X^{(2)}=x^{(2)}\\)，即\\(Y^{(2)}=y^{(2)}\\)的条件下，\\(X^{(1)}=Y^{(1)}+\\Sigma_{12}\\Sigma_{22}^{-1}x^{(2)} \\sim N_q(\\mu_1+\\Sigma_{12}\\Sigma_{22}^{-1}(x^{(2)}-\\mu_2),\\Sigma_{11 \\cdot 2})\\)。 同理，给定\\(X^{(2)}=x^{(2)}\\)时，\\((X^{(1)}|X^{(2)}=x^{(2)}) \\sim N_q(\\mu_{1 \\cdot 2},\\Sigma_{11 \\cdot 2})\\)，其中\\(\\mu_{1 \\cdot 2}=\\mu^{(1)}+\\Sigma_{12}\\Sigma_{22}^{-1}(x^{(2)}-\\mu^{(2)}), \\, \\Sigma_{11 \\cdot 2}=\\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\\) 8.2.3.2 独立性 设\\(X \\sim N_p(\\mu,\\Sigma),\\, Y=AX+a, \\, Z= BX+b\\)，则Y和Z独立当且仅当\\(A\\Sigma B&#39;=0\\)。 证： \\[ W=\\begin{pmatrix} Y \\\\ Z \\end{pmatrix}=\\begin{pmatrix} A \\\\ B \\end{pmatrix}X+\\begin{pmatrix} a \\\\ b \\end{pmatrix} \\\\ W\\sim N\\begin{pmatrix} \\begin{pmatrix} A\\mu+a \\\\ B\\mu +b\\end{pmatrix},\\begin{pmatrix} A\\Sigma A&#39; &amp; A\\Sigma B&#39; \\\\ B\\Sigma A&#39; &amp; B\\Sigma B&#39;\\end{pmatrix} \\end{pmatrix} \\tag{8.31} \\] 显然当\\(A\\Sigma B&#39;=0\\)时，非主对角线元素为0，此时Y与Z独立。 8.2.4 偏相关系数与全相关系数 8.2.4.1 偏相关系数 对于\\((X^{(1)}|X^{(2)}=x^{(2)})\\sim N_q(\\mu_{1\\cdot 2},\\Sigma_{11 \\cdot 2})\\)，有\\(E(X^{(1)}|X^{(2)})=\\mu_{1 \\cdot 2}=\\mu^{(1)}+\\Sigma_{12}\\Sigma_{22}^{-1}(X^{(2)}-\\mu^{(2)})\\)，该形式与回归分析中的条件期望回归类似，因此可将\\(\\Sigma_{12}\\Sigma_{22}^{-1}\\)视为\\(X^{(1)}\\)对\\(X^{(2)}\\)的回归系数。记\\(\\Sigma_{11 \\cdot 2}=(\\sigma_{ij \\cdot q+1,...,p})_{q \\times q}\\)，则称\\(r_{ij \\cdot q+1,...,p}=\\frac{\\sigma_{ij \\cdot q+1,...,p}}{(\\sigma_{ii \\cdot q+1,...,p}\\sigma_{jj \\cdot q+1,...,p})^{\\frac{1}{2}}}\\)为在给定\\(X^{(2)}\\)条件下\\(X_i\\)和\\(X_j\\)的偏相关系数。 就是在给定条件下对条件协差阵求相关系数 8.2.4.2 全相关系数 对于随机向量X和随机变量y，设 \\[ Z=\\begin{pmatrix} X \\\\ y \\end{pmatrix} \\sim N\\begin{pmatrix} \\begin{pmatrix} \\mu_X \\\\ \\mu_y \\end{pmatrix} , \\begin{pmatrix} \\Sigma_{XX} &amp; \\Sigma_{Xy} \\\\ \\Sigma_{yX} &amp; \\sigma_{yy} \\end{pmatrix} \\end{pmatrix} \\] 称 \\[R=\\begin{pmatrix} \\frac{\\Sigma_{yX}\\Sigma_{XX}^{-1}\\Sigma_{Xy}}{\\sigma_{yy}} \\end{pmatrix}^{\\frac{1}{2}} \\] 为y与X的全相关系数。 随机向量拼上随机变量，其中\\(\\Sigma_{yX}\\)为\\(1 \\times p\\)维向量，\\(\\Sigma_{Xy}\\)为\\(p \\times 1\\)维向量 可以简记为大协差阵拆出两个标量构造分式，其中分母是\\(\\sigma_{yy}\\)，整个分式取根号就是全相关系数 特别的，\\(R=\\max\\limits_{Cov(a&#39;X)=1} corr(y,a&#39;X)\\)。 证： \\[ \\begin{aligned} (corr(y,a&#39;X))^2&amp;= \\frac{Cov^2(y,a&#39;X)}{Cov(y)Cov(a&#39;X)} \\\\ &amp;= \\frac{(\\Sigma_{yX}a)^2}{\\sigma_{yy}a&#39;\\Sigma_{XX}a} \\\\ &amp;\\leq \\frac{(\\Sigma_{Xy}&#39;\\Sigma_{XX}^{-1}\\Sigma_{Xy})(a&#39;\\Sigma_{XX}a)}{\\sigma_{yy}a&#39;\\Sigma_{XX}a} \\\\ &amp;= \\frac{(\\Sigma_{Xy}&#39;\\Sigma_{XX}^{-1}\\Sigma_{Xy})}{\\sigma_{yy}} \\\\ &amp;= \\frac{\\Sigma_{yX}\\Sigma_{XX}^{-1}\\Sigma_{Xy}}{\\sigma_{yy}} \\\\ &amp;= R^2 \\end{aligned} \\tag{8.32} \\] Cauchy-Schwarz不等式：设\\(B&gt;0\\)，则\\((x&#39;y)^2 \\leq (x&#39;Bx)(y&#39;B^{-1}y)\\)。这里选取\\(B=\\Sigma_{XX}\\)是为了把分母的\\(a&#39;\\Sigma_{XX}a\\)消掉 "],["ms_3.html", "8.3 主成分分析", " 8.3 主成分分析 定义：对原始变量进行线性变换构造出互不相关的主成分，主成分涵盖了原始变量的绝大部分信息。 其中线性变换意味着是对原始变量的线性组合；互不相关意味着主成分间的协方差为零；绝大部分信息意味着主成分的方差占比大。 令\\(X=(X_1,X_2,\\dots,X_p)&#39;\\)为p维随机向量，均值向量为\\(E(X)=\\mu\\)，协方差阵为\\(Cov(X)=\\Sigma\\)，主成分为\\(Z=(Z_1,Z_2,\\dots,Z_p)&#39;\\)，则主成分表示为 \\[ \\begin{cases} Z_1=a_1&#39;X=a_{11}X_1+\\dots+a_{p1}X_p \\\\ \\qquad \\qquad \\qquad \\quad \\vdots \\\\ Z_p=a_p&#39;X=a_{1p}X_1+\\dots+a_{pp}X_p \\end{cases} \\tag{8.33} \\] 为了限制量纲差异对方差的影响，限制\\(||a||=1\\)，则主成分的性质有 \\[ \\begin{aligned} &amp;(1) \\; a_i&#39;a_i=1 \\\\ &amp;(2) \\; a_i&#39;\\Sigma a_j=0, \\; j=1,2,\\dots,i-1 \\\\ &amp;(3) \\; Var(Z_i)= \\max_{a_i&#39;a_i=1, \\,a_i&#39;\\Sigma a_j=0 \\\\ j=1,2,\\dots,i-1} Var(a_i&#39;X) \\end{aligned} \\tag{8.34} \\] 8.3.1 总体主成分 8.3.1.1 基于协差阵的总体主成分 总体主成分的导出如下所示。不妨先求解第一主成分。 \\[ \\begin{array}{c} \\max \\limits_{a_1} a_1&#39;\\Sigma a_1 \\\\ s.t \\; a_1&#39;a_1=1 \\end{array} \\tag{8.35} \\] 将其转化为拉格朗日函数： \\[ L=a_1&#39;\\Sigma a_1 + \\lambda(1-a_1&#39;a_1) \\tag{8.36} \\] 对\\(a_1\\)求偏导得 \\[ \\frac{\\partial L}{\\partial a_1} = 2\\Sigma a_1 - 2\\lambda_1 a_1 = 0 \\tag{8.37} \\] 可知所求\\(\\lambda_1\\)及\\(a_1\\)即为\\(\\Sigma\\)的最大特征值及对应的特征向量。 进一步地，对于第1个主成分之后的所有主成分而言，都要满足与前面所有主成分均不相关的条件，于是有 \\[ \\begin{array}{c} \\max \\limits_{a_i} a_i&#39;\\Sigma a_i \\\\ s.t \\; a_i&#39;a_i=1, \\; a_i&#39;a_k=0, k=1,2,...,i-1 \\end{array} \\tag{8.38} \\] 转化为拉格朗日函数： \\[ L=a_i&#39; \\Sigma a_i + \\lambda_i(1-a_i&#39;a_i) + \\sum_{k=1}^{i-1}\\gamma_k a_i&#39;a_k \\tag{8.39} \\] 对\\(a_i\\)求偏导得 \\[ \\frac{\\partial L}{\\partial a_i} = 2\\Sigma a_i -2\\lambda_i a_i + \\sum_{k=1}^{i-1}\\gamma_k a_k = 0 \\tag{8.40} \\] 对上式左右两边同时乘以\\(a_1&#39;\\)，得 \\[ \\begin{aligned} 2a_1&#39;\\Sigma a_i -2\\lambda_i a_1&#39;a_i + \\sum_{k=1}^{i-1}\\gamma_k a_1&#39;a_k = 0 \\\\ \\gamma_1 a_1&#39;a_1 = 0 \\\\ \\gamma_1=0 \\end{aligned} \\tag{8.41} \\] 同理，对式(8.40)左右同乘\\(a_k&#39;, k=1,2,...,i-1\\)，可得\\(\\gamma_k=0\\)，于是式(8.40)转化为 \\[ 2\\Sigma a_i -2\\lambda_i a_i=0 \\tag{8.42} \\] 可知\\(\\lambda_i\\)和\\(a_i\\)分别是\\(\\Sigma\\)第\\(i\\)大的特征值及对应的特征向量。 因此，只需求得\\(X\\)的协差阵的特征向量即可得到主成分。 从几何上来看，\\(Z_i=a_i&#39;X\\)是\\(X\\)在\\(a_i\\)方向上的投影，对应的\\(\\lambda_i\\)就是该方向上投影点的方差。 故令\\(\\Sigma = P\\Lambda P&#39;\\)，则\\(Z=P&#39;X\\)，其中\\(P=(a_1|a_2|...|a_p)\\)，其性质如下所示 \\(Cov(Z)=Cov(P&#39;X)=P&#39;Cov(X)P=P&#39;\\Sigma P=\\Lambda\\) \\(\\sum \\lambda_i=tr(\\Lambda)=tr(P&#39;\\Sigma P)=tr(\\Sigma PP&#39;)=tr(\\Sigma)=\\sum\\sigma_{ii}\\) \\(\\rho(Z_k, X_i)=\\frac{Cov(Z_k, X_i)}{\\sqrt{\\lambda_k\\sigma_{ii}}}=\\frac{Cov(a_k&#39;X, e_i&#39;X)}{\\sqrt{\\lambda_k\\sigma_{ii}}}=\\frac{a_k&#39;\\Sigma e_i}{\\sqrt{\\lambda_k\\sigma_{ii}}}=\\frac{\\lambda_ka_k&#39;e_i}{\\sqrt{\\lambda_k\\sigma_{ii}}}=\\frac{\\sqrt{\\lambda_k}a_{ik}}{\\sqrt{\\sigma_{ii}}}\\) \\(\\sum_{k=1}^p\\rho^2(Z_k,X_i)=\\sum_{k=1}^p \\frac{\\lambda_k a_{ik}^2}{\\sigma_{ii}}=\\frac{1}{\\sigma_{ii}}\\sum_{k=1}^p \\lambda_k a_{ik}^2=1\\) 横向的，表示所有主成分对某个原始变量的方差解释百分比为100% \\(\\sum_{i=1}^p \\sigma_{ii}\\rho^2(Z_k,X_i)=\\sum_{i=1}^p \\sigma_{ii}\\frac{\\lambda_ka_{ik}^2}{\\sigma_{ii}}=\\lambda_k\\sum_{i=1}^pa_{ik}^2=\\lambda_k\\) 纵向的，表示单个主成分对所有原始变量的方差贡献，即为特征根 3.\\(e_i\\)为第\\(i\\)个位置上为1，其余位置为0的列向量 3.\\(a_k&#39;\\Sigma = (\\Sigma a_k)&#39; = (\\lambda_k a_k)&#39;\\) 4.\\(\\sigma_{ii}=(a_{i1}, a_{i2}, ..., a_{ip})\\Lambda (a_{i1}, a_{i2}, ..., a_{ip})&#39;=\\sum_{k=1}^p \\lambda_k a_{ik}^2\\) 5.\\(\\sum_{i=1}^pa_{ik}^2=1\\) 关于主成分的指标如下所示： 贡献率(第i个主成分)：\\(\\omega_i=\\frac{\\lambda_i}{\\sum_{k=1}^p \\lambda_k}\\) 累积贡献率(前m个主成分)：\\(f_m=\\frac{\\sum_{k=1}^m \\lambda_k}{\\sum_{i=1}^p \\lambda_i}\\) 方差贡献率(前m个主成分对\\(X_i\\)的贡献率)：\\(v_i^{(m)}=\\frac{\\sum_{k=1}^m \\lambda_k a_{ik}^2}{\\sigma_{ii}}\\) 8.3.1.2 基于相关阵的总体主成分 考虑变量量纲不同的影响，对数据进行标准化处理，得到\\(X_i^*=\\frac{X_i-\\mu_i}{\\sqrt{\\sigma_{ii}}}\\)，此时\\(X^*\\)的协差阵即为\\(X\\)的相关阵\\(R\\)。 同理可得，\\(Z_k^*=a_k^{*&#39;}X^*=a_k^{*&#39;}D^{-\\frac{1}{2}}(X-\\mu)\\)，其中\\(a_k^*=(a_{1k}^*,...,a_{pk}^*)&#39;\\)是\\(R\\)对应于特征值\\(\\lambda_k^*\\)的单位正交特征向量，\\(D^{\\frac{1}{2}}=diag(\\sqrt{\\sigma_{11}},...,\\sqrt{\\sigma_{pp}})\\)，相关性质如下所示： \\(Cov(Z^*)=Cov(P^{*&#39;}X^*)=P^{*&#39;}Cov(X^{*})P^{*}=P^{*&#39;}RP^{*}=\\Lambda^*\\) \\(\\sum \\lambda_i^*=tr(\\Lambda^*)=tr(P^{*&#39;}RP^{*&#39;})=tr(RP^*P^{*&#39;})=tr(R)=p\\) \\(\\rho(Z_k^*, X_i^*)=\\frac{Cov(Z_k^*, X_i^*)}{\\sqrt{\\lambda_k^*}}=\\frac{Cov(a_k^{*&#39;}X^*, e_i&#39;X^*)}{\\sqrt{\\lambda_k^*}}=\\frac{a_k^{*&#39;}R e_i}{\\sqrt{\\lambda_k^*}}=\\frac{\\lambda_k^*a_k^{*&#39;}e_i}{\\sqrt{\\lambda_k^*}}=\\sqrt{\\lambda_k^*}a_{ik}^{*}\\) \\(\\sum_{k=1}^p\\rho^2(Z_k^*,X_i^*)=\\sum_{k=1}^p \\lambda_k^* a_{ik}^{*2}=1\\) \\(\\sum_{i=1}^p \\rho^2(Z_k^*,X_i^*)=\\sum_{i=1}^p \\lambda_k^*a_{ik}^{*2}=\\lambda_k^*\\sum_{i=1}^pa_{ik}^{*2}=\\lambda_k^*\\) 简单记，就把\\(\\sigma_{ii}=1\\)代入基于协差阵的结果，对应符号添加*号即可 什么时候需要标准化？ 对变量进行标准化可以提高方差较小变量对主成分的贡献 当变量量纲差异较大时需要进行标准化处理 一般来说，从X的协方差矩阵导出的主成分和从其相关系数矩阵导出的主成分不相同，两个主成分没有显式数量关系，因此标准化后果是显著差异的 实际问题处理中，考虑是否需要避免出现方差最大主成分和原始变量呈线性比例关系的分析结果 8.3.2 样本主成分 定义观测矩阵为 \\[ X= \\begin{pmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{np} \\end{pmatrix}= \\begin{pmatrix} X&#39;_{(1)} \\\\ X&#39;_{(2)} \\\\ \\vdots \\\\ X&#39;_{(n)} \\\\ \\end{pmatrix} \\tag{8.43} \\] \\(X_{(i)}\\)为第i个样本，则样本均值、协方差阵、相关阵分别为 \\[ \\bar X = \\frac{1}{n}\\sum_{i=1}^nX_{(i)}=(\\bar x_1, ..., \\bar x_p)&#39; \\tag{8.44} \\] \\[ S=\\frac{1}{n-1}\\sum_{i=1}^n (X_{(i)}-\\bar X)(X_{(i)}-\\bar X)&#39;=(s_{ij})_{p\\times p} \\tag{8.45} \\] \\[ \\tilde R=(r_{ij})_{p \\times p}, \\quad r_{ij}=\\frac{s_{ij}}{\\sqrt{s_{ii}s_{jj}}} \\tag{8.46} \\] 8.3.2.1 基于协差阵的样本主成分 记样本协差阵S的特征根和单位正交特征向量为\\((\\hat \\lambda_i, \\hat a_i)\\)，则第i个样本主成分为\\(\\hat Z_i=\\hat a_i&#39;X\\)。 故样本\\(X_{(k)}\\)在第i个主成分上的值为\\(\\hat Z_{ik}=\\hat a_i&#39;X_{(k)}\\)，称其为\\(X_{(k)}\\)在第i个主成分上的得分。 第i个样本主成分的样本均值：\\(\\bar Z_i = \\frac{1}{n}\\sum_{k=1}^n \\hat Z_{ik}=\\hat a_i&#39;\\bar X\\) 第i个样本主成分的样本方差：\\(Cov(\\hat Z_i)=\\frac{1}{n-1}\\sum_{k=1}^n (Z_{ik}-\\bar Z_i)^2=\\frac{1}{n-1}\\sum_{k=1}^n (\\hat a_i&#39;X_{(k)}-\\hat a_i&#39; \\bar X_{(k)})^2=\\hat a_i&#39; S \\hat a_i = \\hat \\lambda_i\\) 当\\(i\\neq j\\)时，各主成分之间的样本协方差为： \\[ \\begin{aligned} Cov(\\hat Z_i, \\hat Z_j)&amp;=\\frac{1}{n-1}\\sum_{k=1}^n(Z_{ik}-\\bar Z_i)(Z_{jk}-\\bar Z_j) \\\\ &amp;=\\frac{1}{n-1}\\sum_{k=1}^n (\\hat a_i&#39;X_{(k)}-\\hat a_i&#39; \\bar X)(\\hat a_j&#39;X_{(k)}-\\hat a_j&#39; \\bar X)&#39; \\\\ &amp;= \\hat a_i&#39;S\\hat a_j \\\\ &amp;= \\hat \\lambda_j \\hat a_i \\hat a_j \\\\ &amp;=0 \\end{aligned} \\tag{8.47} \\] 类似的，总的样本方差为\\(\\sum_{i=1}^p s_{ii}=\\sum_{i=1}^p \\hat \\lambda_i\\)，样本相关系数为\\(r(\\hat Z_k, X_i)=\\frac{\\sqrt{\\hat \\lambda_k}\\hat a_{ik}}{\\sqrt{s_{ii}}}\\)。 8.3.2.2 基于相关阵的样本主成分 同样先对数据做标准化处理，标准化后的数据的协差阵即为原始数据的相关阵，其余操作同基于相关阵的总体主成分，只需要注意使用关于样本的符号即可。 8.3.3 关于主成分 8.3.3.1 主成分的局限性 仅考虑了原始变量的正交/线性变换。 PCA仅依赖于样本数据的均值和协方差矩阵，有些分布无法进行刻画。 当原始变量是相关的时候，使用PCA可以降低维数，若原始变量不相关，则无法有效降维。 PCA容易受到异常点的影响。 8.3.3.2 如何选取主成分个数 前m个主成分的累积贡献率达到某个阈值，如80%或85%以上。 无论是从协差阵还是相关阵出发，一个经验规则是保留特征值大于其平均值（或1）的主成分 绘制碎石图看拐点。 8.3.3.3 R语言实现 PCA相对较为简单，可以自定义函数，如下所示 my_pca &lt;- function(sigma){ n = dim(sigma)[1] var_name = paste0(&#39;var_&#39;, 1:n) component_name = paste0(&#39;comp_&#39;, 1:n) lambda_name = paste0(&#39;lambda_&#39;, 1:n) eigen = eigen(sigma) eigen_value = eigen$values names(eigen_value) = lambda_name eigen_vec = eigen$vectors #特征向量矩阵&amp;主成分矩阵 colnames(eigen_vec) = component_name # 贡献率 contribution_rate = eigen_value/sum(eigen_value) names(contribution_rate) = component_name # 累积贡献率 cum_contribution_rate = cumsum(eigen_value)/sum(eigen_value) names(cum_contribution_rate) = component_name # 各个主成分对变量的贡献率 lambda*a^2/sigma contribution_to_var = diag(diag(sigma)^(-1)) %*% eigen_vec^2 %*% diag(eigen_value) dimnames(contribution_to_var) = list(var_name, component_name) # 碎石图 library(ggplot2) scree_plot = ggplot()+ geom_line(aes(x=component_name, y=eigen_value, group=1), linetype=&#39;dashed&#39;)+ geom_point(aes(x=component_name, y=eigen_value), size=2.5)+ theme_bw()+ labs(x=&#39;&#39;, y=&#39;variance&#39;)+ theme(axis.text.x = element_text(size = rel(1.5)), axis.title.y = element_text(size = rel(1.3))) result=list( lambda = eigen_value, vector = eigen_vec, contribution_rate = contribution_rate, cum_contribution_rate = cum_contribution_rate, contribution_to_var = contribution_to_var, scree_plot = scree_plot ) result } 或者调用R中的函数princomp()、psych::principal()。 "],["ms_4.html", "8.4 因子分析", " 8.4 因子分析 因子分析是一种降维的方法，通过探寻变量的潜在结构，将其归纳为较少的因子，从而实现降维的目的。 8.4.1 正交因子模型 设X为可观测的p维随机向量，其均值为\\(\\mu\\)，协方差矩阵为\\(\\Sigma\\)，则正交因子模型可被表示为 \\[ \\begin{array}{c} X=AF+\\mu+\\varepsilon \\\\ E(F)=0, \\; Cov(F)=I_m \\\\ E(\\varepsilon)=0, \\; Cov(\\varepsilon)=\\Psi=diag(\\psi_1,...,\\psi_2) \\\\ Cov(F,\\varepsilon)=0 \\end{array} \\tag{8.48} \\] 可通过中心化处理而消去\\(\\mu\\) 其中\\(A\\)为\\(p \\times m\\)的因子载荷矩阵，\\(F=(F_1, ...,F_m)&#39;\\)为公共因子向量，\\(\\varepsilon=(\\varepsilon_1,...,\\varepsilon_p)&#39;\\)为特殊因子向量。 则X的协差阵可表示为 \\[ \\Sigma = Cov(X)=Cov(AF+\\mu+\\varepsilon)=ACov(F)A&#39;+Cov(\\varepsilon)=AA&#39;+\\Psi \\tag{8.49} \\] 若X已进行标准化处理，则 \\[ R=AA&#39;+\\Psi \\tag{8.50} \\] 由于\\(\\Psi\\)是对角阵，则\\(\\Sigma\\)和\\(R\\)的非对角线元素全部由因子载荷矩阵A决定 因子载荷矩阵的统计意义 \\[ \\begin{aligned} Cov(X,F)&amp;=E[X-E(X)][F-E(F)]&#39; \\\\ &amp;=E[(X-\\mu)F&#39;] \\\\ &amp;= E[(AF+\\varepsilon)F&#39;] \\\\ &amp;= AE(FF&#39;)+E(\\varepsilon F&#39;) \\\\ &amp;= A \\end{aligned} \\tag{8.51} \\] 由此可知，对于原始数据而言，因子载荷矩阵A记录了X与F之间的协方差，即\\(a_{ij}=Cov(X_i,F_j)\\)。 若对原始数据X进行标准化处理得到\\(X^*\\)，则 \\[ \\rho_{ij}=\\frac{Cov(X_i,F_j)}{\\sqrt{Var(X_i)}\\sqrt{Var(F_j)}}=Cov(X_i, F_j)=a_{ij} \\tag{8.52} \\] 变量共同度的统计意义 由式(8.48)可得 \\[ Var(X_i)=\\sigma_{ii}=\\sum_{j=1}^m a_{ij}^2+\\psi_i:=h_i+\\psi_i \\tag{8.53} \\] 其中\\(h_i\\)表现为全部公共因子对\\(X_i\\)的变异程度的解释，反映了\\(X_i\\)对公共因子的依赖程度，称为变量共同度，即因子载荷矩阵A的行元素平方和。 公共因子方差贡献的统计意义 \\[ \\sum_{i=1}^p Var(X_i) = \\sum_{i=1}^p a_{i1}^2Var(F_1)+...+\\sum_{i=1}^pa_{im}^2Var(F_m)+\\sum_{i=1}^pVar(\\varepsilon):=\\sum_{j=1}^m q_j+\\sum_{i=1}^p \\psi_i \\tag{8.54} \\] 其中\\(q_j=\\sum_{i=1}^p a_{ij}^2\\)，是因子载荷矩阵的列元素之和，反映了第j个公共因子对变量X的贡献及其相对重要性。进一步地，称\\(\\frac{q_j}{\\sum_{i=1}^pVar(X_i)}\\)为公共因子\\(F_j\\)所解释的总方差的比例。 性质： 因子分析具有尺度不变性 即对随机向量X的各个分量进行尺度放缩\\(X^*=CX, \\; C=diag(c_1,...,c_p)\\)后的随机向量\\(X^*\\)仍满足正交因子模型的条件，且公共因子\\(F\\)不变。 \\(CX=C\\mu+CAF+C\\varepsilon\\)，可将\\(CA\\)视作\\(A^*\\)因此公共因子\\(F\\)不变，变的是载荷矩阵 因子载荷矩阵不唯一 对于\\(AF\\)，可以有任一正交矩阵\\(\\Gamma\\)，使得\\(AF=(A\\Gamma)(\\Gamma&#39;F)\\)，其中可将\\(A\\Gamma\\)视作\\(A^*\\)，将\\(\\Gamma&#39;F\\)视作\\(F^*\\)，进行因子旋转后仍保持正交因子模型的条件。 正交矩阵的作用相当于进行旋转 8.4.2 参数估计 在参数估计前，得先判断变量之间是否存在足够强的相关关系来进行因子分析。 KMO检验 KMO检验用于检查变量间的相关性和偏相关性。KMO统计量的取值越接近1，表明变量间的相关性越强，偏相关性越弱，因子分析的效果越好。KMO统计量的值一般在0.7以上比较适合做因子分析。 R语言：KMO() Bartlett球形检验 Bartlett球形检验的原假设是变量的相关阵是单位阵，当拒绝原假设时，则说明变量间具有相关性，因子分析有效。 R语言：psych::cortest.bartlett() 8.4.2.1 主成分法 令\\(\\Sigma\\)的特征值为\\(\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_p \\geq 0\\)，对\\(\\Sigma\\)进行谱分解 \\[ \\begin{aligned} \\Sigma &amp;= P\\Lambda P&#39; \\\\ &amp;= \\sum_{i=1}^p \\lambda_ip_ip_i&#39; \\\\ &amp;= \\sum_{i=1}^m \\lambda_ip_ip_i&#39;+\\sum_{i=m+1}^p \\lambda_ip_ip_i&#39; \\\\ &amp;\\approx \\begin{pmatrix} \\sqrt{\\lambda_1}p_1 &amp; ... &amp; \\sqrt{\\lambda_m}p_m \\end{pmatrix} \\begin{pmatrix} \\sqrt{\\lambda_1}p_1&#39; \\\\ \\vdots \\\\ \\sqrt{\\lambda_m}p_m&#39; \\end{pmatrix} + \\begin{pmatrix} \\psi_1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; \\psi_p \\end{pmatrix} \\\\ &amp;= \\hat A \\hat A&#39;+\\hat \\Psi \\end{aligned} \\tag{8.55} \\] 对\\(\\Sigma\\)进行谱分解，取特征值较大的前m个特征值及对应的特征向量构成因子载荷矩阵A，忽视掉其余较小的特征值及对应的特征向量。其中的\\(\\psi_i\\)则通过\\(\\hat \\psi_i = \\sigma_{ii}-\\sum_{j=1}^m a_{ij}^2\\)计算得到。 此时因子载荷矩阵中的每一列都是\\(\\sqrt{\\lambda_i}p_i\\)。特别的，当\\(\\lambda_{m+1}=...=\\lambda_p=0\\)时，此时有\\(\\Sigma=\\hat A \\hat A&#39;\\)，表明\\(\\hat A=P\\Lambda^{\\frac{1}{2}}\\)就是真实的载荷矩阵 当然也可以对变量进行标准化后利用主成分法进行求解。此时公共因子\\(F_j\\)的贡献为\\(q_j=\\sum_{i=1}^p \\hat a_{ij}^2=||\\sqrt{\\lambda_j}p_j||^2 = \\lambda_j\\) m的选择可参考主成分分析 8.4.2.2 主因子法 由于因子具有尺度不变性，对X进行标准化后得到 \\[ R=AA&#39;+\\Psi \\tag{8.56} \\] 若\\(R\\)和\\(\\Psi\\)都已知，则称\\(R^*=R-\\Psi=AA&#39;\\)为“约相关矩阵”。 其中\\(R\\)可表示为 \\[ \\begin{cases} \\rho_{ij}=a_{i1}a_{j1}+a_{i2}a_{j2}+...+a_{im}a_{jm} \\quad &amp;1 \\leq i \\neq j \\leq q \\\\ \\rho_{ii} = a_{i1}^2 + a_{i2}^2 + ...+a_{im}^2+\\psi_i = h_i+\\psi_i=1 \\quad &amp;i=1,2,...,p \\end{cases} \\tag{8.57} \\] \\(R^*\\)可表示为 \\[ \\begin{cases} \\rho_{ij}=a_{i1}a_{j1}+a_{i2}a_{j2}+...+a_{im}a_{jm} \\quad &amp;1 \\leq i \\neq j \\leq q \\\\ \\rho_{ii} = a_{i1}^2 + a_{i2}^2 + ...+a_{im}^2= h_i \\quad &amp;i=1,2,...,p \\end{cases} \\tag{8.58} \\] 关键点为\\(h_i=1-\\psi_i\\) 设\\(R^*\\)的特征值为\\(\\lambda_1^* \\geq \\lambda_2^* \\geq ... \\geq \\lambda_p^* \\geq 0\\)，对应的特征向量为\\(p_i^*\\)，此时可对\\(R^*\\)进行谱分解，得到 \\[ R^*=R-\\Psi=\\hat A \\hat A&#39; \\approx \\begin{pmatrix} \\sqrt{\\lambda_1^*}p_1^* &amp; ... &amp; \\sqrt{\\lambda_m^*}p_m^* \\end{pmatrix} \\begin{pmatrix} \\sqrt{\\lambda_1^*}p_1^{*&#39;} \\\\ \\vdots \\\\ \\sqrt{\\lambda_m^*}p_m^{*&#39;} \\end{pmatrix} \\tag{8.59} \\] 此时\\(\\hat A = \\begin{pmatrix} \\sqrt{\\lambda_1^*}p_1^* &amp; ... &amp; \\sqrt{\\lambda_m^*}p_m^* \\end{pmatrix}\\)，特殊因子方差的估计为\\(\\hat \\psi_i = 1-\\sum_{j=1}^m \\hat a_{ij}^2\\) 主因子法的前提是得知道\\(\\Psi\\)，但在实际中\\(\\Psi\\)一般未知，因此可根据样本进行估计。也可根据迭代算法，根据初始值\\(\\Psi^{(0)}\\)开始迭代，得到\\(\\hat A^{(1)}\\)，再根据\\(\\hat A^{(1)}\\)得到\\(\\Psi^{(1)}\\)，不断重复，直至收敛 8.4.2.3 极大似然法 假设公共因子\\(F \\sim N_m(0,I)\\)，特殊因子\\(\\varepsilon\\sim N_p(0,\\Psi)\\)，且相互独立，则\\(X \\sim N_p(\\mu, \\Sigma)\\)。构造似然函数如下所示 \\[ L(\\mu, \\Sigma)=(2\\pi)^{-\\frac{np}{2}}|\\Sigma|^{-\\frac{n}{2}}\\exp[-\\frac{1}{2}\\sum_{i=1}^n(X_{(i)}-\\mu)&#39;\\Sigma^{-1}(X_{(i)}-\\mu)] \\tag{8.60} \\] 其中\\(\\mu\\)的极大似然估计为\\(\\hat \\mu = \\bar X\\)，回代可得 \\[ \\begin{aligned} L(\\Sigma)&amp;=(2\\pi)^{-\\frac{np}{2}}|\\Sigma|^{-\\frac{n}{2}}\\exp[-\\frac{1}{2}\\sum_{i=1}^n(X_{(i)}-\\bar X)&#39;\\Sigma^{-1}(X_{(i)}-\\bar X)] \\\\ &amp;= (2\\pi)^{-\\frac{np}{2}}|\\Sigma|^{-\\frac{n}{2}}\\exp[-\\frac{1}{2}tr(\\sum_{i=1}^n(X_{(i)}-\\bar X)&#39;\\Sigma^{-1}(X_{(i)}-\\bar X))] \\\\ &amp;= (2\\pi)^{-\\frac{np}{2}}|\\Sigma|^{-\\frac{n}{2}}\\exp[-\\frac{1}{2}tr(\\sum_{i=1}^n(X_{(i)}-\\bar X)(X_{(i)}-\\bar X)&#39;\\Sigma^{-1})] \\\\ &amp;= (2\\pi)^{-\\frac{np}{2}}|\\Sigma|^{-\\frac{n}{2}}\\exp[-\\frac{1}{2}tr(D\\Sigma^{-1})] \\end{aligned} \\tag{8.61} \\] 其中\\(D=\\sum_{i=1}^n(X_{(i)}-\\bar X)(X_{(i)}-\\bar X)&#39;\\)为样本离差阵。 根据\\(\\Sigma = AA&#39;+\\Psi\\)，则 \\[ L(A,\\Psi)=(2\\pi)^{-\\frac{np}{2}}|AA&#39;+\\Psi|^{-\\frac{n}{2}}\\exp[-\\frac{1}{2}tr(D(AA&#39;+\\Psi)^{-1})] \\tag{8.62} \\] 求解\\(\\hat A\\)和\\(\\hat \\Psi\\)使得对数似然函数\\(\\ln L(A,\\Psi)\\)达到最大即可。 8.4.2.4 R语言实现 R语言factanal()、psych::fa() 8.4.3 因子旋转 前面已经介绍了正交因子模型的因子载荷矩阵不唯一。为了因子载荷矩阵的可解释性，可对因子载荷矩阵进行旋转，常用的方法有最大方差旋转法，该方法使得各列载荷向量的方差尽可能地大，从而在一列载荷向量上同时出现较大的载荷与较小的载荷，则该因子主要表现为较大的载荷。 8.4.4 因子得分 Bartlett因子得分和Thomson因子得分。 8.4.5 因子分析和主成分分析的区别与联系 区别： 主成分分析中主成分是变量的线性组合；而因子分析中自变量是因子的线性组合 主成分分析中的主成分不可旋转且唯一，在解释性上稍差；因子分析中的因子不唯一，可以通过因子旋转来获取较好的解释性 主成分分析只能通过对协方差阵进行特征分解来获取主成分；因子分析能够通过主成分法、主因子法、极大似然法等多种方法进行估计 联系： 二者都是以降维为目的，希望通过少量彼此不相关的主成分或因子来反映原始变量的绝大部分信息 二者分析的变量间均需要具有一定的相关性 二者都可运用原始变量的协差阵或相关阵进行求解 "],["ms_5.html", "8.5 判别分析", " 8.5 判别分析 8.5.1 距离判别 根据样品距离各个总体的远近来判断该样品属于哪个总体。记\\(d(X,G_i)\\)表示样品\\(X\\)到总体\\(G_i\\)的距离，则判别法则可表示为 \\[ \\delta(X) = \\mathop{\\arg\\max}\\limits_{i} \\; d(X,G_i) \\tag{8.63} \\] 距离取马氏距离\\(d(X,G_i)=(X-\\mu)&#39;\\Sigma_i^{-1}(X-\\mu)\\)，其中\\(\\Sigma_i\\)表示总体\\(G_i\\)的协方差矩阵。 8.5.1.1 两总体且具有相同协差阵 当两总体\\(\\Sigma_1=\\Sigma_2=\\Sigma\\)时，有 \\[ \\begin{aligned} &amp;d(X,G_1)-d(X,G_2) \\\\ =&amp; (X-\\mu_1)&#39;\\Sigma^{-1}(X-\\mu_1)-(X-\\mu_2)&#39;\\Sigma^{-1}(X-\\mu_2) \\\\ &amp;= X&#39;\\Sigma^{-1}X-2X&#39;\\Sigma^{-1}\\mu_1+\\mu_1&#39;\\Sigma^{-1}\\mu_1-(X&#39;\\Sigma^{-1}X-2X&#39;\\Sigma^{-1}\\mu_2+\\mu_2&#39;\\Sigma^{-1}\\mu_2) \\\\ &amp;= 2X&#39;\\Sigma^{-1}(\\mu_2-\\mu_1)+\\mu_1&#39;\\Sigma^{-1}\\mu_1-\\mu_2&#39;\\Sigma^{-1}\\mu_2 \\\\ &amp;= 2X&#39;\\Sigma^{-1}(\\mu_2-\\mu_1)+(\\mu_1+\\mu_2)&#39;\\Sigma^{-1}(\\mu_1-\\mu_2) \\\\ &amp;= -2(X-\\frac{\\mu_1+\\mu_2}{2})&#39;\\Sigma^{-1}(\\mu_1-\\mu_2) \\\\ &amp;= -2(X-\\bar \\mu)&#39;a \\\\ &amp;= -2a&#39;(X-\\bar \\mu) \\end{aligned} \\tag{8.64} \\] 其中\\(\\bar \\mu=\\frac{\\mu_1+\\mu_2}{2}\\)，\\(a=\\Sigma^{-1}(\\mu_1-\\mu_2)\\)，称a为判别系数向量。 称\\(W(X)=a&#39;(X-\\bar \\mu)\\)为判别函数，则 \\[ \\begin{cases} X\\in G_1, &amp;\\textrm{if } W(X) \\geq 0 \\\\ X\\in G_2, &amp;\\textrm{if } W(X) \\lt 0 \\end{cases} \\tag{8.65} \\] 进一步地，当已知总体分布时，不妨令\\(G_1 \\sim N_p(\\mu_1, \\Sigma)\\)，则当\\(X \\in G_1\\)时，\\(W(X)=a&#39;(X-\\bar \\mu) \\sim N(a&#39;(\\mu_1-\\mu_2)/2,a&#39;\\Sigma a)\\)。 根据\\(a=\\Sigma^{-1}(\\mu_1-\\mu_2)\\)，又有\\((\\mu_1-\\mu_2)=\\Sigma\\Sigma^{-1}(\\mu_1-\\mu_2)=\\Sigma a\\)，故令\\(\\Delta^2=(\\mu_1-\\mu_2)&#39;\\Sigma^{-1} (\\mu_1-\\mu_2) = a&#39;(\\mu_1-\\mu_2)=a&#39;\\Sigma a\\)。则\\(W(X) \\sim N(\\frac{1}{2}\\Delta^2,\\Delta^2)\\)，故误判概率为 \\[ P(2|1) = P(W(X)&lt;0|X\\in G_1)=\\Phi(-\\frac{\\Delta}{2}) \\tag{8.66} \\] 同理，对于\\(G_2 \\sim N_p(\\mu_2, \\Sigma)\\)，当\\(X \\in G_2\\)时，有\\(W(X)\\sim N(-\\frac{1}{2}\\Delta^2,\\Delta^2)\\)，误判概率为 \\[ P(1|2)=P(W(X) \\geq 0|X \\in G_2)=1-\\Phi(\\frac{\\Delta}{2})=\\Phi(-\\frac{\\Delta}{2}) \\tag{8.67} \\] 在实践中，常用样本均值替代总体均值，样本方差替代总体方差，若假设等方差，则使用联合估计 若不能假定总体为正态分布时，可用样本的误判比例替代误判概率。 回代法 用所有样本构造判别函数，再用该判别函数去判断样品所属总体，计算误判比例。 划分样本 将所有样本划分为训练集和验证集，训练集用于构造判别函数，验证集用于计算误判比例。 交叉验证法 分别从\\(G_1\\)、\\(G_2\\)中各取出1个观测值，再用剩余观测值来构造判别函数，根据判别函数来对这两个观测值进行判断，记录判断结果。不断重复这个过程，最后计算误判比例。 8.5.1.2 其他情形 这里的其他情形是指多总体的或协差阵不相同的情况，核心思想还是找到最小的\\(d(X,G_i)\\)。 例如多总体且协差阵相同时，有 \\[ d(X,G_i)=X&#39;\\Sigma^{-1}X-2\\mu_i&#39; \\Sigma^{-1} X+\\mu_i&#39;\\Sigma^{-1} \\mu_i=X&#39;\\Sigma^{-1}X-2(I_i&#39;X+c_i) \\tag{8.67} \\] 其中\\(I_i=\\Sigma^{-1}\\mu_i, \\; c_i=-\\frac{1}{2}\\mu_i&#39;\\Sigma^{-1}\\mu_i\\)。可以看到，\\(d(X,G_i)\\)中的\\(X&#39;\\Sigma^{-1}X\\)是固定的，因此判别规则即为\\(X\\in G_i, \\textrm{if }I_i&#39;X+c_i=\\max\\limits_{j}(I&#39;_jX+c_j)\\)。 8.5.2 贝叶斯判别 贝叶斯判别相较于距离判别法多考虑了先验信息，并根据样本信息得到后验概率分布，通过后验概率分布来进行统计推断。 假设有k个总体G，各个总体的概率密度函数为\\(f_i(x)\\)，先验概率为\\(q_i\\)，则根据样本得到的后验概率为 \\[ P(G_i|x)=\\frac{q_if_i(x)}{\\sum_{j=1}^kq_jf_j(x)} \\tag{8.68} \\] 先验概率可根据历史资料或经验得到，或者用训练样本中各类所占比例作为先验概率，或者直接令各类先验概率均相等 8.5.2.1 最大后验概率法 根据样品计算得到的后验概率进行判别，取最大后验概率对应的总体作为样品的判别结果，即 \\[ X \\in G_i, \\quad \\textrm{if } P(G_i|x)=\\max\\limits_{j} P(G_j|x) \\tag{8.69} \\] 当总体为正态分布时，对应的密度函数为 \\[ f_i(x)=(2\\pi)^{-\\frac{p}{2}}|\\Sigma_i|^{-\\frac{1}{2}}\\exp(-\\frac{1}{2}(x-\\mu_i)&#39;\\Sigma_i^{-1}(x-\\mu_i))=(2\\pi)^{-\\frac{p}{2}}|\\Sigma_i|^{-\\frac{1}{2}}\\exp(-\\frac{1}{2}d(x,G_i)) \\tag{8.70} \\] 则后验概率为 \\[ \\begin{aligned} P(G_i|x)&amp;=\\frac{q_i(2\\pi)^{-\\frac{p}{2}}|\\Sigma_i|^{-\\frac{1}{2}}\\exp(-\\frac{1}{2}d(x,G_i))}{\\sum_{j=1}^k q_j(2\\pi)^{-\\frac{p}{2}}|\\Sigma_j|^{-\\frac{1}{2}}\\exp(-\\frac{1}{2}d(x,G_j))} \\\\ &amp;= \\frac{q_i|\\Sigma_i|^{-\\frac{1}{2}}\\exp(-\\frac{1}{2}d(x,G_i))}{\\sum_{j=1}^k q_j |\\Sigma_j|^{-\\frac{1}{2}}\\exp(-\\frac{1}{2}d(x,G_j))} \\\\ &amp;= \\frac{\\exp (-\\frac{1}{2}d(x,G_i)+\\ln q_i-\\frac{1}{2}\\ln |\\Sigma_i|)}{\\sum_{j=1}^k \\exp (-\\frac{1}{2}d(x,G_j)+\\ln q_j-\\frac{1}{2}\\ln |\\Sigma_j|)} \\\\ &amp;= \\frac{\\exp [-\\frac{1}{2}(d(x,G_i)-2\\ln q_i+\\ln |\\Sigma_i|)]}{\\sum_{j=1}^k \\exp [-\\frac{1}{2}(d(x,G_j)-2\\ln q_j+\\ln |\\Sigma_j|)]} \\end{aligned} \\tag{8.71} \\] 当假定先验概率均相等或者协差阵均相等时，对应的\\(q\\)和\\(|\\Sigma|\\)均可消掉。不妨令\\(D(x,G_i)=d(x,G_i)+\\ln|\\Sigma_i|-2\\ln q_i\\)，称其为广义平方距离，则正态假设下的最大后验概率法等价于 \\[ X \\in G_i, \\quad \\textrm{if } D(x,G_i)=\\min\\limits_{j} D(x,G_j) \\tag{8.72} \\] 注意\\(\\exp\\)里面有负号，所以最大化就变成最小化了 8.5.2.2 最小期望误判代价法 以两总体的情形为例，沿用最大后验概率法中的记号，并引入误判损失，记为\\(C(2|1),\\,C(1|2)\\)，分别表示来自总体\\(G_1\\)却被误判为\\(G_2\\)的损失和来自总体\\(G_2\\)却被误判为\\(G_1\\)的损失。在判别规则下，\\(R^p\\)空间被划分为两个空间\\(R_1\\)和\\(R_2\\)，且$R_1 R_2=, ,R_1 R_2= $。 因此，若样品来自\\(G_1\\)，则正确判别的概率为 \\[ P(1|1)=P(x\\in R_1 | x \\in G_1)=\\int_{R_1}f_1(x)dx \\tag{8.73} \\] 误判的概率为 \\[ P(2|1)=P(x\\in R_2 | x \\in G_1)=\\int_{R_2}f_1(x)dx \\tag{8.74} \\] 若样品来自\\(G_2\\)同理。 定义期望误判损失(Expected Cost of Misclassification) \\[ \\begin{aligned} ECM&amp;=E(C(l|i)) \\\\ &amp;= C(2|1)P(x\\in G_1, x\\in R_2)+C(1|2)P(x\\in G_2, x\\in R_1) \\\\ &amp;= C(2|1)P(x \\in R_2 | x \\in G_1)P(x \\in G_1)+C(1|2)P(x \\in R_1 | x \\in G_2)P(x \\in G_2) \\\\ &amp;= C(2|1)P(2|1)q_1+C(1|2)P(1|2)q_2 \\\\ &amp;= C(2|1)q_1\\int_{R_2}f_1(x)dx+C(1|2)q_2\\int_{R_1}f_2(x)dx \\\\ &amp;= C(2|1)q_1\\int_{R_2}f_1(x)dx+C(1|2)q_2(1-\\int_{R_2}f_2(x)dx) \\\\ &amp;= \\int_{R_2} [C(2|1)q_1f_1(x)-C(1|2)q_2f_2(x)]dx + C(1|2)q_2 \\\\ &amp;\\propto \\int_{R_2} [C(2|1)q_1f_1(x)-C(1|2)q_2f_2(x)]dx \\\\ &amp;=1- \\int_{R_1} [C(2|1)q_1f_1(x)-C(1|2)q_2f_2(x)]dx \\end{aligned} \\tag{8.75} \\] 积分是曲线下的有向面积，当\\(C(2|1)q_1f_1(x)-C(1|2)q_2f_2(x)\\geq 0\\)时，对其积分会变大，为了最小化ECM，应归为\\(R_1\\)，同理，当\\(C(2|1)q_1f_1(x)-C(1|2)q_2f_2(x)\\lt 0\\)时，应归为\\(R_2\\)，即 \\[ R_1=\\{x:C(2|1)q_1f_1(x)\\geq C(1|2)q_2f_2(x)\\} \\\\ R_2=\\{x:C(2|1)q_1f_1(x) \\lt C(1|2)q_2f_2(x)\\} \\tag{8.76} \\] 当总体为正态分布，且协差阵相等时，则 \\[ \\begin{aligned} C(2|1)q_1f_1(x) &amp;\\geq C(1|2)q_2f_2(x) \\\\ \\frac{f_1(x)}{f_2(x)} &amp;\\geq \\frac{C(1|2)q_2}{C(2|1)q_1} \\\\ \\frac{\\exp\\{-\\frac{1}{2}(x-\\mu_1)&#39;\\Sigma^{-1}(x-\\mu_1)\\}}{\\exp\\{-\\frac{1}{2}(x-\\mu_2)&#39;\\Sigma^{-1}(x-\\mu_2)\\}} &amp;\\geq \\frac{C(1|2)q_2}{C(2|1)q_1} \\\\ \\exp\\{-\\frac{1}{2}[(x-\\mu_1)&#39;\\Sigma^{-1}(x-\\mu_1)-(x-\\mu_2)&#39;\\Sigma^{-1}(x-\\mu_2)]\\} &amp;\\geq \\frac{C(1|2)q_2}{C(2|1)q_1} \\\\ \\exp\\{-\\frac{1}{2}[-2x&#39;\\Sigma^{-1}\\mu_1+\\mu_1&#39;\\Sigma^{-1}\\mu_1+2x&#39;\\Sigma^{-1}\\mu_2-\\mu_2&#39;\\Sigma^{-1}\\mu_2]\\} &amp;\\geq \\frac{C(1|2)q_2}{C(2|1)q_1} \\\\ \\exp\\{-\\frac{1}{2}[-2x&#39;\\Sigma^{-1}(\\mu_1-\\mu_2)+(\\mu_1+\\mu_2)&#39;\\Sigma^{-1}(\\mu_1-\\mu_2)]\\} &amp;\\geq \\frac{C(1|2)q_2}{C(2|1)q_1} \\\\ \\exp\\{(x-\\frac{\\mu_1+\\mu_2}{2})&#39;\\Sigma^{-1}(\\mu_1-\\mu_2) \\} &amp;\\geq \\frac{C(1|2)q_2}{C(2|1)q_1} \\\\ a&#39;(x-\\bar \\mu) &amp;\\geq \\ln{[\\frac{C(1|2)q_2}{C(2|1)q_1}]} \\end{aligned} \\tag{8.77} \\] 其中\\(a=\\Sigma^{-1}(\\mu_1-\\mu_2), \\, \\bar \\mu =\\frac{\\mu_1+\\mu_2}{2}\\)。这等价于 \\[ R_1=\\{x:a&#39;(x-\\bar \\mu) \\geq \\ln{[\\frac{C(1|2)q_2}{C(2|1)q_1}]}\\} \\\\ R_2=\\{x:a&#39;(x-\\bar \\mu) \\lt \\ln{[\\frac{C(1|2)q_2}{C(2|1)q_1}]}\\} \\tag{8.78} \\] 这里出现了距离判别中的\\(a&#39;(x-\\bar \\mu)\\)，当不考虑先验概率和误判代价时，距离判别和贝叶斯判别等价 上述假定了两总体是正态的，且协差阵相等的情形，倘若协差阵不相等时，同第4.3.1.2节一样，直接把正态密度函数的指数部分看成马氏距离\\(d(x,G_i)\\)，并且保留\\(|\\Sigma_i|\\)，再进行讨论。 更一般的情形则是 \\[ \\begin{aligned} ECM&amp;=E[C(l|i)] \\\\ &amp;=\\sum_{i=1}^k\\sum_{l=1}^k C(l|i)P(x\\in G_i,x \\in R_l) \\\\ &amp;= \\sum_{i=1}^k\\sum_{l=1}^k C(l|i)P(x \\in R_l|x\\in G_i)P(x\\in G_i) \\\\ &amp;= \\sum_{i=1}^k\\sum_{l=1}^k C(l|i)P(l|i)q_i \\\\ &amp;= \\sum_{l=1}^k\\sum_{i=1}^k C(l|i)P(l|i)q_i \\end{aligned} \\tag{8.79} \\] 最后两行虽然都是对所有情形的遍历，但内外求和顺序的不同代表着其思想也是不同。倒数第二行的外层是对\\(i\\)求和，内层是对\\(l\\)求和，其含义是先确定样品所属的总体，再考虑把该样品判给其他总体所带来的损失和。而最后一行的外层是对\\(l\\)求和，内层是对\\(i\\)求和，其含义是先确定把样品判别给某一总体，再考虑不同总体下的样品判别给该总体的损失和。显然，后者更符合我们判别的诉求，也就是把样品归类，并计算这样做的平均代价。因此让ECM最小的判别规则为 \\[ R_t=\\{x:\\sum_{i\\neq t}^k C(t|i)P(t|i)q_i = \\min_{l} \\sum_{i\\neq l}^k C(l|i)P(l|i)q_i\\} \\tag{8.80} \\] 考虑\\(ECM=10=2+4+1+3\\)，这意味着现在有四个总体，把这个样品判别给这四个总体会分别带来2、4、1、3的平均损失，我们要最小化ECM，因此会选择平均损失为1的那个总体作为该样品的判别结果 "],["ms_6.html", "8.6 聚类分析", " 8.6 聚类分析 8.6.1 距离与相似性的度量 8.6.1.1 距离的度量 距离的定义应当具有如下性质 非负性：\\(d_{ij} \\geq 0\\) 对称性：\\(d_{ij}=d_{ji}\\) 三角不等式： \\(d_{ij} \\leq d_{ik} + d_{kj}\\) 常用的距离有闵可夫斯基距离、欧氏距离、马氏距离、曼哈顿距离。 8.6.1.2 相似性的度量 有时可能会对指标之间进行聚类，指标间的距离常用相似系数进行度量。相似系数具有如下性质 \\(c_{ij}= \\pm 1 \\Leftrightarrow X_i=aX_j, \\, a\\neq 0\\) \\(|c_{ij}| \\leq 1\\)，若\\(|c_{ij}|\\)越接近1则变量间关系越密切，越接近0则关系越疏远 \\(c_{ij}=c_{ji}\\) 常用的相似系数有相关系数、夹角余弦。 距离与相似系数之间可以相互转化 8.6.2 聚类效果的评价指标 8.6.2.1 内部评价指标 轮廓系数 \\[ s(i)=\\frac{b(i)-a(i)}{\\max\\{a(i), b(i)\\}} \\] \\(a(i)\\)表示样本i到同簇其他点的平均距离，度量簇内紧密度；\\(b(i)\\)表示样本i到最近邻簇所有点的平均距离，度量簇间分离度。 平均轮廓系数越大，表明聚类效果越好。 DBI \\[ DBI = \\frac{1}{k}\\sum_{i=1}^k \\max_{j \\neq i }(\\frac{\\sigma_i + \\sigma_j}{d(c_i, c_j)}) \\] 其中\\(\\sigma_i\\)表示簇i内样本到质心的平均距离，\\(d(c_i, d_j)\\)表示簇i与j质心间距离。 DBI越小越好。 CH指数 \\[ CH = \\frac{SSB/(k-1)}{SSW/(n-k)} \\] 其中\\(SSB\\)表示簇间平方和，\\(SSW\\)表示簇内平方和。 CH值越大越好。 8.6.2.2 外部评价指标 调整兰德指数 调整兰德指数是在兰德指数的基础上考虑了随机聚类的期望值，兰德指数定义为 \\(RI=\\frac{a+d}{a+b+c+d}\\) 其中a表示同属于一个簇且同属于一个真实类的样本对数，b表示同属于一个簇但不属于一个真实类的样本对数，c表示不属于一个簇但同属于一个真实类的样本对数，d表示不属于一个簇且不属于一个真实类的样本对数。 而调整兰德指数定义为 \\[ ARI = \\frac{RI-E(RI)}{\\max(RI)-E(RI)} \\] 混淆矩阵 8.6.3 系统聚类 8.6.3.1 类与类之间的距离 最短距离法：\\(D_{KL} = \\min\\limits_{i\\in G_K, j \\in G_L} \\{d_{ij}\\}\\) 适用于长条形的或不规则现状的类，对球形类的效果不是很好 最长距离法：\\(D_{KL} = \\max\\limits_{i\\in G_K, j \\in G_L} \\{d_{ij}\\}\\) 倾向于产生直径相等的类，易受异常值的影响 类平均法：\\(D_{KL}=\\frac{\\sum_{i\\in G_K, j \\in G_L}d_{ij}}{n_Kn_L}\\) 两类中所有距离的平均数，倾向于先合并方差小的类，而且偏向于产生方差相同的类 中间距离法：\\(D_{MJ}^2=\\frac{1}{2}D_{KJ}^2+\\frac{1}{2}D_{LJ}^2-\\frac{1}{4}D_{KL}^2\\) 平行四边形的对角线的中线距离， 重心法：\\(D_{KL}^2 = ||\\bar X_K - \\bar X_L||^2 = (\\bar X_K-\\bar X_L)&#39;(\\bar X_K-\\bar X_L)\\) 注意这里是平方距离，是重心差的内积。和其他系统聚类方法相比在处理异常值方面更稳健 Ward法：\\(D_{KL}^2 = ||\\bar X_K - \\bar X_L||^2/(\\frac{1}{n_K}+\\frac{1}{n_L})\\) 在重心法的基础上多除了各自数量的倒数和。该法倾向于先合并样品少的类，且对异常值非常敏感 下面是不同形状数据及各种距离的系统聚类方法效果。 能完全分开的球状数据 各种距离的系统聚类方法均适用。 不能完全分开的球状数据 Ward法、类平均法、重心法的聚类形状差不多，Ward法比较适合样本大小相等的球状数据的聚类。 最短距离法最差。 样本大小不等的球状数据 重心法的聚类效果最好，类平均法偏向产生方差相等的类。 并排拉长的数据 最短距离法效果最好。 Ward法、类平均法、重心法都不行。数据进行预处理后可以得到较好的聚类结果。 非球状数据 最短距离法聚类效果最好。 Ward法、类平均法、重心法都不行，即使对数据进行预处理后仍不起作用。 系统距离的特点： 无需事先指定类的数目。 需要确定相异度（距离/相似性系数）和联接度量准则。 运算量较大，适用于小规模数据。 一旦完成合并或分裂，则无法撤销或修正。 8.6.4 Kmeans 算法步骤： 输入观测样本数据和聚类数目K。 随机将所有观测样本分配到K个类中，作为样本初始类。 分别计算K个类的中心\\(\\bar x^{(i)}, \\, i=1,...,K\\)。 计算每个观测样本到其所属类的中心的平方距离\\(SSE=\\sum_{i=1}^K\\sum_{j\\in G_i} ||x_j-\\bar x^{(i)}||^2\\)。 重新将每个观测样本分配到距离其最近的类中心所在的类中，使得SSE减少。 重复第3-5步，直至SSE不在减少，得到最终的聚类结果。 kmeans聚类的特点： 核心思想是使同类内所有样本点的总体差异性尽可能小。 适用于中大规模样本数据。 需要事先指定聚类的数目K。 不同的初始类可能会导致不同结果。 适用于发现球状类。 可能会陷入局部解。 对异常值非常敏感。 8.6.5 其他聚类方法 8.6.5.1 DBSCAN DBSCAN(Density-Based Spatial Clustering of Applications with Noise)基于数据点在空间分布上的密度来发现任意形状的簇。簇被定义为密度相连的点的最大集合。密度低于某个阈值的区域被视为噪声。 并非所有观测点都属于簇，允许噪声的存在 DBSCAN需要定义两个参数：半径eps与区域内最少点数minPts。对于任意一个点，在其半径内若存在多于区域内最少点数个点，那么就将该店标记为核心点，找到所有的核心点，并将所有密度可达的核心点归于一个簇中。非核心点若与核心点相邻则也属于该簇，反之则为离群点。 在R中，可用dbscan包的dbscan()实现，并通过kNNdistplot()来辅助判断半径大小。 当给定minPts时（一般minPts&gt;=维度数+1），kNNdistplot()的横轴表示按到k近邻的距离从小到大排序的点，纵轴表示距离，找到曲线的拐点对应的距离就是较为合适的半径 8.6.5.2 GMM 假设整个数据集是K个多元高斯分布的混合体。每个点属于某个簇的概率由该高斯分布生成的概率决定。通常使用EM算法迭代优化。 在R中，可用mclust包的Mclust()实现。 "],["ms_7.html", "8.7 典型相关分析", " 8.7 典型相关分析 设X为随机向量，Y为随机变量，且满足\\(Cov\\begin{pmatrix} X \\\\ Y\\end{pmatrix}=\\begin{pmatrix} \\Sigma_{XX} &amp; \\Sigma_{XY} \\\\ \\Sigma_{YX} &amp; \\sigma_{Y} \\end{pmatrix}, \\, E\\begin{pmatrix} X \\\\ Y\\end{pmatrix}=0\\)，则复相关系数定义如下 \\[ \\max_{a \\in R^p} \\rho(Y,a&#39;X)= \\max_{a \\in R^p} \\frac{a&#39; \\Sigma_{XY}}{\\sigma_Y\\sqrt{a&#39;\\Sigma_{XX}a}}=\\frac{1}{\\sigma_Y}\\sqrt{\\Sigma_{YX}\\Sigma_{XX}^{-1}\\Sigma_{XY}} \\tag{8.81} \\] 设X为p维随机向量，Y为q维随机向量，且满足\\(Cov\\begin{pmatrix} X \\\\ Y\\end{pmatrix}=\\Sigma=\\begin{pmatrix} \\Sigma_{XX} &amp; \\Sigma_{XY} \\\\ \\Sigma_{YX} &amp; \\Sigma_{YY} \\end{pmatrix} &gt; 0\\)，设\\(a_1,\\,b_1\\)分别为p维和q维任意非零的常数向量，使得 \\[ \\max_{Var(a_1&#39;X)=1,Var(b_1&#39;Y)=1} \\rho(a_1&#39;X,b_1&#39;Y)=\\max_{Var(a_1&#39;X)=1,Var(b_1&#39;Y)=1}a_1&#39;\\Sigma_{XY}b_1 \\tag{8.82} \\] 则称\\((a_1&#39;X,b_1Y)\\)为第1对典型相关变量，他们之间的相关系数为第1典型相关系数。 接下来同主成分分析一样，要求第k典型相关系数，要求第k对典型变量与前k-1对典型变量均不相关，并且要求第k典型相关系数在\\(Var(a_k&#39;X)=Var(b_k&#39;Y)=1\\)的约束下是最大的。 "],["ml.html", "9 机器学习", " 9 机器学习 这个章节介绍一下常用的机器学习算法。 "],["ml_1.html", "9.1 基础知识", " 9.1 基础知识 9.1.1 偏差-方差权衡 记\\(y=f(x)+\\varepsilon, \\; E(\\varepsilon)=0\\)，\\(f\\)表示真实模型，\\(\\hat f\\)是模型某次训练得到的结果，\\(E(\\hat f)\\)表示训练模型的期望表现。 \\[ \\begin{aligned} E[(\\hat f-y)^2] &amp;= E[(\\hat f - E(\\hat f) + E(\\hat f)-y)^2] \\\\ &amp;= E[(\\hat f - E(\\hat f))^2] + E[(E(\\hat f)-y)^2] + 2E[(\\hat f - E(\\hat f))(E(\\hat f)-y)] \\\\ &amp;= E[(\\hat f - E(\\hat f))^2] + E[(E(\\hat f)-y)^2] \\\\ &amp;= E[(\\hat f - E(\\hat f))^2] + E[(E(\\hat f)-f-\\varepsilon)^2] \\\\ &amp;= E[(\\hat f - E(\\hat f))^2] + E[(E(\\hat f) - f)^2] + \\varepsilon^2 \\end{aligned} \\] 故模型的期望泛化错误率可拆解为方差+偏差+噪声 9.1.2 评价指标 分类问题 准确率 \\[ Accuracy = \\frac{TP + TN}{TP + TN + FP +FN} \\] 精确率（查准率）：有没有误报 \\[ Precision = \\frac{TP}{TP+FP} \\] 召回率（查全率）：有没有漏报 \\[ Recall = \\frac{TP}{TP+FN} \\] F1与\\(F_\\beta\\) \\[ F1 = \\frac{2*Precision*Recall}{Precision + Recall} \\\\ F_\\beta = \\frac{(1+\\beta^2)*Precision*Recall}{\\beta^2*Precision + Recall} \\] \\(0&lt;\\beta&lt;1\\)时精确率有更大影响，\\(\\beta&gt;1\\)时召回率有更大影响 ROC曲线与AUC：横轴假阳率FPR，纵轴真阳率TPR，全局性能评估 \\[ TPR = \\frac{TP}{TP+FN} \\\\ FPR = \\frac{FP}{FP+TN} \\] PR曲线与AUC：横轴召回率，纵轴精确率，更关注正样本预测质量 当存在类别不平衡情况时，PR曲线相较ROC曲线更敏感，能捕捉到异常 代价曲线：引入误判代价 宏平均：对于多个混淆矩阵，先计算各个混淆矩阵的指标，再求平均 微平均：对于多个混淆矩阵，先平均各个混淆矩阵，再求指标 回归问题 均方误差：对异常值敏感 均方根误差：量纲与目标变量一致 平均绝对误差：对异常值不敏感 \\(R^2\\)与\\(R^2_{adj}\\) 其他 AIC \\[ AIC = -2L(\\hat \\theta)_{max} + 2k \\] k是参数数量 BIC \\[ BIC = -2L(\\hat \\theta)_{max}+ k\\ln(n) \\] 9.1.3 特征工程 通俗地说，模型性能的上限由“数据与特征”决定，而算法只是尽可能逼近这个上限。因此，特征工程是把“原始数据”转化为“有信息量的特征”的过程，核心目标是让模型更容易学习到规律、提高预测精度与泛化能力。 9.1.3.1 探索性数据分析 了解数据在分布、类型、统计量、缺失值、异常值、实际含义等方面的基本信息。 方法： 数据可视化 注意辛普森悖论，引入分层变量进行探索 描述性统计 专家的先验知识 相关性分析 皮尔逊相关系数、斯皮尔曼秩相关系数（非参）、肯德尔秩相关系数（非参，有序变量）、列联表检验 9.1.3.2 数据预处理 缺失值 删除：删除记录或者直接删除特征 填充：用均值、中位数、众数、模型预测值、插值等方法进行填充 naniar包用于可视化缺失值，如vis_miss()、miss_var_summary()、miss_case_summary() 异常值 先要检查指标的口径与定义是否不一致。 识别：箱线图与四分位距IQR、Z-score、孤立森林 处理：删除、裁剪、数据分箱 9.1.3.3 特征构造 通过组合、聚合或数学操作生成新的特征。 取对数、取平方等数学变换 统计特征：如取均值、中位数、最值等 业务特征，如根据总量指标与人数指标构建人均指标 例如美团比赛中，给出骑手的基础指标（准时单量、拒绝单量、完成单量、开工时长、有单时长），就可以额外拓展出更能评价骑手表现的指标（准时率、拒绝率、工作效率、有单时长占比等指标） 又如要想让xgboost具有时序预测能力，可在特征中添加因变量的滞后项 数据分箱 9.1.3.4 特征变换 改变特征的尺度、编码方式。 中心标准化 极差标准化 哑变量编码：无多重共线性，适合线性模型 独热编码：适合树模型 标签编码：整数，但数字大小无意义 序数编码：整数，适用于顺序变量 目标编码：将分类变量与目标值的统计特征联系在一起 WOE编码：适用于风控建模场景，能够反映类别对目标的区分度 9.1.3.5 特征选择 剔除无关或冗余的特征。 惩罚函数法，如单变量选择、群组变量选择 基于统计指标（AIC、BIC、R方）等的模型选择（前向、后向选择） 递归特征消除（RFE）：重复训练模型，依次剔除最不重要的特征，直到特征数降到一定数量 树模型的重要性得分 SHAP值 降维方法：PCA、t-SNE、UMAP、自编码器 最后可根据交叉验证来评价给定模型在不同特征子集上的表现 "],["ml_2.html", "9.2 pandas与sklearn", " 9.2 pandas与sklearn 9.2.1 导入数据集 import pandas as pd # 读取CSV文件 df = pd.read_csv(path) # 读取Excel文件 df = pd.read_excel(path) # 读取Excel文件中的多个sheet，存储为字典 df = pd.read_excel(path, sheet_name=[&#39;Sheet1&#39;, &#39;Sheet2&#39;]) # 读取Excel文件中所有sheet，存储为字典 df = pd.read_excel(path, sheet_name=None) 9.2.2 数据预处理 缺失值处理 # 各列缺失值数量 df.count() df.isna().sum() # df.notna() # 各列缺失值比例 df.isnull().mean() * 100 # 缺失值可视化 import missingno as msno import matplotlib.pyplot as plt msno.matrix(df) # 矩阵图：直观展示缺失值在数据中的分布 plt.show() msno.bar(df) # 条形图：按列展示完整数据比例 plt.show() # 删除缺失值 # axis表示要压缩的维度。若axis=0，则压缩行，保留列方向的操作结果 df = df.dropna() # 删除包含任何缺失值的行 df = df.dropna(axis=1, how=&#39;all&#39;) # 删除全是缺失值的列 df = df.dropna(subset=[&quot;col1&quot;, &quot;col2&quot;]) # 删除特定列缺失值所在的行 # 填充缺失值 df_filled = df.fillna(0) # 用固定值填充 df[&#39;A&#39;] = df[&#39;A&#39;].fillna(df[&#39;A&#39;].mean()) # 均值 df[&#39;B&#39;] = df[&#39;B&#39;].fillna(df[&#39;B&#39;].median()) # 中位数 df = df.ffill() # 前向填充 df = df.bfill() # 后向填充 df = df.interpolate() # 用插值法填充（更平滑的填充方式） 异常值处理 # Z-score df_num = df.select_dtypes(include=[&#39;number&#39;]) # 筛选数值列 df_zscore = df_num.apply(lambda col:(col-col.mean())/col.std(), axis = 0) # df_zscore是数据框 df[(df_zscores.abs() &gt; 2).any(axis = 1)] # 四分位距 Q1 = num_df.quantile(0.25) Q3 = num_df.quantile(0.75) IQR = Q3 - Q1 outliers = ((num_df &lt; (Q1 - 1.5 * IQR)) | (num_df &gt; (Q3 + 1.5 * IQR))) df[df_num.columns] = df_num.clip(lower=Q1, upper=Q3, axis = 1) 标准化 from sklearn.preprocessing import StandardScaler # from sklearn.preprocessing import MinMaxScaler # 选取数值列 numeric_cols = df.select_dtypes(include=[&#39;number&#39;]).columns # 标准化并替换 scaler = StandardScaler() # scaler = MinMaxScaler() df[numeric_cols] = scaler.fit_transform(df[numeric_cols]) 编码 # 独热编码 df = pd.get_dummies(df, dtype = int) # 也可考虑from sklearn.preprocessing import OneHotEncoder # 哑变量编码 df = pd.get_dummies(df, dtype = int, drop_first=True) # 频率编码 frequency = df[&#39;A&#39;].value_counts(normalize=True) df[&#39;A&#39;] = df[&#39;A&#39;].map(frequency) # 有序变量编码 df[&#39;评分&#39;] = pd.Categorical( df[&#39;评分&#39;], categories=[&#39;低&#39;, &#39;中&#39;, &#39;高&#39;, &#39;极高&#39;], ordered=True ) df[&#39;评分编码&#39;] = df[&#39;评分&#39;].cat.codes # 自定义编码 edu_map = {&#39;小学&#39;: 1, &#39;中学&#39;: 2, &#39;大学&#39;: 3, &#39;硕士&#39;: 4, &#39;博士&#39;: 5} df[&#39;edu&#39;] = df[&#39;edu&#39;].map(edu_map) 表的连接与拼接 # on表示主键 # how表示连接方式：inner、outer、left、right df_merge = df1.merge(df2, on=&#39;id&#39;, how=&#39;inner&#39;) df_merge = pd.merge(df1, df2, on=&#39;id&#39;, how=&#39;inner&#39;) # 当键名不同时 df = pd.merge(df1, df2, left_on=&#39;id&#39;, right_on=&#39;user_id&#39;, how=&#39;inner&#39;) # 表拼接 # ignore_index=True表示充值索引 df_concat = pd.concat([df1, df2], axis = 0) # 纵向堆叠 df_concat = pd.concat([df1, df2], axis = 1) # 横向堆叠 数据透视表 .pivot()和pd.melt()都不会保留未用到的列，得注意。 # 长表变宽表 wide = df.pivot(index=&#39;name&#39;, columns=&#39;subject&#39;, values=&#39;score&#39;).reset_index() # 将索引列转化为第一列 wide = wide.merge(df[[&#39;name&#39;, &#39;age&#39;]].drop_duplicates(), on=&#39;name&#39;, how=&#39;left&#39;) # 保留原始列 # 若index存在重复值，则考虑pivot_table()，可以指定处理重复值的聚合函数 # 宽表变长表 # 参数value_vars是个列表，存储需要压缩的列名，如果不填，默认除id_vars外的所有列 pd.melt(wide, id_vars=[&#39;name&#39;, &#39;age&#39;], var_name=&#39;subject&#39;, value_name=&#39;score&#39;) 分组聚合 # 核心表达式 DataFrame.groupby(keys)[columns].agg(func) # 如果是多个列则columns应该再用列表嵌套 # 单函数聚合 df.groupby(&#39;region&#39;)[&#39;revenue&#39;].agg(&#39;sum&#39;) # 多函数聚合 df.groupby(&#39;region&#39;)[[&#39;revenue&#39;, &#39;unit&#39;]].agg([&#39;sum&#39;, &#39;mean&#39;]) # 字典映射（只对特定列进行聚合）：列名为键，函数为值 df.groupby(&#39;region&#39;).agg({ &#39;revenue&#39;: &#39;sum&#39;, &#39;units&#39;: &#39;mean&#39; }) # 命名聚合：新列名=(&#39;旧列名&#39;, &#39;函数名&#39;) df.groupby(&#39;region&#39;).agg( total_revenue=(&#39;revenue&#39;, &#39;sum&#39;), avg_units=(&#39;units&#39;, &#39;mean&#39;), max_revenue=(&#39;revenue&#39;, &#39;max&#39;) ) # 自定义函数 df.groupby(&#39;region&#39;)[&#39;revenue&#39;].agg(lambda x: x.max() - x.min()) 9.2.3 分割数据集 from sklearn.model_selection import train_test_split # 训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 训练集和验证集 X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42) # 分层分割 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify= categorical_var) # 交叉验证 from sklearn.model_selection import KFold kf = KFold(n_splits=10, shuffle=True, random_state=42) for train_index, test_index in kf.split(X, y): # 获取分割后的训练集和测试集 X_train, X_test = X[train_index], X[test_index] y_train, y_test = y[train_index], y[test_index] ... # 也可使用cross_val_score及cross_validate来简化交叉验证流程 cross_val_score(model, X, y, cv=5, scoring=&#39;accuracy&#39;) # 单个评估指标 cross_validate(model, X, y, cv=5, scoring=[&#39;accuracy&#39;, &#39;f1_macro&#39;], return_train_score=True) # 多个评估指标 "],["ml_3.html", "9.3 决策树", " 9.3 决策树 决策树(Decision Tree)可用于分类和回归任务。决策树从根结点出发，在一定的判断标准下，决策树在每个内部结点上寻找合适的特征来划分样本，使得划分后的结点之间具有最大的区分度。对内部结点重复上述操作，便可得到多层结点，直至达到各个叶结点（终止结点）。 9.3.1 分类树 分类树考虑响应变量为分类变量的情形，以叶结点处数量最多的类别作为叶结点的类别标签。首先考虑决策树是如何“生长”的，也就是根据什么准则来分裂结点。我们总是希望分裂后的结点中的样本尽可能的属于同一类别，即该结点有着较高的“纯度”。一言以蔽之，划分的过程就是从混乱走向有序的过程。 信息增益 考虑在结点\\(D\\)中的所有样本共属于\\(M\\)个类别，考虑信息熵\\(Ent(D)\\) \\[ \\textrm{Ent}(D)=-\\sum_{m=1}^Mp_m \\log_2p_m \\tag{9.1} \\] 如何理解信息熵？可以这样简单理解：对于不确定的、离谱的事情，人们就会拿不准，从而产生疑惑，想要知道更多的信息，此时信息熵就大；而对于确定的、理所应当的事情，人们就会很有把握，不会多问，此时信息熵就小。举个例子，一枚正常硬币猜正反，无论猜正面朝上还是猜反面朝上，概率都是0.5，因此只能瞎猜，没有什么把握。但对于一枚有特殊倾向的硬币，即正面朝上概率为0.9，那么大家自然都会猜正面朝上，并且有很大的把握。因此，信息熵是对不确定性的度量，信息熵越大，越不确定。 由此可得信息增益的定义： \\[ \\textrm{Gain}(D,x)=\\textrm{Ent}(D)-\\sum_{v=1}^V\\frac{n_v}{n}\\textrm{Ent}(D^v) \\tag{9.2} \\] 其中\\(x\\)表示某个特征，\\(V\\)表示该特征有多少个取值，\\(n\\)和\\(n_v\\)分别表示原结点与各个分支结点的样本量。 若\\(x\\)是连续变量，则可将其离散化，并且连续变量可以在后续划分中进一步细分，而离散变量只能使用一次。 我们要寻求合适的特征使得划分后的子结点与原结点相比信息熵平均下降得最多。这就是信息增益的准则。 代表算法：ID3决策树 增益率 信息增益准则对取值数目较多的特征有所偏好。由此引入增益率，其定义为 \\[ \\textrm{Gain_ratio}(D,x)=\\frac{\\textrm{Gain}(D,x)}{\\textrm{IV(x)}} \\tag{9.3} \\] 其中 \\[ \\textrm{IV}(x)=-\\sum_{v=1}^V\\frac{n_v}{n}\\textrm{log}_2\\frac{n_v}{n} \\tag{9.4} \\] 事实上，\\(\\textrm{IV}(x)\\)就是信息熵的形式，可以视作某种对\\(x\\)取值数量的惩罚。 代表算法：C4.5决策树（先选信息增益高的，再选增益率高的） 基尼系数 定义基尼系数 \\[ \\textrm{Gini}(D)=\\sum_{m=1}^Mp_m(1-p_m)=1-\\sum_{m=1}^Mp_m^2 \\tag{9.5} \\] 直观来看，基尼系数衡量的就是从结点\\(D\\)中随机抽取两个样本，其类别不一致的概率。因此基尼系数越小，该结点纯度越高。 代表算法：CART决策树 正如园艺里需要对植物进行修剪一样，决策树也能进行剪枝。决策树的剪枝策略分为预剪枝和后剪枝。 预剪枝 在决策树的生成过程中，对每个结点在分裂前进行估计，若对该结点划分不能提高泛化能力，则停止划分并将该结点设置为叶结点。 预剪枝降低了过拟合的风险，但存在欠拟合的可能。 后剪枝 先完整地生成决策树，然后自下而上地对所有非叶结点进行考察。若将该非叶结点替换为叶结点后能够提升泛化性能，则进行替换。 后剪枝消耗的时间比预剪枝长，但欠拟合的风险较小，其泛化性能往往优于预剪枝策略。 9.3.2 回归树 回归树考虑响应变量为连续变量的情形，以叶结点的响应变量平均值作为该叶结点的预测值。 回归树划分结点的准则多种多样，但都是类似的，如最小化平方误差MSE、最小化均方根误差RMSE、最小化平均绝对误差MAE等。 同样，回归树也能进行剪枝操作。 预剪枝 回归树的预剪枝策略可以设置一个样本量阈值，当结点分裂后的样本量小于该阈值，则不进行分裂。 常见规则： 当前节点的样本数小于阈值； 当前节点的纯度高于阈值； 划分后信息增益（或增益率）小于阈值； 使用验证集判断：若划分后准确率没有提高，则停止划分。 后剪枝 后剪枝可以采取代价复杂性剪枝的策略，即先生成一棵完整的树，然后考虑如下的惩罚残差平方和函数 \\[ \\textrm{SSE}+\\lambda|T| \\tag{9.6} \\] 步骤： 从最完整的树\\(T_0\\)开始； 逐步增加\\(lambda\\)，找到每次最优剪枝节点； 生成一系列子树序列\\(T_0, T_1, ...\\) 在验证集或交叉验证下选择最优\\(\\lambda\\)对应的树 其中\\(|T|\\)表示该棵决策树叶结点的数量。\\(\\lambda\\)的值可通过交叉验证的方法进行确定。 9.3.3 实现 rpart包中的rpart()函数可以用来构建回归树和分类树。 树的形式为二叉树 rpart()参数 formula 模型公式，看看谁是响应变量谁是预测变量。 data 数据框。 weights 设置权重。 subset 指示数据框中的哪些样本会被用于建模。 na.action 如何对待缺失值。默认使用na.rpart()函数，即删除缺失响应变量的样本或者缺失所有预测变量的样本（缺失部分预测变量也会被保留）。 method 可选值为anova、poisson、class、exp。其中anova对应构建回归树，class对应构建分类树。 model 是否在结果中保存模型框架。 感觉用不上 x 是否在结果中保存预测变量，默认为FALSE。 y 是否在结果中保存响应变量，默认为TRUE。 parms 添加到分裂函数中的额外参数。对于回归树而言，不用额外添加。对于分类树，可传入一个列表，列表中分为三个元素：prior、loss、split，分别表示先验概率（正数，且总和需为1）、损失矩阵（规定了错分时的损失，要求对角线元素为0，非对角线元素为正）、划分标准（可选值为基尼系数gini、信息增益information）。 control 为rpart.control()函数以列表形式传入的参数。 详情建议问ai，懒癌犯了0.0 cost 一个向量长度为预测变量数的非负向量，在拆分预测变量时用作缩放比例，默认为1。若该值越大，则对应预测变量的重要性程度越低。在量纲不一致的情形，该参数可用于减轻量纲带来的影响，因为算法会倾向于选择尺度较大的预测变量。 其余函数 rpart.control() 可更为细致地控制决策树的生长逻辑，如设置结点的最小训练样本数。 prune() 用于剪枝。 predict() 用于预测。 rpart.plot包 用于绘制决策树的结果，是对plot.rpart()函数的拓展。 "],["ml_4.html", "9.4 随机森林", " 9.4 随机森林 在介绍随机森林前，先介绍装袋法Bagging。 Bagging基于自助采样法bootstrap来抽取多个具有相同样本量的训练集，然后在各个训练集上对基学习器进行训练，最终将这些基学习器结合起来。而对于那些没有被纳入到训练集中的样本，可以作为测试集来计算测试误差，称为袋外误差OOB-error。 结合策略可以是投票法、简单平均法、加权平均法等策略。 随机森林，顾名思义，基于Bagging的方法构建多棵决策树形成森林，其“随机”不仅体现在训练集的随机，还体现在每棵决策树的初始特征也是随机选取的。这使得随机森林相较决策树有更加优秀的泛化能力。 9.4.1 实现 randomForest包是R中专门用来构建随机森林模型的包。下面将详细介绍包中的核心函数randomForest()，并罗列其余函数的作用。 用途 该函数使用Breiman的随机森林算法进行回归与分类任务。 参数 x 存储预测变量的数据框或矩阵。 y 响应变量向量。若为因子型变量，则视为分类树，否则视为回归树。若为省略，则为无监督模式。 xtest 预测变量的测试集，为数据框或矩阵格式。 ytest 响应变量的测试集，向量格式。 ntree 决策树的数目，默认为500。 mtry 随机属性子集的大小。分类任务为属性总量的平方根，回归任务为属性总量三分之一。 weights 权重向量，在采样时为训练集中的不同观测点设置权重。 replace 是否为有放回抽样，默认为TRUE。 classwt 在分类任务中，设置类的先验概率。注意传入的是一个向量，向量的分量表示不同类别的比例，这些分量无须加总为1。 cutoff 在分类任务中，设置投票法的阈值，即超过多少比例才认为该观测点属于特定的一类。 strata 一个被用于分层抽样的因子型变量。 sampsize 样本容量。在分类任务中，若其为与strata相同长度的向量，则表示不同层的样本容量。 nodesize 表示叶结点位置的最小样本数，默认分类任务为1，回归任务为5。 maxnodes 表示叶结点的最大个数，若不指定，则决策树将会尽可能地生长。 importance 是否评估预测变量的重要性，默认为FALSE。 localImp 是否需要计算重要性度量，默认为FALSE。若为TRUE，则会覆盖掉importance参数。 该参数和importance参数的区别貌似在于前者用于局部重要性度量，后者用于全局重要性度量。 nPerm 在回归任务中，该参数用于评估变量重要性时对每棵树的袋外数据进行排列的次数。 proximity 是否计算观测点之间的相似度。 oob.prox 是否计算袋外数据观测点之间的相似度。 norm.votes 在分类任务中，若为TRUE，则以比例形式展示最终的投票结果；若为FALSE，则展示原始票数。默认为TRUE。 do.trace 是否在控制台输出详细的运行过程，默认为FALSE。若为整数，则表示每构建多少棵树就输出一次详情。 keep.forest 是否在输出结果中保留森林。若给定了xtest，则默认为FALSE。 corr.bias 在回归任务中，是否对回归结果进行偏差校正。 该参数是实验性的，风险自担。 keep.inbag 是否返回\\(n \\times ntree\\)矩阵用以记录哪些观测点在哪棵树中被使用。 输出 call 模型的输入信息。 type 树的类别，回归任务还是分类任务还是无监督模式。 predicted 基于袋外样本的预测值。 importance 重要性度量矩阵，返回所有变量的平均下降精度、平均下降基尼系数或者平均下降MSE。 importanceSD 重要性度量的标准误矩阵。 localImp 局部重要性度量矩阵，返回变量对观测点重要性的度量。 ntree 决策树的数量。 mtry 每个结点上随机属性子集的大小。 forest 包含整个森林的列表。当randomForest()处于无监督模式或者keep.forest为FALSE时为NULL值。 err.rate 在分类任务中，第i棵树及之前所有树在袋外数据中的分类错误率。 confusion 在分类任务中，分类结果的混淆矩阵。 votes 在分类任务中，显示得票比例或者得票数。 oob.times 观测点归为袋外数据的次数。 proximity 接近度矩阵，根据观测点在同一结点出现的频率来计算观测点之间的相似性。 mse 回归任务中的均方误差。 rsq 伪R方，\\(1-\\frac{mse}{Var(y)}\\) test 若在输入中给出了测试集的数据，则在结果中会以列表形式存储关于测试集的有关结果。 randomForest包中的其余函数的作用如下表所示。 函数名 用途 classCenter 返回不同类别的原型 combine 将多个森林合并为一个大森林 getTree 从森林中提取一棵决策树 grow 为森林新添决策树 importance 提取变量重要性度量 imports85 一个UCI机器学习数据集 margin 计算或绘制分类边界 MDSplot 绘制接近度矩阵的多维尺度图 na.roughfix 利用中位数或众数来估算缺失值 outlier 根据接近度矩阵计算离群点 partialPlot 绘制偏依赖图 plot.randomForest 绘制随机森林的分类错误率或者MSE predict.randomForest 用测试集进行预测 treecv 利用交叉验证法进行特征选择 treeImpute 利用接近度矩阵来估算自变量中的缺失值 treeNews 查看randomForest包的更新文件 treesize 查看每棵树的叶结点数或总结点数 tunetree 调优以寻找mtry的最优参数值 varImpPlot 可视化变量的重要性度量 varUsed 查看随机森林实际用到了哪些自变量 下面生成随机数据供随机森林进行模拟。 library(tidyverse) library(randomForest) library(MASS) # 用到多元正态随机数 首先自定义函数，用于生成模拟数据。 # 自定义函数——生成多元正态数据 gen_data &lt;- function(level, size, mu, sigma) { # 初始化数据框 df = data.frame() # 生成每个类别的数据 for (i in 1:level) { # 生成类别标签 category_label = rep(LETTERS[i], size[i]) # 生成数据 category_data = as.data.frame(mvrnorm(n = size[i], mu = mu[[i]], Sigma = sigma[[i]])) # 添加类别标签 category_data$category = factor(category_label) # 将数据添加到数据框 df = rbind(df, category_data) } # 返回数据框 return(df) } # 自定义函数——生成协方差矩阵 gen_sigma &lt;- function(n) { # 生成一个n x n的随机矩阵 L = matrix(runif(n * n, min = 1, max = 2), ncol = n) diag(L) = abs(diag(L)) # 确保对角线上的元素为正 # 填充上三角部分为0 L[upper.tri(L)] = 0 # 计算Cholesky分解 A = L %*% t(L) return(A) } 分类任务 这里生成具有3个水平的响应变量和9个预测变量。注意到不同类别之间的样本量存在一定的差异。 set.seed(123) mu &lt;- list(runif(9, min = 1, max = 4), runif(9, min = 1, max = 4), runif(9, min = 1, max = 4)) sigma &lt;- list(gen_sigma(9), gen_sigma(9), gen_sigma(9)) df_train &lt;- gen_data(level = 3, size = c(100, 300, 600), mu = mu, sigma = sigma) df_test &lt;- gen_data(level = 3, size = c(30, 100, 180), mu = mu, sigma = sigma) 接着运行模型。 tree_class &lt;- randomForest(x = df_train[, -10], y = df_train$category, importance = T) print(tree_class) ## ## Call: ## randomForest(x = df_train[, -10], y = df_train$category, importance = T) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 3 ## ## OOB estimate of error rate: 16.2% ## Confusion matrix: ## A B C class.error ## A 34 35 31 0.66000000 ## B 3 221 76 0.26333333 ## C 0 17 583 0.02833333 可以看到，袋外数据的分类错误率为16.2%。其中A类的分类错误率高达66%，这可能和训练集中的类别比例有关。 进一步地，绘制出分类错误率随决策树增加的变化趋势，可以更清晰地看到分类错误率的收敛情况。 plot(tree_class, col=c(&#39;black&#39;,&#39;red&#39;,&#39;blue&#39;,&#39;brown&#39;), lty=1, lwd=2, main=&#39;OOB err.rate&#39;) legend(&#39;topright&#39;, legend=colnames(tree_class$err.rate), col=c(&#39;black&#39;,&#39;red&#39;,&#39;blue&#39;,&#39;brown&#39;), lty=1, lwd=2) 图 9.1: 袋外数据的分类错误率 再来看看不同特征的重要程度。对于一棵树，在随机打乱某个特征的值的顺序之后，可以作差得到前后预测精度的下降情况，对于所有树取平均即可得到平均下降精度(MDA)。显然，如果MDA越大，说明该特征就越重要。同理，平均下降基尼系数(MDI)通过计算每个特征在所有树上节点分裂时导致的基尼系数平均下降量来评估特征的重要性。基尼系数反映了不纯度，下降得越多说明结点越容易从“不纯”走向了“纯”，意味着该特征在区分不同类别时能够较为显著地发挥作用。由图可知，两种评价准则得到的结果较为一致。 varImpPlot(tree_class) 图 9.2: 分类_重要性度量 下面关注如何缓解类不平衡问题及如何选取最优参数mtry。 注意到训练集中类别的比例为1:3:6，存在一定程度的类不平衡问题。对此，在运行随机森林模型时可以设置classwt参数来设定各个类别的先验概率。 tree_class_prior &lt;- randomForest(x = df_train[, -10], y = df_train$category, importance = T, proximity = T, classwt = c(1, 3, 6)) print(tree_class_prior) ## ## Call: ## randomForest(x = df_train[, -10], y = df_train$category, classwt = c(1, 3, 6), importance = T, proximity = T) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 3 ## ## OOB estimate of error rate: 15.8% ## Confusion matrix: ## A B C class.error ## A 32 37 31 0.68 ## B 5 228 67 0.24 ## C 0 18 582 0.03 而对于参数mtry的选取，除了可以使用randomForest包自带的tunetree()函数进行调参，还可以自己写个循环，直接根据测试集来选取最优参数。 err.rate &lt;- c(1:9) for (i in 1:9) { tree = randomForest(x = df_train[, -10], y = df_train$category, mtry = i) fit_test = predict(tree, df_test[, -10]) err.rate[i] &lt;- sum(fit_test != df_test$category)/nrow(df_test) } plot(1:9, err.rate, type = &quot;b&quot;, lwd = 2, xlab = &quot;mtry&quot;, ylab = &quot;test err.rate&quot;) 图 9.3: 分类_mtry调优 由图可知，当mtry=3时，在测试集上的袋外数据分类错误率达到最小，为14.2%。 回归任务 这里首先生成自变量数据，然后在自变量的线性组合的基础上添加噪声，得到因变量数据。 set.seed(111) mu &lt;- list(runif(9, min = 1, max = 4)) sigma &lt;- list(gen_sigma(9)) df_train &lt;- gen_data(level = 1, size = c(1000), mu = mu, sigma = sigma) df_train$epsilon &lt;- rnorm(1000, sd = 3) df_train &lt;- df_train %&gt;% mutate(y = 2 * V1 + 3 * V2 + V3 + 4 * V4 + 3 * V5 + V6 + 2 * V7 + 3 * V8 + 4 * V9 + epsilon) df_test &lt;- gen_data(level = 1, size = c(100), mu = mu, sigma = sigma) df_test$epsilon &lt;- rnorm(100, sd = 3) df_test &lt;- df_test %&gt;% mutate(y = 2 * V1 + 3 * V2 + V3 + 4 * V4 + 3 * V5 + V6 + 2 * V7 + 3 * V8 + 4 * V9 + epsilon) 接着直接运行模型。 tree_reg &lt;- randomForest(x = df_train[, 1:9], y = df_train$y, importance = T) print(tree_reg) ## ## Call: ## randomForest(x = df_train[, 1:9], y = df_train$y, importance = T) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 3 ## ## Mean of squared residuals: 46.75519 ## % Var explained: 98.91 从结果中可以看到，均方误差为46.7551872，自变量能解释的变异程度为98.91%。 进一步地，下面给出了模型的袋外数据MSE。 plot(tree_reg, main=&#39;OOB mse&#39;, lwd=2) 图 9.4: 袋外数据MSE 再来看看特征的重要性度量。其中%IncMSE表示重排某个特征前后袋外数据MSE的上升百分比，上升的幅度越大，说明该特征对模型更加重要。IncNodePurity则表示结点分列时残差平方和的下降情况。 varImpPlot(tree_reg) 图 9.5: 回归_重要性度量 最后，给模型参数mtry调调优。 mse &lt;- c(1:9) for (i in 1:9) { tree = randomForest(x = df_train[, 1:9], y = df_train$y, mtry = i) fit_test = predict(tree, df_test[, 1:9]) mse[i] &lt;- sum((fit_test - df_test$y)^2)/nrow(df_test) } plot(1:9, mse, type = &quot;b&quot;, lwd = 2, xlab = &quot;mtry&quot;, ylab = &quot;test MSE&quot;) 图 9.6: 回归_mtry调优 由图可知，当mtry=2时，在测试集上的袋外数据MSE达到最小，为31.9。 "],["ml_5.html", "9.5 XGBoost", " 9.5 XGBoost 文献：XGBoost: A Scalable Tree Boosting System 官方文档 9.5.1 原理 基础思想 采用boosting的思想，串行训练多个弱学习器，当前弱学习器从上一个弱学习器的残差中进行学习，最后加权综合各个弱学习器，即\\(\\hat y_i=\\phi(x_i)=\\sum_{k=1}^Kf_k(x_i)\\)。 目标函数 \\[ L(\\phi) = \\sum_{i=1}^n L(y_i, \\hat y_i) + \\sum_{k=1}^K \\Omega(f_k) \\] \\(L(\\cdot)\\)表示损失函数，用于度量\\(y_i\\)和\\(\\hat y_i\\)之间的差异，回归任务可以为均方误差，分类任务可以为交叉熵。 \\(\\Omega(f_k)\\)表示对第k棵树复杂度的惩罚，用于防止过拟合，定义为\\(\\Omega(f) = \\gamma T+\\frac{1}{2}\\lambda ||w||^2\\)，其中\\(T\\)为叶子节点数，\\(w\\)为叶子权重（即对应叶子节点的输出值），\\(\\gamma\\)和\\(\\lambda\\)为惩罚系数（超参数）。 计算损失函数 与梯度提升树只利用梯度信息不一样，XGBoost还利用二阶泰勒展开（利用了黑塞矩阵的信息）来更为精准地近似损失函数。 当模型训练到第t棵树时，我们需要最小化下面的损失函数 \\[ \\begin{aligned} L^{(t)}&amp;=\\sum_{i=1}^n L(y_i, \\hat y_i^{(t)})+\\Omega (f_t) \\\\ &amp;=\\sum_{i=1}^n L(y_i, \\hat y_i^{(t-1)}+f_t(x_i))+\\Omega (f_t) \\\\ &amp;\\approx \\sum_{i=1}^n [L(y_i, \\hat y_i^{(t-1)})+g_if_t(x_i)+\\frac{1}{2}h_if_t^2(x_i)]+\\Omega (f_t) \\\\ &amp; \\propto \\sum_{i=1}^n [g_if_t(x_i)+\\frac{1}{2}h_if_t^2(x_i)]+\\Omega (f_t) \\end{aligned} \\] 由于前t-1棵树的结构已经确定，因此\\(\\sum_{i=1}^{t-1}\\Omega (f_i)\\)也随之确定，即为常数 \\(g_i\\)和\\(h_i\\)分别表示损失函数的一阶导和二阶导 \\(L(y_i, \\hat y_i^{(t-1)})\\)为常数 记\\(I_j=\\{i|q(x_i)=j\\}\\)，表示落在叶子节点\\(j\\)上的观测集合，其中\\(q(\\cdot)\\)表示从样本到叶子节点的映射，即为树的结构。则有 \\[ \\begin{aligned} \\tilde L^{(t)} &amp;= \\sum_{i=1}^n [g_if_t(x_i)+\\frac{1}{2}h_if_t^2(x_i)]+\\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^T w_j^2 \\\\ &amp;= \\sum_{j=1}^T[(\\sum_{i \\in I_j}g_i)w_j + \\frac{1}{2}(\\sum_{i \\in I_j}h_i + \\lambda)w_j^2] + \\gamma T \\end{aligned} \\] 引入\\(I_j\\)的目的就是为了把\\(f_t(x_i)\\)转化为对应叶子节点的预测值 之后便可将\\(\\tilde L^{(t)}\\)视作关于\\(w_j\\)的二次函数，故最优权重\\(w_j^*\\)为 \\[ w_j^* = -\\frac{\\sum_{i \\in I_j}g_i}{\\sum_{i \\in I_j}h_i + \\lambda} \\] 对应的目标函数最小值为 \\[ \\tilde L^{(t)}(q)=-\\frac{1}{2}\\sum_{j=1}^T \\frac{(\\sum_{i \\in I_j}g_i)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T \\] 因此，可根据\\(\\tilde L^{(t)}(q)\\)来判断当前树模型的好坏，值越小，结构越好。 树的分裂 我们没办法遍历各种树的结构，因此采用贪心算法进行分裂，记\\(I_L\\)和\\(I_R\\)分别为分裂后左右叶子节点的样本集合，则损失函数减少量为 \\[ L_{split} = \\frac{1}{2}[\\frac{(\\sum_{i \\in I_L}g_i)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{(\\sum_{i \\in I_R}g_i)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{(\\sum_{i \\in I_j}g_i)^2}{\\sum_{i \\in I_j} h_i + \\lambda}] - \\gamma \\] 注意符号，\\(L_{split}\\)是分裂前的损失函数值减去分裂后的损失函数值，\\(L_{split}\\)越大，越倾向于分裂 此处的\\(\\gamma\\)同样用于惩罚，能够使得差值达到一定程度时才选择分裂。 收缩与列采样 除了在目标函数中增加正则项以防止过拟合，还采取收缩与列采样的技术防止过拟合。 引入收缩因子\\(\\eta\\)用于减少每棵树的影响，即\\(\\eta f_t(x_i)\\)，从而为后续的树留有改善空间。这里的收缩因子类似学习率。 列采样就是在训练每棵树时随机从所有特征中抽取一个子集用于训练。 缺失值处理 当某个特征中存在缺失值时，首先删掉所有缺失值对应的观测，将完整的观测按正常操作进行分裂。之后，比较把所有缺失值样本放到左子节点及右子节点的增益大小，将这些缺失值对应的观测分配到能获得更大增益的子节点，并记录分配节点作为默认方向。在测试集上的缺失值则分配到默认方向。 9.5.2 实现 超参数详见官方手册 调参技巧： 控制模型复杂度（最优先） 模型复杂度过高容易过拟合；过低则欠拟合。 重点调整以下三个参数： max_depth：树的最大深度 -默认值：6 建议范围：3 ~ 10 越大模型越复杂；过拟合时应减小 min_child_weight：每个叶子节点的最小样本权重和（这里的权重指的是二阶梯度） 默认值：1 建议范围：1 ~ 10 越大模型越保守，可防止过拟合 gamma (min_split_loss)：节点分裂所需的最小损失减少量 默认值：0 建议范围：0 ~ 5 增大 gamma 可减少分裂、抑制过拟合 学习率与迭代次数 学习率决定每棵树对最终模型的贡献；迭代次数决定累积学习量，两者必须配合使用。 eta (learning_rate)：学习率 默认值：0.3 建议范围：0.01 ~ 0.2 小 eta + 大 n_estimators = 稳定、效果好，但训练时间长 n_estimators：迭代次数（树的数量） 建议范围：500 ~ 5000 通常配合 early_stopping_rounds 使用 子采样与特征采样（防止过拟合） 通过随机采样样本或特征来增加模型的泛化能力。 subsample：每棵树训练使用的样本比例 默认值：1 建议范围：0.6 ~ 0.9 值太低可能欠拟合，太高容易过拟合 colsample_bytree：每棵树随机选择的特征比例 默认值：1 建议范围：0.6 ~ 0.9 正则化参数（进一步防过拟合） 控制叶子权重的惩罚力度，提升模型稳定性。 lambda (reg_lambda)：L2 正则项（权重平方惩罚） 默认值：1 建议范围：1 ~ 10 增大能提升模型泛化性 alpha (reg_alpha)：L1 正则项（稀疏化） 默认值：0 建议范围：0 ~ 10 增大能进行特征筛选（部分特征权重归零） python的xgboost库，示例如下。 import optuna def obj_fun(trial): params = { &#39;n_estimators&#39;:trial.suggest_int(&#39;n_estimators&#39;, 100, 300), &#39;max_depth&#39;:trial.suggest_int(&#39;max_depth&#39;, 3, 10), &#39;learning_rate&#39;:trial.suggest_float(&#39;learning_rate&#39;, 0.01, 0.05, log = True), &#39;subsample&#39;: trial.suggest_float(&#39;subsample&#39;, 0.5, 1.0), &#39;colsample_bytree&#39;: trial.suggest_float(&#39;colsample_bytree&#39;, 0.5, 1.0), &#39;eval_metric&#39;: &#39;auc&#39;, &#39;random_state&#39;: 42 } # from xgboost.callback import EarlyStopping # early_stop_callback = EarlyStopping(rounds=50, metric_name=&#39;auc&#39;, save_best=True) # model = XGBClassifier(**params, callbacks=[early_stop_callback]) model = XGBClassifier(**params, early_stopping_rounds=50) model.fit( X_train, y_train, eval_set = [(X_val, y_val)], verbose = False ) y_pred_prob = model.predict_proba(X_val)[:,1] auc = roc_auc_score(y_val, y_pred_prob) return auc study = optuna.create_study( direction=&quot;maximize&quot;, # 我们希望最大化 AUC study_name=&quot;xgb_optuna_tuning&quot; ) study.optimize(obj_fun, n_trials=50, n_jobs=-1) print(&quot;最优参数：&quot;, study.best_params) print(&quot;最优 AUC：&quot;, study.best_value) best_params = study.best_params best_model = XGBClassifier( **best_params, eval_metric=&#39;auc&#39;, early_stopping_rounds=50, random_state=42 ) best_model.fit(X_trainval, y_trainval, verbose=True) y_pred_prob = best_model.predict_proba(X_test)[:, 1] auc = roc_auc_score(y_test, y_pred_prob) print(&quot;Test AUC:&quot;, auc) "],["ml_6.html", "9.6 LightGBM", " 9.6 LightGBM 文献：LightGBM: A Highly Efficient Gradient Boosting Decision Tree 官方文档 LightGBM的核心目标是通过减少数据量和特征维度来加速训练，同时保持模型精度。其创新点主要体现在GOSS（梯度单边采样）和EFB（互斥特征捆绑）两项技术上。 LightGBM相较于XGBoost，更适合在大数据或高维特征场合使用。 9.6.1 原理 Gradient-based One-Side Sampling 传统GBDT需扫描所有数据计算信息增益，计算成本高。而GOSS保留了梯度大的样本，并随机采样梯度小的样本，通过权重补偿修正数据分布偏差。如此，大梯度样本就能够主导信息增益计算，同时这种加权修正也能够近似原始分布。 Exclusive Feature Bundling 在高维特征场合，存在“特征互斥”的现象，即某些特征永远不会同时非零（如独热编码）。鉴于此，将这些互斥的特征捆绑为单一特征，减少特征数量。 生长策略 LightGBM采用Leaf-wise的树生长策略，每次选择损失下降最大的叶子节点分裂，深度优先。因此，LightGBM能够更快降低损失，生成更复杂的不对称树。 XGBoost采取Level-wise的树生长策略，逐层分裂树，每层分裂所有叶子节点，广度优先。 直方图算法 LightGBM对连续特征离散化为直方图，降低计算复杂度。 XGBoost既支持预排序特征值，又支持直方图算法 9.6.2 实现 调参技巧： 控制树模型复杂度 num_leaves：叶子节点数量（最重要的参数） 默认值：31 建议范围：31 ~ 255 越大模型越复杂，过拟合风险上升 通常应满足：num_leaves &lt;= 2^(max_depth) max_depth：最大树深度 默认值：-1（不限制） 建议范围：3 ~ 10 限制树深度可显著降低过拟合 min_data_in_leaf：叶子节点最小样本数 默认值：20 建议范围：20 ~ 100 增大能平滑模型、提升泛化能力 min_sum_hessian_in_leaf：叶子节点最小Hessian和 默认值：1e-3 建议范围：1e-3 ~ 1e-1 数据量大时可适度增大，防止过拟合 学习率与迭代次数（核心控制） learning_rate 默认值：0.1 建议范围：0.01 ~ 0.1 越小训练越慢但泛化更好 n_estimators 默认值：100 建议范围：500 ~ 5000 通常与 learning_rate 组合调节 建议开启早停：early_stopping_rounds = 50~200 防止过拟合的随机采样 feature_fraction（列采样率） 默认值：1.0 建议范围：0.6 ~ 0.9 每棵树随机使用部分特征 bagging_fraction（样本采样率） 默认值：1.0 建议范围：0.6 ~ 0.9 每次建树使用部分样本 bagging_freq 默认值：0（禁用） 建议值：5 表示每 5 次迭代重新随机采样一次 正则化参数（控制权重大小） lambda_l1：L1 正则化（稀疏化） 默认值：0 建议范围：0 ~ 10 lambda_l2：L2 正则化（平滑化） 默认值：0 建议范围：0 ~ 10 min_gain_to_split：节点分裂所需的最小增益 默认值：0 建议范围：0 ~ 0.2 限制无效分裂，提高泛化能力 import numpy as np import pandas as pd import lightgbm as lgb from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt import logging # 关闭 LightGBM 日志输出 logging.getLogger(&quot;lightgbm&quot;).setLevel(logging.ERROR) # ============================================== # 1️⃣ 生成模拟数据 # ============================================== np.random.seed(42) n_samples = 2000 n_features = 10 # 生成特征矩阵 X = np.random.randn(n_samples, n_features) # 构造非线性目标变量 y = ( 5 * np.sin(X[:, 0]) + 3 * X[:, 1] ** 2 + 2 * X[:, 2] + np.random.normal(0, 0.5, n_samples) # 添加噪声 ) # 转换为DataFrame，带列名 feature_names = [f&quot;f{i}&quot; for i in range(n_features)] X = pd.DataFrame(X, columns=feature_names) y = pd.Series(y, name=&quot;target&quot;) # ============================================== # 2️⃣ 划分训练 / 验证 / 测试集 # ============================================== X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42) X_train, X_valid, y_train, y_valid = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42) # → 训练:60%，验证:20%，测试:20% print(f&quot;训练集: {X_train.shape}, 验证集: {X_valid.shape}, 测试集: {X_test.shape}&quot;) # ============================================== # 3️⃣ 构建 LightGBM 模型（sklearn 风格） # ============================================== model = lgb.LGBMRegressor( objective=&quot;regression&quot;, metric=&quot;rmse&quot;, learning_rate=0.05, n_estimators=2000, num_leaves=63, max_depth=6, min_child_samples=30, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=5, lambda_l2=1.0, verbose=-1 # 静默模式 ) # ============================================== # 4️⃣ 模型训练（含 early stopping） # ============================================== model.fit( X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric=&quot;rmse&quot;, callbacks=[ lgb.early_stopping(stopping_rounds=100, verbose=False), lgb.log_evaluation(period=0) ] ) # ============================================== # 5️⃣ 模型评估 # ============================================== y_pred = model.predict(X_test) rmse = np.sqrt(mean_squared_error(y_test, y_pred)) print(f&quot;✅ 测试集 RMSE: {rmse:.4f}&quot;) # ============================================== # 6️⃣ 可视化预测结果 # ============================================== plt.figure(figsize=(8, 6)) plt.scatter(y_test, y_pred, alpha=0.6) plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], &quot;r--&quot;) plt.xlabel(&quot;True Values&quot;) plt.ylabel(&quot;Predicted Values&quot;) plt.title(&quot;LightGBM (sklearn-style, DataFrame) - Prediction vs True&quot;) plt.grid(True) plt.show() # ============================================== # 7️⃣ 特征重要性（可选） # ============================================== plt.figure(figsize=(8, 5)) lgb.plot_importance(model, max_num_features=10) plt.title(&quot;Feature Importance (LightGBM)&quot;) plt.tight_layout() plt.show() "],["ml_7.html", "9.7 因果森林", " 9.7 因果森林 9.7.1 原理 无混杂条件 在无混杂条件下，方可进行准确的因果推断。 对于\\(\\{X_i, Y_i, W_i\\}\\)的数据，其中\\(W_i\\)表示是否接受处理，个体处理效应表示为 \\[ \\tau(x) = E[Y_i^{(1)}-Y_i^{(0)}|X_i = x] \\] 传统因果推断考虑平均处理效应ATE，即处理组与对照组平均结果的差 由于在现实中没法同时观测到\\(Y_i^{(1)}\\)和\\(Y_i^{(0)}\\)，在无混杂条件 \\[ \\{Y_i^{(0)}, Y_i^{(1)}\\} \\perp W_i | X_i \\] 无混杂条件说明了在控制\\(X\\)的情况下，对象是否接受干预是随机的，正是因为这种随机性使得我们可以根据现实观测来估计处理效应 成立的情况下，传统方法可以通过倾向得分\\(e(x) = E(W_i|X_i=x)\\)来估计处理效应 \\[ \\begin{gather} \\tilde Y = Y_i(\\frac{W_i}{e(x)} - \\frac{1-W_i}{1-e(x)}) \\\\ E[\\tilde Y|X_i = x]=\\tau(x) \\end{gather} \\] 倾向得分可以通过logistic回归得到 倾向得分可以用于匹配对照组和实验组的样本（PSM），也可用于加权从而修正样本选择偏差 诚实树与因果森林 在高维场合下，树和森林可被视作具有自适应距离度量的NearestNeighbor方法。落在相同叶节点的观测具有高度相似性，当叶节点足够小时可以相信这些观测独立同分布。 KNN算法是基于某种距离度量找到邻居，而树中叶节点里的观测都是邻居 对此，根据树模型得到的局部处理效应估计为 \\[ \\begin{aligned} \\hat \\tau(x) &amp;= \\frac{1}{|\\{i:W_i = 1, X_i \\in L\\}|}\\sum_{\\{i:W_i = 1, X_i \\in L\\}}Y_i \\\\ &amp;-\\frac{1}{|\\{i:W_i = 0, X_i \\in L\\}|}\\sum_{\\{i:W_i = 0, X_i \\in L\\}}{Y_i} \\end{aligned} \\] 范围局限在一个叶节点中，故为局部 若是森林，则综合B棵树的估计结果 \\[ \\hat \\tau(x) = B^{-1}\\sum_{b=1}^B \\hat \\tau_b(x) \\] 为了能够实现上述的估计，树模型也需要满足无混杂条件，因此需要引入诚实树的概念。对于任一观测，该观测要么用来分裂，要么用来估计因果效应。只有这样，对因果效应的估计才有一致性和渐近正态性。 对于诚实树，有两种构建方法——双重样本树和倾向树。 双重样本树，顾名思义，将样本分为两个独立子集，一个子集用于分裂（使用X，W和\\(\\tilde Y\\)），另一个子集用于在叶节点内估计处理效应（使用X，W，Y）。 双重样本树每次分裂的目标是最大化左右两个子节点的平均处理效应差异 倾向树，将指示变量W视为分类目标的响应变量，构建分类树，即用X去预测W（某种程度上就是在做倾向得分的事），然后再在生成的叶节点内基于Y估计处理效应。 9.7.2 实现 R语言grf包。 causal_forest()：构建因果森林 multi_arm_causal_forest()：适用于W为多分类水平的因果森林 average_treatment_effect()：输出平均处理效应，支持统计推断，可获得置信区间 predictions：输出每个观测的预测处理效应值 "],["ml_8.html", "9.8 SVM", " 9.8 SVM 关于支持向量机SVM的介绍参见视频。 e1071包的tune()函数能够对超参数进行网格搜索，并保留最优模型。 library(e1071) # 设置参数范围，进行网格搜索 tune_grid &lt;- list( cost = c(0.1, 0.5, 1, 5), gamma = c(0.01, 0.1, 1, 10), kernel = c(&#39;radial&#39;, &#39;linear&#39;) ) set.seed(123) # tune()函数能够自行调优 model_tune_svm &lt;- tune( METHOD = svm, train.x = label ~ score + departure, data = train_set, ranges = tune_grid, tunecontrol = tune.control( sampling = &quot;cross&quot;, # 交叉验证 cross = 5, # 5折交叉验证 best.model = TRUE # 保留最佳模型 ) ) summary(model_tune_svm) model_tune_svm$best.parameters # 最优参数组合 model_svm &lt;- model_tune_svm$best.model # 提取最优模型 fit_svm &lt;- predict(model_svm, train_set) # 测试集预测 "],["ml_9.html", "9.9 聚类分析", " 9.9 聚类分析 9.9.1 聚类结果评价 轮廓系数 轮廓系数综合考虑了样本在自身簇内的紧密程度和与其他簇的分离程度。 \\[ a(i) = \\frac{1}{|C_i| - 1} \\sum_{j \\in C_i, j \\neq i} d(i, j) \\] \\[ b(i) = \\min_{k \\neq i} \\frac{1}{|C_k|} \\sum_{j \\in C_k} d(i, j) \\] 其中： - \\(a(i)\\)：样本 \\(i\\) 到同簇中其他样本的平均距离； - \\(b(i)\\)：样本 \\(i\\) 到最近其他簇的平均距离。 轮廓系数定义为： \\[ s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))} \\] 整体平均轮廓系数： \\[ S = \\frac{1}{N} \\sum_{i=1}^{N} s(i) \\] 取值范围： \\([-1, 1]\\)，越接近 1 表示聚类效果越好。 CH指数 CH指数衡量类间方差与类内方差的比值，类似于方差分析（ANOVA）中的 F 统计量。 设共有\\(N\\)个样本、\\(K\\)个簇，第\\(k\\)个簇的样本数为\\(n_k\\)，簇中心为\\(c_k\\)，全局中心为\\(c\\)。 定义： \\[ B_k = n_k \\| c_k - c \\|^2 \\] \\[ W_k = \\sum_{x_i \\in C_k} \\| x_i - c_k \\|^2 \\] 则CH指数为： \\[ CH = \\frac{\\text{Tr}(B)}{\\text{Tr}(W)} \\times \\frac{N - K}{K - 1} \\] 其中： \\[ \\text{Tr}(B) = \\sum_{k=1}^{K} B_k, \\quad \\text{Tr}(W) = \\sum_{k=1}^{K} W_k \\] 取值方向： 越大越好，表示类间分离更明显、类内更加紧密。 Dunn指数 Dunn指数衡量聚类结果的“最差情况”，即最小簇间距离与最大簇内直径的比值。 定义： \\[ \\delta(C_i, C_j) = \\min_{x \\in C_i, y \\in C_j} d(x, y) \\] \\[ \\Delta(C_k) = \\max_{x, y \\in C_k} d(x, y) \\] Dunn 指数为： \\[ D = \\frac{\\min_{i \\neq j} \\delta(C_i, C_j)}{\\max_{k} \\Delta(C_k)} \\] 取值方向： 越大越好，表示簇间更远、簇内更紧密。 但对噪声较敏感。 9.9.2 Kmeans Kmeans是一种基于距离的划分式聚类算法。其目标是将样本划分为\\(K\\)个簇，使得同一簇内的样本尽可能相似，不同簇之间的样本尽可能不同。 算法流程： 随机选择\\(K\\)个样本作为初始中心 对每个样本\\(x_i\\)，计算其到各中心的距离： \\[ \\text{assign } x_i \\text{ to } C_k \\text{ if } \\|x_i - \\mu_k\\|^2 \\le \\|x_i - \\mu_j\\|^2, \\forall j \\] 对每个簇\\(C_k\\)更新中心 \\[ \\mu_k = \\frac{1}{|C_k|}\\sum_{x_i \\in C_k}x_i \\] 重复迭代步骤2、3，直至簇分配不再变化或达到最大迭代次数 对初始中心敏感，容易陷入局部最优 需要指定K 只能处理凸形簇 如何确定参数K？ 手肘法。计算不同K值下的簇内误差平方和 随着K增大，SSE会逐渐减小，但减小速度在某点后变缓。该“拐点”即为最佳K值。 \\[ \\text{SSE}(K)=\\sum_{k=1}^{K}\\sum_{x_i \\in C_k}\\|x_i-\\mu_k\\|^2 \\] 轮廓系数法 \\(a_i\\)表示样本与同簇内其他点的平均距离，\\(b_i\\)表示样本与最近邻簇中所有点的平均距离。轮廓系数\\(S\\)越大表示聚类效果越好。 \\[ \\begin{gather} s_i = \\frac{b_i - a_i}{\\max(a_i, b_i)} \\\\ S = \\frac{1}{n}\\sum_{i=1}^{n}s_i, \\quad S \\in [-1,1] \\end{gather} \\] 9.9.3 Kmeans++ Kmeans的核心问题在于——对初始簇中心的选择非常敏感： 若初始中心选得不好，算法可能陷入局部最优； 聚类结果依赖随机性，稳定性差； 可能导致“簇塌缩”（多个中心聚到同一区域）。 因此Kmeans++让初始中心尽可能远离彼此，从而覆盖数据空间的主要区域。 算法流程： 给定数据集\\(X = \\{x_1, x_2, \\ldots, x_n\\}\\)，以及目标簇数\\(K\\)，随机选择第一个中心： \\[ \\mu_1 = x_i, \\quad x_i \\text{ 从 } X \\text{ 中随机选取} \\] 计算每个点到已有最近中心的距离平方，当前已有中心集合为\\(\\{\\mu_1, \\ldots, \\mu_m\\}\\)。 \\[ D(x_i)^2 = \\min_{1 \\le j \\le m} \\|x_i - \\mu_j\\|^2 \\] 按距离加权概率选择下一个中心，距离当前中心越远的点被选为新中心的概率越大。 \\[ P(x_i) = \\frac{D(x_i)^2}{\\sum_j D(x_j)^2} \\] 重复步骤2、3，直到选出K个中心。 之后就开始正常的kmeans迭代。 9.9.4 DBSCAN DBSCAN是一种基于密度的聚类算法，能够在任意形状的数据分布中识别出高密度区域，并自动识别噪声点。 DBSCAN有两个参数：邻域半径\\(\\varepsilon\\)（定义多远的距离算作邻居）与最小样本数minPts（邻域内至少多少个点才算稠密）。 \\(\\varepsilon\\)-邻域定义为 \\[ N_\\varepsilon(x_i) = \\{x_j \\mid \\text{dist}(x_i, x_j) \\le \\varepsilon\\} \\] 点类型 条件 含义 核心点（Core Point） \\(|N_\\varepsilon(x_i)| \\ge \\text{minPts}\\) 位于高密度区域 边界点（Border Point） \\(|N_\\varepsilon(x_i)| &lt; \\text{minPts}\\)，但与核心点相邻 位于簇的边缘 噪声点（Noise Point） 不属于任何核心点的邻域 离群样本 直接密度可达： 若\\(x_j \\in N_\\varepsilon(x_i)\\)，且\\(x_i\\)是核心点，则\\(x_j\\)对\\(x_i\\)直接密度可达。 圆心的影响力 密度可达： 若存在一系列核心点\\(x_1, x_2, ..., x_n\\)，其中\\(x_{i+1}\\)对\\(x_i\\)直接密度可达，则\\(x_n\\)对\\(x_1\\)密度可达。 圆心相连 密度相连： 若存在核心点\\(x_k\\)，使得\\(x_i, x_j\\)都密度可达于\\(x_k\\)，则 \\(x_i\\)与\\(x_j\\)密度相连。 圆的并集内都是密度相连的 图 9.7: 密度 算法流程： 根据\\(\\varepsilon\\)和minPts找到样本中所有的核心点。 随机取一个核心点，在其邻域中找到所有密度可达点，在从这些密度可达点出发不断拓展，直至拓展后的邻域中再无密度可达点。将这些密度可达点及其邻域内的点归为一个簇。 从剩下的尚未归类的核心点中重新抽取一个核心点，重复步骤2. 当所有核心点已归类时，尚未归类的样本点记为噪声点。 DBSCAN如何调参： 绘制K-距离图： 对每个点计算到第minPts个最近邻的距离； 将这些距离排序并绘制曲线； 曲线的“拐点”即为较优的\\(\\varepsilon\\)值。 2️. 经验规则： minPts ≈ 2 × 数据维度； 二维数据常取 4~6，高维数据取 10~20。 9.9.5 HDBSCAN HDBSCAN是一种基于密度的层次聚类算法，是DBSCAN的改进版本。它通过构建多尺度密度层次结构自动识别不同密度的簇并剔除噪声点，从而克服了DBSCAN需要固定\\(\\varepsilon\\)参数、难以处理不同密度区域的缺陷。 传统DBSCAN存在两大局限： 需要手动设定 ε（邻域半径）：不同密度簇无法用同一\\(\\varepsilon\\)表示。 无法发现不同密度的簇：固定半径会导致稠密区域被过分细分，稀疏区域被忽略。 HDBSCAN 的核心改进思路是： 不固定\\(\\varepsilon\\)，而是从所有可能的\\(\\varepsilon\\)值构建层次结构， 然后通过分析“簇的稳定性”自动剪枝，选出最稳定的聚类结果。 算法流程： 计算核心距离（Core Distance） 对每个点\\(x_i\\)，计算其到第min_samples个最近邻的距离： \\[ \\text{core}_k(x_i) = \\text{distance to the } k\\text{-th nearest neighbor of } x_i \\] 计算互相可达距离（Mutual Reachability Distance） 对任意两点\\(a, b\\)定义： \\[ d_{\\text{reach}}(a,b) = \\max\\{\\text{core}_k(a), \\text{core}_k(b), d(a,b)\\} \\] 这样可以防止高密度点“误连接”低密度簇。 构建最小生成树MST 以互相可达距离为边权构造最小生成树（Minimum Spanning Tree）。 构建层次聚类树（Condensed Cluster Tree） 逐步增加距离阈值，相当于逐层“切割” MST，每当一条边被移除，一个簇可能分裂成多个子簇，形成一棵密度树。 计算簇稳定性 对每个簇，计算其在不同密度尺度下的持续时间（即“生存时间”）： \\[ \\text{stability}(C) = \\sum_{x_i \\in C} (\\lambda_{\\text{death}} - \\lambda_{\\text{birth}}) \\] 其中\\(\\lambda = 1/\\varepsilon\\)，表示密度水平。 提取最稳定簇 根据簇的稳定性评分，剪枝层次树并输出最稳定的聚类结构。 参数 含义 作用 min_cluster_size 最小簇大小 控制簇的最小规模 min_samples 最小样本数 控制密度估计的平滑度（可近似为 DBSCAN 的 minPts） metric 距离度量 默认为欧氏距离，可改为曼哈顿、余弦等 cluster_selection_method 簇选择方式 \"eom\"（Excess of Mass）或 \"leaf\"（选择层次树的叶节点） "],["dl.html", "10 深度学习", " 10 深度学习 这一章节主要介绍深度学习领域各个神经网络的原理及实现方法——参考《动手学深度学习：PyTorch版》。 鉴于神经网络的可解释性完全不如其他机器学习方法，故一开始对其并不感冒，但没办法，它太powerful了，得学。 参考资料： 菜鸟教程 PyTorch文档 "],["dl_1.html", "10.1 预备知识", " 10.1 预备知识 10.1.1 数据操作 import torch 形状 shape 输出形状 reshape() 更改形状，-1表示自适应。默认按行排列，必要时可先改变形状，后转置得到按列排列的结果 numel() 元素数量有多少 拼接 torch.cat() 在已有维度上拼接 torch.stack() torch.vstack() torch.stack() 在新的维度上堆叠 逐元素操作 传统运算符+ - * / ** sum() 若为空则对所有元素求和；dim指定轴方向，0,1,2...表示维度从外到内 x = torch.arange(24).reshape(2,3,-1) x &quot;&quot;&quot; tensor([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) &quot;&quot;&quot; x.sum(dim=0) &quot;&quot;&quot; tensor([[12, 14, 16, 18], [20, 22, 24, 26], [28, 30, 32, 34]]) &quot;&quot;&quot; x.sum(dim=1) &quot;&quot;&quot; tensor([[12, 15, 18, 21], [48, 51, 54, 57]]) &quot;&quot;&quot; x.sum(dim=2) &quot;&quot;&quot; tensor([[ 6, 22, 38], [54, 70, 86]]) &quot;&quot;&quot; 广播机制 维度对齐，从尾部(最右边)开始逐维度比较，形状不足的张量在左边补1个维度 维度大小为1的轴自动”复制”以匹配较大尺寸 A = torch.tensor([[1, 2, 3], [4, 5, 6]]) # (2, 3) b = torch.tensor([10, 20, 30]) # (3,) result = A * b &quot;&quot;&quot; tensor([[ 10, 40, 90], [ 40, 100, 180]]) &quot;&quot;&quot; # 广播过程：b → (1,3) → (2,3) # 由于张量b维度为1，A的维度为2，相当于张量b先复制了一行，再与A逐元素相乘 节省内存 对于形如X=X+Y的操作，事实上赋值前的X和赋值后的X占用了两个地方的内存（即使变量名相同），建议改为X[:]=X+Y，这样前后X的内存地址就一致了 类型转换 类型 方法 备注 数组-&gt;张量 torch.from_numpy() 共享内存 数组-&gt;张量 torch.tensor() 仅复制 张量-&gt;数组 .numpy() 共享内存 张量-&gt;数组 .clone().numpy() 仅复制 数据框-&gt;数组 .values 内存高 数据框-&gt;数组 .to_numpy(copy=False) 内存低 数组-&gt;数据框 pd.DataFrame() - 默认张量在CPU上，若在GPU上，则先将其移到CPU上，再.cpu().numpy() 存有梯度的张量不能直接转为数组，应.detach().numpy()或.detach().cpu().numpy() 10.1.2 自动微分 深度学习框架能够自动计算导数：先将梯度附加到想要计算偏导数的变量上，然后对目标值进行反向传播backward()，访问得到的梯度。 x = torch.arange(4) x.requires_grad_(True) # 等价于x = torch.arange(4, requires_grad=True) x.grad # 默认值为None y = 2 * torch.dot(x,x) y.backward() x.grad x.grad.zero_() # 变量会累积梯度，在必要时需要清空 对于复合函数y=f(x), z=g(y,x)，有时想控制y直接计算z关于x的梯度，则需要将y剥离出来。 y = x * x u = y.detach() z = u * x z.sum().backward() x.grad == u 10.1.3 加载数据集 from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split 创建数据集对象 TensorDataset(*tensor)用于将内存中的多个张量包装为一个数据集对象。 x = torch.arange(12).reshape(3,4) # 特征 y = torch.tensor([0, 1, 0]) # 标签 dataset = TensorDataset(x,y) # 数据集对象 也可根据抽象类Dataset自定义数据集对象，切记一定要重写__len__()和__getitem__()。 # 自定义数据集类 class MyDataset(Dataset): def __init__(self, X_data, Y_data): &quot;&quot;&quot; 初始化数据集，X_data 和 Y_data 是两个列表或数组 X_data: 输入特征 Y_data: 目标标签 &quot;&quot;&quot; self.X_data = X_data self.Y_data = Y_data def __len__(self): &quot;&quot;&quot;返回数据集的大小&quot;&quot;&quot; return len(self.X_data) def __getitem__(self, idx): &quot;&quot;&quot;返回指定索引的数据&quot;&quot;&quot; x = torch.tensor(self.X_data[idx], dtype=torch.float32) # 转换为 Tensor y = torch.tensor(self.Y_data[idx], dtype=torch.float32) return x, y 加载数据集 DataLoader()用于从数据集中加载数据，并支持打乱、划分批次等操作。 loader = DataLoader( dataset, # 数据集对象 batch_size=128, # 每个批次的样本数 shuffle=True, # 是否打乱数据顺序 sampler=None, # 抽样策略 num_workers=4, # 用于数据加载的进程数 pin_memory=True, # 是否使用固定内存(CUDA) drop_last=False # 是否丢弃最后的不完整批次 ) 划分数据集 random_split()用于将一整个数据集分割为几个不重合的子集。 # generator用于精细化控制每个生成器的种子 # 也可设置全局随机数种子 # torch.manual_seed(132) # 设置随机种子保证每次分割相同 generator = torch.Generator().manual_seed(42) # 固定随机种子 train_set, val_set, test_set = random_split( dataset, [0.7, 0.15, 0.15], # 子集大小 generator=generator # 传递随机数生成器 ) 10.1.4 调参技巧 10.1.4.1 优化器 SGD 每次仅使用一个样本或一个小批次的样本来估计梯度，从而加快训练速度。 \\[ \\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_{\\theta_t} L(\\theta_t) \\] 优点： 简单、高效； 泛化能力较好。 缺点： 由于梯度估计的随机性，容易出现振荡； 对学习率敏感。 SGD + Momentum 引入“动量”概念，让参数更新时参考过去的梯度方向，从而减少震荡、加快收敛。 \\[ v_t = \\beta v_{t-1} + (1 - \\beta) \\nabla_{\\theta_t} L(\\theta_t) \\] \\[ \\theta_{t+1} = \\theta_t - \\eta v_t \\] Adagrad 为每个参数分配不同的学习率，出现频繁的参数自动降低学习率，从而在稀疏特征场景（如 NLP）表现良好。 \\[ G_t = G_{t-1} + g_t^2 \\] \\[ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} g_t \\] RMSProp 为了解决 Adagrad 学习率“衰减过快”的问题，RMSProp用指数加权平均代替累积平方梯度。 \\[ E[g^2]_t = \\beta E[g^2]_{t-1} + (1 - \\beta) g_t^2 \\] \\[ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} g_t \\] Adam 结合了Momentum（动量）与RMSProp（自适应学习率）的优点，通过同时跟踪一阶矩（均值）与二阶矩（方差）来动态调整更新步长。 \\[ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\] \\[ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\] \\[ \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} \\] \\[ \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} \\] AdamW Adam 的改进版本，将L2权重衰减与梯度更新“解耦”，从而更好地控制正则化。 \\[ \\theta_{t+1} = (1 - \\eta \\lambda)\\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} \\] 将权重衰减从梯度计算中分离出来，作为独立项直接应用于参数更新 "],["dl_2.html", "10.2 线性神经网络", " 10.2 线性神经网络 10.2.1 线性回归 nn.Linear是线性层，对输入数据进行仿射变换\\(y=XW^T+b\\)，其中\\(W\\)是权重矩阵，\\(b\\)是偏置。 对于简单的线性回归模型，故只需一层线性层即可。 import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import TensorDataset, DataLoader torch.manual_seed(123) # 设置全局随机数种子 X = torch.randn(100,2) # 标准正态抽样 beta = torch.rand(2) # 均匀分布[0,1)抽样 intercept = torch.rand(1) y = intercept + torch.matmul(X, beta) + torch.randn(100)*0.1 y = y.reshape(-1,1) dataset = TensorDataset(X,y) dataloader = DataLoader(dataset, batch_size=10, shuffle=True) # 搭建网络结构 linear_model = nn.Sequential( nn.Linear(2,1) ) criterion = nn.MSELoss() optimizer = optim.SGD(linear_model.parameters(), lr=0.05) # lr为学习率 for epoch in range(100): # 迭代50次 linear_model.train() # 训练模式 for batch_X, batch_y in dataloader: y_pred = linear_model(batch_X) # 前向传播，计算预测值 loss = criterion(y_pred, batch_y) # 计算损失 optimizer.zero_grad() # 清零梯度 loss.backward() # 反向传播，计算梯度 optimizer.step() # 更新模型参数 linear_model.eval() # 评估模式 with torch.no_grad(): epoch_loss = criterion(linear_model(X),y) if (epoch + 1) % 10 == 0: print(f&#39;Epoch_{epoch + 1}: {epoch_loss.item():.6f}&#39;) print(linear_model) # 查看网络结构 print(linear_model[0].weight.data) # 查看第0层的权重 print(linear_model[0].bias.data) # 查看第0层的偏置 说明： DataLoader()将数据分批次，变为可迭代的对象，每次返回一个批次的数据。也可结合enumerate()适用。 nn.Sequential()按顺序组织多个神经网络层，相较自定义类无需定义forward方法，适用于简单场合的模型构建。 除了nn.Sequential()，还可以通过继承nn.Module来自定义模型架构，从而创建更复杂的模型 nn.Linear()是线性全连接层，用来实现仿射变换\\(y=XW^T+b\\)。 nn.MSELoss()定义了损失函数的类型，计算两个形状相同的张量之间的MSE optim.SGD()定义了优化算法为随机梯度下降法，linear_model.parameters()用于传递需要优化的参数。 .train()、.eval()分别代表模型的训练模式和评估模式，不同模式下会影响部分层(如Dropout层)的行为，一般在训练时开始.train()，在计算相关指标时.eval()。特别的，在评估时还可配合torch.no_grad()来禁用梯度计算，节省内存和计算资源，从而提高计算速度。 10.2.2 二分类问题 对于二分类问题，输出层的维度应该为1，并且输出层的激活函数为Sigmoid函数，损失函数为nn.BCELoss()等适用于二分类场合的函数。 nn.BCELoss()即\\(-y_i \\log \\hat y_i - (1-y_i)\\log (1-\\hat y_i)\\) import torch import torch.nn as nn import torch.optim as optim from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler import numpy as np from torch.utils.data import TensorDataset, DataLoader # 设置随机数种子 torch.manual_seed(123) np.random.seed(321) def gen_data(n_samples=2000, n_features=10, n_classes=2): # 生成复杂的分类型数据（有重叠） X, y = make_classification( n_samples=n_samples, # 样本数 n_features=n_features, # 特征数 n_informative=8, # 有信息量的特征数量 n_redundant=2, # 冗余特征数量 n_repeated=0, # 重复特征数量 n_classes=n_classes, # 类别数 flip_y=0.15, # 15%的噪声 class_sep=0.8 # 类间分离程度 ) # 划分训练集和测试集 X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42 ) # 数据标准化 scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # 转换为PyTorch张量 X_train = torch.tensor(X_train, dtype=torch.float32) y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1) # 二分类需要二维标签 X_test = torch.tensor(X_test, dtype=torch.float32) y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1) return X_train, X_test, y_train, y_test # 生成数据 X_train, X_test, y_train, y_test = gen_data() # 创建数据加载器 train_dataset = TensorDataset(X_train, y_train) test_dataset = TensorDataset(X_test, y_test) train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False) model = nn.Sequential( nn.Linear(10,64), nn.ReLU(), nn.BatchNorm1d(64), # 对该批次数据进行标准化操作，并进行缩放和平移，可在一定程度上缓解“内部协变量偏移”情况 nn.Dropout(0.2), # 以一定概率丢弃某些神经元，从而缓解过拟合现象 nn.Linear(64,32), nn.ReLU(), nn.BatchNorm1d(32), nn.Dropout(0.2), nn.Linear(32,1), nn.Sigmoid() ) criterion = nn.BCELoss() optimizer = optim.SGD(model.parameters(), lr=0.05) # 将模型移到GPU上 device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) model.to(device) # 存储训练指标 train_losses = [] train_accuracies = [] val_losses = [] val_accuracies = [] for epoch in range(100): # 训练模式 model.train() running_loss = 0.0 correct_train = 0 total_train = 0 for batch_x, batch_y in train_loader: # 移动数据到设备 batch_x, batch_y = batch_x.to(device), batch_y.to(device) # 前向传播 outputs = model(batch_x) # 计算损失 loss = criterion(outputs, batch_y) # 反向传播和优化 optimizer.zero_grad() loss.backward() optimizer.step() # 统计训练情况 running_loss += loss.item() # 计算准确率 predictions = (outputs &gt; 0.5).float() correct_train += (predictions == batch_y).sum().item() total_train += batch_y.size(0) # 计算本轮训练的平均损失和准确率 epoch_loss = running_loss / len(train_loader) epoch_acc = correct_train / total_train if total_train &gt; 0 else 0 train_losses.append(epoch_loss) train_accuracies.append(epoch_acc) # 验证评估 model.eval() with torch.no_grad(): # 移动测试数据到设备 test_x, test_y = X_test.to(device), y_test.to(device) # 前向传播 outputs = model(test_x) # 计算损失 val_loss = criterion(outputs, test_y).item() # 计算预测结果 predictions = (outputs &gt; 0.5).float() # 计算准确率 correct_val = (predictions == test_y).sum().item() total_val = test_y.size(0) val_acc = correct_val / total_val # 存储验证结果 val_losses.append(val_loss) val_accuracies.append(val_acc) # 每10个epoch打印一次进度 if (epoch + 1) % 10 == 0: print(&quot;=&quot;*10, f&quot;Epoch_{epoch+1}&quot;, &quot;=&quot;*10, f&quot;\\nTrain Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}\\n&quot;, f&quot;Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}&quot;) 说明： 在将数据存储为张量时就要统一特征和标签的数据类型为浮点数，否则后续特征为浮点数，标签为整数会报错。同时，标签的形状也应变为二维的。 nn.BatchNorm1d()对该批次数据进行标准化操作，并进行缩放和平移，可在一定程度上缓解“内部协变量偏移”情况 nn.Dropout()在训练model.train()时会以一定概率丢弃某些神经元，从而缓解过拟合现象，是一种正则化技术。 无论如何，模型和数据都要在同一设备上。张量数据必须重新赋值batch_x = batch_x.to(device)，而模型则可以直接model.to(device) 10.2.3 多分类问题 对于多分类问题，输出层维度为类别数，无需添加softmax函数，因为在交叉熵损失函数nn.CrossEntropyLoss()中自带了softmax运算。 import torch import torch.nn as nn import torch.optim as optim from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler import numpy as np from torch.utils.data import TensorDataset, DataLoader # 设置随机数种子 torch.manual_seed(123) np.random.seed(321) def gen_data(n_samples=2000, n_features=10, n_classes=5): # 生成复杂的分类型数据（有重叠） X, y = make_classification( n_samples=n_samples, # 样本数 n_features=n_features, # 特征数 n_informative=8, # 有信息量的特征数量 n_redundant=2, # 冗余特征数量 n_repeated=0, # 重复特征数量 n_classes=n_classes, # 类别数 flip_y=0.15, # 15%的噪声 class_sep=0.8 # 类间分离程度 ) # 划分训练集和测试集 X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42 ) # 数据标准化 scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # 转换为PyTorch张量 X_train = torch.tensor(X_train, dtype=torch.float32) y_train = torch.tensor(y_train, dtype=torch.long) # 交叉熵损失函数要求标签为整数型 X_test = torch.tensor(X_test, dtype=torch.float32) y_test = torch.tensor(y_test, dtype=torch.long) return X_train, X_test, y_train, y_test # 生成数据，多分类任务 X_train, X_test, y_train, y_test = gen_data() # 创建数据加载器 train_dataset = TensorDataset(X_train, y_train) test_dataset = TensorDataset(X_test, y_test) train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False) model = nn.Sequential( nn.Linear(10,128), nn.ReLU(), nn.BatchNorm1d(128), # 对该批次数据进行标准化操作，并进行缩放和平移，可在一定程度上缓解“内部协变量偏移”情况 nn.Dropout(0.2), # 以一定概率丢弃某些神经元，从而缓解过拟合现象 nn.Linear(128,64), nn.ReLU(), nn.BatchNorm1d(64), nn.Dropout(0.2), nn.Linear(64,32), nn.ReLU(), nn.BatchNorm1d(32), nn.Dropout(0.2), nn.Linear(32,5) # 输出维度为类别数 ) criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.05) device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) model.to(device) # 存储训练指标 train_losses = [] train_accuracies = [] val_losses = [] val_accuracies = [] for epoch in range(100): # 训练模式 model.train() running_loss = 0.0 correct_train = 0 total_train = 0 for batch_x, batch_y in train_loader: # 移动数据到设备 batch_x, batch_y = batch_x.to(device), batch_y.to(device) # 前向传播 outputs = model(batch_x) # 计算损失 loss = criterion(outputs, batch_y) # 反向传播和优化 optimizer.zero_grad() loss.backward() optimizer.step() # 统计训练情况 running_loss += loss.item() # 计算准确率 predictions = torch.argmax(outputs, dim = 1) # logits值最大的为预测类别 correct_train += (predictions == batch_y).sum().item() total_train += batch_y.size(0) # 计算本轮训练的平均损失和准确率 epoch_loss = running_loss / len(train_loader) epoch_acc = correct_train / total_train if total_train &gt; 0 else 0 train_losses.append(epoch_loss) train_accuracies.append(epoch_acc) # 验证评估 model.eval() with torch.no_grad(): # 移动测试数据到设备 test_x, test_y = X_test.to(device), y_test.to(device) # 前向传播 outputs = model(test_x) # 计算损失 val_loss = criterion(outputs, test_y).item() # 计算预测结果 predictions = torch.argmax(outputs, dim = 1) # 计算准确率 correct_val = (predictions == test_y).sum().item() total_val = test_y.size(0) val_acc = correct_val / total_val # 存储验证结果 val_losses.append(val_loss) val_accuracies.append(val_acc) # 每10个epoch打印一次进度 if (epoch + 1) % 10 == 0: print(&quot;=&quot;*10, f&quot;Epoch_{epoch+1}&quot;, &quot;=&quot;*10, f&quot;\\nTrain Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}\\n&quot;, f&quot;Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}&quot;) 说明： nn.CrossEntropyLoss()接收预测值logits（原值）与标签。其中logits为神经网络的原始输出，无需在输出时添加Softmax激活函数，nn.CrossEntropyLoss()的内部会自动进行Softmax计算，避免重复计算。同时，标签要求为整数型且维度为1，不需要独热编码。 如果需要输出概率，可以在输出logits后手动计算torch.softmax(outputs, dim=1)。 如果要输出预测类别，可以torch.argmax(outputs, dim=1)。 "],["dl_9.html", "10.3 RNN", " 10.3 RNN 图 10.1: RNN 简单来说，RNN将隐状态在时间上依次传递，当前时间步的隐状态由当前时间步的输入与上一时间步的隐状态得到，当前时间步的输出由当前时间步的隐状态得到。 \\[ \\begin{gather} s_t = f(W_s s_{t-1} + W_x x_t) \\\\ o_t = g(W_os_t) \\end{gather} \\] 但是在训练过程中，由于这种循环结构，容易导致梯度爆炸或消失问题。 对于一个简单的循环神经网络 (RNN)，隐藏状态的更新为： \\[ s_t = f(W_s s_{t-1} + W_x x_t) \\] 训练RNN时，需要对损失函数\\(L_T\\)关于参数\\(W_s\\)求导。 梯度在时间维度上通过链式法则传播： \\[ \\frac{\\partial L_T}{\\partial W_s} = \\sum_{t=1}^{T} \\frac{\\partial L_T}{\\partial s_t} \\frac{\\partial s_t}{\\partial W_s} \\] 关键项为梯度在时间维上的传播： \\[ \\frac{\\partial L_T}{\\partial s_t} = \\frac{\\partial L_T}{\\partial s_T} \\prod_{k=t+1}^{T} \\frac{\\partial s_k}{\\partial s_{k-1}} \\] 而每一步的梯度传递因子为： \\[ \\frac{\\partial s_k}{\\partial s_{k-1}} = W_s^T \\cdot \\text{diag}(f&#39;(W_s s_{k-2} + W_x x_{k-1})) \\] 也就是说，RNN的梯度是由多个线性变换与激活函数导数的连乘积组成。 对于常见的激活函数，例如tanh，其导数位于[0,1]之间，因此在多次连乘下极容易梯度消失。而当权重矩阵\\(W_s\\)过大时，又容易产生梯度爆炸问题。 解决方法： 梯度裁剪 将梯度范数超过阈值时进行缩放，可抑制梯度爆炸。 门控结构（LSTM/GRU） LSTM和GRU都是通过门控加法机制实现信息传递。以LSTM为例： \\[ C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \\] \\[ \\frac{\\partial C_t}{\\partial C_{t-1}} = f_t \\] \\[ \\frac{\\partial L}{\\partial C_{t-1}} = \\frac{\\partial L}{\\partial C_t} \\odot f_t \\] 梯度只与门控值\\(f_t\\)相乘，而不涉及矩阵连乘。若\\(f_t \\approx 1\\)，梯度几乎可恒等传播；若\\(f_t &lt; 1\\)，梯度以可控方式衰减。 矩阵乘法会改变梯度的方向与长度，导致非线性扰动；而逐元素标量乘法不会改变方向，只调整幅度。门控\\(f_t \\in [0,1]\\)还可以自适应地学习需要保留的梯度比例，从而在数值上保持稳定传播。 "],["dl_3.html", "10.4 LSTM", " 10.4 LSTM 10.4.1 原理 在学习LSTM之前，可以先了解一下RNN，再去看LSTM。 【循环神经网络】5分钟搞懂RNN，3D动画深入浅出 【LSTM长短期记忆网络】3D模型一目了然，带你领略算法背后的逻辑 图 10.2: LSTM LSTM的原理简单表示为下面几个公式。 记输入为\\(X\\)，隐状态为\\(H\\)，记忆元为\\(C\\)，输入门为\\(I\\)，遗忘门为\\(F\\)，输出门为\\(O\\)，则有 记忆元代表长期记忆，隐状态代表短期记忆 \\[ \\begin{aligned} I_t &amp;= \\sigma (X_tW_{xi}+H_{t-1}W_{hi}+b_i) \\\\ F_t &amp;= \\sigma (X_tW_{xf}+H_{t-1}W_{hf}+b_f) \\\\ O_t &amp;= \\sigma (X_tW_{xo}+H_{t-1}W_{ho}+b_o) \\\\ \\tilde C_t &amp;= \\tanh(X_tW_{xc}+H_{t-1}W_{hc}+b_c) \\\\ C_t &amp;= F_t \\odot C_{t-1} + I_t \\odot \\tilde C_t \\\\ H_t &amp;= O_t \\odot \\tanh (C_t) \\end{aligned} \\] 其中\\(W,b\\)分别代表权重与偏置，\\(\\sigma\\)表示sigmoid函数，值域为[0,1]，代表着信息剩余的比例，\\(\\tanh\\)表示双曲正切函数，值域为[-1,1]，代表着信息的大小及方向。 简单来看，\\(X\\)和\\(H\\)是对短期内的信息进行加工，然后将其上传到长期记忆中，而长期记忆也会遗忘部分信息，因此更新后的长期记忆表现为剩余的长期记忆与短期信息的和。而短期记忆又是长期记忆部分的一部分，并且会受到长期记忆的影响，因此\\(H\\)又可以由\\(C\\)产生。在这个过程中，就由输入门、遗忘门和输出门来控制信息损耗的比例。 10.4.2 示例 import torch import torch.nn as nn import numpy as np from torch.utils.data import Dataset, DataLoader # 设置随机种子以确保结果可复现 torch.manual_seed(42) np.random.seed(42) # 1. 生成正弦波数据 total_length = 1000 time_steps = np.linspace(0, 20 * np.pi, total_length) data_sequence = np.sin(time_steps) + np.random.normal(0, 0.1, total_length) # 2. 划分训练集和测试集 split_idx = int(total_length * 0.8) # 80%训练集，20%测试集 train_data = data_sequence[:split_idx] test_data = data_sequence[split_idx:] # 3. 创建序列数据集类 class SequenceDataset(Dataset): def __init__(self, data, seq_length=20): self.data = torch.FloatTensor(data) self.seq_length = seq_length def __len__(self): return len(self.data) - self.seq_length def __getitem__(self, idx): input_seq = self.data[idx:idx+self.seq_length] target = self.data[idx+self.seq_length] return input_seq.view(self.seq_length, 1), target.view(1) # 添加特征维度 # 创建数据集和数据加载器 seq_length = 20 batch_size = 32 # 训练集 train_dataset = SequenceDataset(train_data, seq_length) train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True) # 测试集 test_dataset = SequenceDataset(test_data, seq_length) test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True) # 4. 定义LSTM模型 class LSTMModel(nn.Module): def __init__(self, input_size=1, hidden_size=32, output_size=1, num_layers=1): super().__init__() self.num_layers = num_layers self.hidden_size = hidden_size # LSTM层 self.lstm = nn.LSTM( input_size=input_size, # 默认为1即时序的自回归结构 hidden_size=hidden_size, num_layers=num_layers, batch_first=True ) # 输出层 self.linear = nn.Linear(hidden_size, output_size) def forward(self, x, hidden=None): # 初始化隐藏状态 h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(&#39;cuda&#39;) c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(&#39;cuda&#39;) hidden = (h0, c0) # 前向传播LSTM # 需要将输入形状调整为 [batch, seq_len, features] x = x.reshape(x.size(0), -1, 1) if x.dim() == 2 else x # out表示LSTM最后一层所有时间步的输出 # hidden表示LSTM所有层在最后一个时间步的最终状态 out, hidden = self.lstm(x, hidden) # 只取最后一个时间步的输出(batch, seq, features) out = self.linear(out[:, -1, :]) return out, hidden # 5. 实例化模型 input_size = 1 hidden_size = 64 output_size = 1 num_layers = 1 model = LSTMModel(input_size, hidden_size, output_size, num_layers) # 将模型移到GPU上 model.to(&#39;cuda&#39;) # 6. 定义损失函数和优化器 criterion = nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # 7. 训练模型 num_epochs = 50 train_losses = [] test_losses = [] for epoch in range(num_epochs): # --- 训练阶段 --- model.train() epoch_train_loss = 0.0 for inputs, targets in train_loader: # 输入形状: [batch_size, seq_length, 1] # 目标形状: [batch_size, 1] optimizer.zero_grad() inputs, targets = inputs.to(&#39;cuda&#39;), targets.to(&#39;cuda&#39;) # 前向传播 outputs, _ = model(inputs) loss = criterion(outputs, targets) # 反向传播和优化 loss.backward() optimizer.step() epoch_train_loss += loss.item() # 计算平均训练损失 avg_train_loss = epoch_train_loss / len(train_loader) train_losses.append(avg_train_loss) # --- 测试阶段 --- model.eval() epoch_test_loss = 0.0 with torch.no_grad(): for inputs, targets in test_loader: inputs, targets = inputs.to(&#39;cuda&#39;), targets.to(&#39;cuda&#39;) outputs, _ = model(inputs) loss = criterion(outputs, targets) epoch_test_loss += loss.item() # 计算平均测试损失 avg_test_loss = epoch_test_loss / len(test_loader) test_losses.append(avg_test_loss) # 每5个epoch打印一次进度 if (epoch+1) % 5 == 0 or epoch == 0: print(f&quot;Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.6f}, Test Loss: {avg_test_loss:.6f}&quot;) 说明： 对于时序数据，给定t个数据后，t+1时刻的值即为响应变量。除了目标变量的自回归结构，还可以添加其他预测变量，记得要修改input_size。 关于h和c的初始状态，绝大部分场景下都需要进行重置，即不同序列之间的初始状态是独立的。同时，初始状态也不是需要学习的参数，除特殊任务外，一般都无需设置梯度。 如果要进行预测，则需要根据特定步长的窗口序列来预测下一时刻的目标值。单步预测直接让全连接层的输出维度为1；多步预测可以让全连接层的输出维度为目标维度，这等价于把未来步间的依赖压缩在最后的隐状态里，亦或者采取滚动预测的方式进行多步输出；除此之外，可考虑Seq2Seq的方法进行多步预测。 10.4.3 拓展 多层LSTM 多层LSTM将第一层LSTM的输出序列（通常是每个时间步的隐藏状态）作为输入，相较于单层LSTM能够提取更为复杂的特征。对于简单任务还是使用单层LSTM，一般层数也不宜超过4层。 单向与双向 常规的LSTM都是从历史数据出发，由老及新，根据历史去预测未来。而双向LSTM则包含了两个LSTM层，一个在时间上从前到后，另一个在时间上从后到前。这使得模型能够捕捉序列的“历史信息”与“未来信息”，在输出时融合这两个LSTM层的隐藏状态作为最终输出。 对于时间序列的预测任务只能使用单向LSTM。 可与注意力机制结合起来提升性能。 "],["dl_4.html", "10.5 GRU", " 10.5 GRU 10.5.1 原理 GRU是LSTM的简化版本，仅有两个门控——重置门（遗忘）与更新门，同时也缺少记忆元，这使得GRU在训练时更加快捷。 图 10.3: GRU \\[ \\begin{aligned} R_t &amp;= \\sigma(X_t W_{xr} + H_{t-1} W_{hr} + b_r) \\\\ Z_t &amp;= \\sigma(X_t W_{xz} + H_{t-1} W_{hz} + b_z) \\\\ \\tilde{H}_t &amp;= \\tanh(X_t W_{xh} + (R_t \\odot H_{t-1}) W_{hh} + b_h) \\\\ H_t &amp;= Z_t \\odot H_{t-1} + (1 - Z_t) \\odot \\tilde{H}_t \\end{aligned} \\] 重置门\\(R_t\\)用于控制过去的隐藏状态有多少内容被用于生成当前候选隐藏状态，更新门\\(Z_t\\)用于控制生成当前隐藏状态时过去隐藏状态和候选隐藏状态的权重。 10.5.2 示例 import torch import torch.nn as nn import torch.optim as optim import numpy as np import matplotlib.pyplot as plt # ------------------------------ # 1. 生成模拟数据 # ------------------------------ # 我们造一个简单的任务：输入一个时间序列（正弦+噪声），预测最后一个时刻的值 np.random.seed(42) torch.manual_seed(42) def generate_data(num_samples=200, seq_len=20): X = [] y = [] for _ in range(num_samples): freq = np.random.uniform(0.5, 1.5) phase = np.random.uniform(0, np.pi) noise = np.random.normal(0, 0.1, seq_len) seq = np.sin(np.linspace(0, 2 * np.pi * freq, seq_len) + phase) + noise X.append(seq) y.append(seq[-1]) # 预测最后一个点 X = np.expand_dims(np.array(X), axis=2) # (N, T, 1) y = np.expand_dims(np.array(y), axis=1) # (N, 1) return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32) X, y = generate_data(num_samples=300, seq_len=30) train_X, test_X = X[:240], X[240:] train_y, test_y = y[:240], y[240:] # ------------------------------ # 2. 定义 GRU 模型 # ------------------------------ class GRUNet(nn.Module): def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2, bidirectional=False): super(GRUNet, self).__init__() self.hidden_size = hidden_size self.num_layers = num_layers self.bidirectional = bidirectional self.gru = nn.GRU( input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout if num_layers &gt; 1 else 0.0, bidirectional=bidirectional, batch_first=True ) # 如果是双向GRU，需要乘2 direction_factor = 2 if bidirectional else 1 self.fc = nn.Linear(hidden_size * direction_factor, output_size) def forward(self, x): out, h = self.gru(x) # out: (batch, seq, hidden*direction) out = self.fc(out[:, -1, :]) # 取最后一个时间步的输出 return out # ------------------------------ # 3. 初始化模型与优化器 # ------------------------------ model = GRUNet( input_size=1, # 每个时间步输入1个特征 hidden_size=32, # 隐层维度 num_layers=1, # 堆叠1层GRU output_size=1, # 输出一个数（预测值） dropout=0.2, bidirectional=False # 是否使用双向GRU ) criterion = nn.MSELoss() optimizer = optim.Adam(model.parameters(), lr=0.005) # ------------------------------ # 4. 训练模型 # ------------------------------ epochs = 100 train_losses = [] for epoch in range(epochs): model.train() optimizer.zero_grad() output = model(train_X) loss = criterion(output, train_y) loss.backward() optimizer.step() train_losses.append(loss.item()) if (epoch + 1) % 20 == 0: print(f&quot;Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}&quot;) # 绘制训练损失曲线 plt.figure(figsize=(6,4)) plt.plot(train_losses) plt.title(&quot;Training Loss Curve&quot;) plt.xlabel(&quot;Epoch&quot;) plt.ylabel(&quot;MSE Loss&quot;) plt.show() # ------------------------------ # 5. 测试与可视化 # ------------------------------ model.eval() with torch.no_grad(): pred = model(test_X).squeeze().numpy() truth = test_y.squeeze().numpy() plt.figure(figsize=(8,5)) plt.plot(truth, label=&quot;True&quot;) plt.plot(pred, label=&quot;Predicted&quot;) plt.legend() plt.title(&quot;GRU Prediction on Test Set&quot;) plt.show() # 计算误差指标 mse = np.mean((pred - truth)**2) mae = np.mean(np.abs(pred - truth)) print(f&quot;Test MSE: {mse:.6f}, MAE: {mae:.6f}&quot;) 10.5.3 拓展 同LSTM。 "],["dl_5.html", "10.6 Seq2Seq", " 10.6 Seq2Seq 10.6.1 原理 图 10.4: seq2seq 当然对于翻译任务，应当有个词嵌入环节 Seq2Seq由编码器和解码器组成，二者均是RNN（包括LSTM和GRU）结构，用于解决由原始序列输出目标序列的任务，两个序列可以不等长。 在编码器部分，由于输入序列已知，因此可以使用双向RNN结构用于提取信息，然后在最后一个时间步输出隐状态，并将此隐状态作为上下文向量context vector，记为\\(c\\)。 上下文向量相当于是对原始序列信息的一个浓缩 在解码器部分，若记解码器的隐状态为\\(s\\)，则 \\[ s_t = f(s_{t-1}, y_{t-1}, c) \\] 即解码器t时刻的隐状态由上一步的隐状态、上一步的预测值、编码器的上下文向量共同输入得到。 在训练时，采取\\(Teacher Forcing\\)策略，即每次输入的\\(y\\)值为真实值，这有助于模型训练。但在预测时则接收上一步的预测值作为输入 但是，Seq2Seq有很明显的缺陷，即在解码器中使用的上下文向量是固定的，而由于编码器的RNN结构，导致这个上下文向量难以记住更早的重要信息。于是，提出Seq2Seq+注意力机制的方法。 下面介绍Luong的论文Effective Approaches to Attention-based Neural Machine Translation。 这篇论文可以说是对Bahdanau的NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE这篇论文的改进，有细微差异，例如这篇论文用\\(s_{t-1}\\)来更新\\(c_t\\)，而Luong用\\(s_t\\) 图 10.5: Seq2Seq与注意力机制（Input-Feeding） 计算流程为： 编码阶段 输入序列 \\((x_1, x_2, \\ldots, x_S)\\) 经双向LSTM编码得到： \\[ h_1, h_2, \\ldots, h_S = \\text{EncoderRNN}(x_1, \\ldots, x_S) \\] 每个\\(h_s\\)是源序列中第\\(s\\)个词的隐状态输出。 解码器状态更新 在时间步\\(t\\)，解码器根据上一步输出的目标词\\(y_{t-1}\\)与前一隐藏状态 \\(s_{t-1}\\)，更新当前解码器状态： \\[ s_t = \\text{DecoderRNN}(y_{t-1}, s_{t-1}) \\] 此时 \\(s_t\\) 表示“当前要生成第 \\(t\\) 个词”的语义状态。 论文还提出了Input-Feeding机制，即在生成\\(s_t\\)时也用到了\\(\\tilde{s}_{t-1}\\)的信息，\\(\\tilde{s}_{t-1}\\)后续会介绍。 \\[ s_t = \\text{DecoderRNN}(y_{t-1}, \\tilde{s}_{t-1}, s_{t-1}) \\] 注意力机制 通过相似度函数计算\\(s_t\\)与每个\\(h_s\\)的匹配程度，论文中介绍了三种计算方法： \\[ \\text{score}(s_t, h_s) = \\begin{cases} s_t^\\top h_s, &amp; \\text{(Dot)} \\\\ s_t^\\top W_a h_s, &amp; \\text{(General)} \\\\ v_a^\\top \\tanh(W_a [s_t; h_s]), &amp; \\text{(Concat)} \\end{cases} \\] Bahdanau的论文中使用\\(e_{t,i}= v_a^{T} \\text{tanh}(W_ss_{t-1}+W_hh_i)\\)加性注意力函数来计算相似度得分 之后通过softmax归一化得到每个源词的注意力权重： \\[ a_t(s) = \\frac{\\exp(\\text{score}(s_t, h_s))} {\\sum_{s&#39;=1}^{S} \\exp(\\text{score}(s_t, h_{s&#39;}))} \\] 其中\\(a_t(s)\\) 表示当前解码器在生成第\\(t\\)个词时，关注源序列\\(s\\)个位置的程度。 计算上下文向量 对编码器输出进行加权求和，得到上下文向量： \\[ c_t = \\sum_{s=1}^{S} a_t(s)\\, h_s \\] \\(c_t\\)是源端信息的加权摘要，代表模型在第\\(t\\)步“看到”的输入信息。 信息融合 Luong 定义了attentional hidden state： \\[ \\tilde{s}_t = \\tanh(W_c [c_t; s_t]) \\] 该向量综合了当前目标语义与注意到的源端信息。 输出词预测 通过线性层与 softmax 计算目标词概率分布： \\[ p(y_t \\mid y_{&lt;t}, x) = \\text{softmax}(W_o \\tilde{s}_t) \\] 取概率最大的词作为预测输出： \\[ \\hat{y}_t = \\arg\\max_y p(y_t) \\] "],["dl_6.html", "10.7 BNN", " 10.7 BNN 10.7.1 原理 在传统神经网络中，模型参数（权重\\(w\\)）被视为固定值，通过最小化损失函数获得最优点估计： \\[ \\hat{w} = \\arg\\max_w p(D|w) \\] 其中\\(D=\\{(x_1​,y_1​),(x_2​,y_2​),…,(x_N​,y_N​)\\}\\)表示数据集。 然而，在现实问题中，数据噪声与异质性会导致模型存在显著不确定性。贝叶斯神经网络（Bayesian Neural Network,BNN）的核心思想是将模型参数\\(w\\)看作随机变量，并通过贝叶斯推断来量化模型不确定性。 BNN 的关键思想源于贝叶斯定理： \\[ p(w|D) = \\frac{p(D|w)p(w)}{p(D)} \\] 其中： \\(p(w)\\)：参数的先验分布； \\(p(D|w)\\)：数据在给定参数下的似然函数； \\(p(w|D)\\)：参数的后验分布； \\(p(D)\\)：边际似然（证据）。 因此，BNN不再求单点参数\\(\\hat{w}\\)，而是学习整个参数分布\\(p(w|D)\\)。 给定新输入\\(x^*\\)，预测输出\\(y^*\\)的分布为： \\[ p(y^*|x^*, D) = \\int p(y^*|x^*, w) \\, p(w|D) \\, dw \\] 由于该积分难以解析计算，通常采用近似推断方法求解，如： 变分推断 马尔科夫链蒙特卡洛 Monte Carlo Dropout 深度集合 在这里仅介绍变分推断和MC Dropout方法。 变分推断法就是用一个可学习分布\\(q(w|\\theta)\\)来近似后验分布\\(p(w|D)\\)，通过最小化两者的KL散度实现优化。经过一系列推导可知，最小化KL散度等价于最大化ELBO： 详细推导可见什么是变分推断 \\[ ELBO = \\mathbb{E}_{q(w|\\theta)}[\\log p(D|w)] - KL(q(w|\\theta)\\;||\\;p(w)) \\] 若令先验分布\\(p(w)\\)为标准正态分布，近似后验分布为正态分布，则： \\[ KL(\\mathcal{N}(\\mu,\\sigma^2)\\;||\\;\\mathcal{N}(0,1)) = \\frac{1}{2}(\\sigma^2 + \\mu^2 - 1 - \\log\\sigma^2) \\] 据此可用重参数化技巧\\(w = \\mu + \\sigma \\epsilon, \\epsilon \\sim N(0,1)\\)来对\\(w\\)进行MC抽样。 MC Dropout是一种近似贝叶斯推断方法： 在训练和预测阶段都启用 Dropout； 每次前向传播都会随机丢弃部分神经元； 多次采样预测结果，计算均值与方差。 \\[ p(y|x, D) \\approx \\frac{1}{M} \\sum_{i=1}^{M} f(x; \\hat{w}_i) \\] 每个\\(\\hat{w}_i\\)对应一次随机Dropout后的网络参数。 10.7.2 示例 变分推断法： # =============================================== # 贝叶斯神经网络 (Bayesian Neural Network) 示例 # 使用变分近似 + 多次采样预测不确定性 # =============================================== import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim import matplotlib.pyplot as plt # ------------------------------------------------ # 1. 生成模拟数据：y = sin(x) + 噪声 # ------------------------------------------------ torch.manual_seed(42) N = 100 x = torch.linspace(-3, 3, N).unsqueeze(1) y_true = torch.sin(x) y = y_true + 0.2 * torch.randn_like(y_true) # 添加噪声 plt.figure(figsize=(7, 4)) plt.scatter(x, y, label=&quot;Noisy observations&quot;, s=15) plt.plot(x, y_true, color=&#39;orange&#39;, label=&quot;True function&quot;) plt.legend() plt.title(&quot;Training Data (sin function + noise)&quot;) plt.show() # ------------------------------------------------ # 2. 定义贝叶斯线性层 # ------------------------------------------------ class BayesianLinear(nn.Module): def __init__(self, in_features, out_features): super().__init__() # 均值与log方差参数 self.w_mu = nn.Parameter(torch.Tensor(out_features, in_features).normal_(0, 0.1)) self.w_logvar = nn.Parameter(torch.Tensor(out_features, in_features).normal_(-3, 0.1)) self.b_mu = nn.Parameter(torch.Tensor(out_features).normal_(0, 0.1)) self.b_logvar = nn.Parameter(torch.Tensor(out_features).normal_(-3, 0.1)) def forward(self, x, sample=True): if sample: w = self.w_mu + torch.exp(0.5 * self.w_logvar) * torch.randn_like(self.w_mu) b = self.b_mu + torch.exp(0.5 * self.b_logvar) * torch.randn_like(self.b_mu) else: w, b = self.w_mu, self.b_mu return F.linear(x, w, b) def kl_loss(self): # KL 散度项：衡量权重分布与先验 N(0,1) 的距离 return 0.5 * torch.sum( torch.exp(self.w_logvar) + self.w_mu**2 - 1.0 - self.w_logvar ) + 0.5 * torch.sum( torch.exp(self.b_logvar) + self.b_mu**2 - 1.0 - self.b_logvar ) # ------------------------------------------------ # 3. 定义贝叶斯神经网络模型 # ------------------------------------------------ class BayesianNN(nn.Module): def __init__(self): super().__init__() self.fc1 = BayesianLinear(1, 20) self.fc2 = BayesianLinear(20, 20) self.fc3 = BayesianLinear(20, 1) def forward(self, x, sample=True): x = torch.relu(self.fc1(x, sample)) x = torch.relu(self.fc2(x, sample)) return self.fc3(x, sample) def kl_loss(self): return self.fc1.kl_loss() + self.fc2.kl_loss() + self.fc3.kl_loss() # ------------------------------------------------ # 4. 训练模型 # ------------------------------------------------ model = BayesianNN() optimizer = optim.Adam(model.parameters(), lr=0.01) epochs = 2000 for epoch in range(epochs): optimizer.zero_grad() y_pred = model(x, sample=True) # 似然项 (MSE) likelihood = F.mse_loss(y_pred, y, reduction=&#39;sum&#39;) # KL 散度项 kl = model.kl_loss() # 总损失 = 似然项 + KL权重 loss = likelihood + 1e-3 * kl loss.backward() optimizer.step() if (epoch + 1) % 200 == 0: print(f&quot;Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}&quot;) # ------------------------------------------------ # 5. 多次采样预测，计算不确定性 # ------------------------------------------------ model.eval() x_test = torch.linspace(-3, 3, 200).unsqueeze(1) pred_samples = [] with torch.no_grad(): for _ in range(100): # 采样100次 pred = model(x_test, sample=True) pred_samples.append(pred) pred_stack = torch.stack(pred_samples) # (100, 200, 1) y_mean = pred_stack.mean(0).squeeze() y_std = pred_stack.std(0).squeeze() # ------------------------------------------------ # 6. 可视化预测结果与置信区间 # ------------------------------------------------ plt.figure(figsize=(8,5)) plt.plot(x_test, torch.sin(x_test), &#39;orange&#39;, label=&#39;True function&#39;) plt.scatter(x, y, color=&#39;gray&#39;, s=15, label=&#39;Training data&#39;) plt.plot(x_test, y_mean, &#39;b&#39;, label=&#39;Predicted mean&#39;) plt.fill_between( x_test.squeeze().numpy(), (y_mean - 2*y_std).numpy(), (y_mean + 2*y_std).numpy(), color=&#39;lightblue&#39;, alpha=0.4, label=&#39;±2 std (uncertainty)&#39; ) plt.legend() plt.title(&quot;Bayesian Neural Network Prediction with Uncertainty&quot;) plt.show() Droptout法： # =========================================================== # Monte Carlo Dropout 版 贝叶斯神经网络 (Bayesian NN) # =========================================================== import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim import matplotlib.pyplot as plt # ------------------------------------------------- # 1. 生成模拟数据：y = sin(x) + 噪声 # ------------------------------------------------- torch.manual_seed(42) N = 100 x = torch.linspace(-3, 3, N).unsqueeze(1) y_true = torch.sin(x) y = y_true + 0.2 * torch.randn_like(y_true) plt.figure(figsize=(7,4)) plt.scatter(x, y, label=&quot;Noisy observations&quot;, s=15) plt.plot(x, y_true, color=&#39;orange&#39;, label=&quot;True function&quot;) plt.legend() plt.title(&quot;Training Data (sin function + noise)&quot;) plt.show() # ------------------------------------------------- # 2. 定义 Dropout 版神经网络 # ------------------------------------------------- class MCDropoutNN(nn.Module): def __init__(self, input_dim=1, hidden_dim=64, output_dim=1, dropout_p=0.2): super(MCDropoutNN, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.fc2 = nn.Linear(hidden_dim, hidden_dim) self.fc3 = nn.Linear(hidden_dim, output_dim) self.dropout = nn.Dropout(p=dropout_p) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.dropout(x) x = torch.relu(self.fc2(x)) x = self.dropout(x) x = self.fc3(x) return x # ------------------------------------------------- # 3. 训练模型 # ------------------------------------------------- model = MCDropoutNN(input_dim=1, hidden_dim=64, dropout_p=0.2) optimizer = optim.Adam(model.parameters(), lr=0.01) criterion = nn.MSELoss() epochs = 2000 for epoch in range(epochs): model.train() optimizer.zero_grad() y_pred = model(x) loss = criterion(y_pred, y) loss.backward() optimizer.step() if (epoch + 1) % 200 == 0: print(f&quot;Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}&quot;) # ------------------------------------------------- # 4. 测试阶段：启用 Dropout，多次采样预测 # ------------------------------------------------- def mc_dropout_predict(model, x_test, n_samples=100): model.train() # 关键！保持 Dropout 激活状态 preds = [] with torch.no_grad(): for _ in range(n_samples): pred = model(x_test) preds.append(pred) preds = torch.stack(preds) # (n_samples, N, 1) return preds x_test = torch.linspace(-3, 3, 200).unsqueeze(1) preds = mc_dropout_predict(model, x_test, n_samples=100) y_mean = preds.mean(0).squeeze() y_std = preds.std(0).squeeze() # ------------------------------------------------- # 5. 可视化结果 # ------------------------------------------------- plt.figure(figsize=(8,5)) plt.plot(x_test, torch.sin(x_test), &#39;orange&#39;, label=&#39;True function&#39;) plt.scatter(x, y, color=&#39;gray&#39;, s=15, label=&#39;Training data&#39;) plt.plot(x_test, y_mean, &#39;b&#39;, label=&#39;Predicted mean&#39;) plt.fill_between( x_test.squeeze().numpy(), (y_mean - 2*y_std).numpy(), (y_mean + 2*y_std).numpy(), color=&#39;lightblue&#39;, alpha=0.4, label=&#39;±2 std (uncertainty)&#39; ) plt.legend() plt.title(&quot;MC Dropout Bayesian Neural Network (Uncertainty Estimation)&quot;) plt.show() "],["dl_7.html", "10.8 GNN", " 10.8 GNN 零基础多图详解图神经网络（GNN/GCN）【论文精读】 10.8.1 原理 图的结构 图由边与节点构成，即\\(G=\\{V,E\\}\\)，有时还会附带全局信息\\(\\text{Master Node(U)}\\)。无论是节点、边还是全局信息，都可通过向量来存储数据。而对于图的连接信息，则可通过邻接列表（二维列表，每个子列表代表谁与谁连接）来存储。 信息聚合 边、节点亦或是全局信息都可通过信息聚合的方式从邻居（三者都可以）处获取信息。可通过求和、求平均、取最大值的形式完成信息聚合。 sum、mean、max的操作没有显著差异 输入与输出 图神经网络的输入与输出都是图，每个GNN层都是一次信息聚合，从而完成节点、边或全局信息的更新。堆叠层数越多就是让元素能够逐步整合更大范围的图结构信息。 10.8.2 图神经网络的类型 GCN（图卷积神经网络） GCN以节点为研究单位，根据连接关系从邻居节点处聚合特征。 GAT（图注意力网络） 引入注意力机制，使得GCN能够根据注意力得分对邻居特征进行加权。 ST-GNN（时空图神经网络） 利用图神经网络从空间角度建模，也就是说可以用图神经网络对具有网络结构的数据进行特征提取。之后可将提取后的特征代入到时序模型，例如LSTM、GRU等。通过同时捕捉节点间拓扑依赖和时间动态变化，实现对时空关联数据的精准预测。 10.8.3 示例 图神经网络建模可通过torch_geometric实现。基本建模过程就是定义图数据结构（确定每个节点的特征、构建边关系），之后再定义图神经网络模型即可。 注意图神经网络的视角是空间视角，抓住数据中的结构关系即可 import torch import torch.nn.functional as F from torch_geometric.data import Data from torch_geometric.nn import GCNConv # =============================== # 1️⃣ 定义图结构 # =============================== # 图的边 (source, target)，采用 COO 格式 # 例如：0↔1, 1↔2, 2↔3, 3↔4, 4↔0 edge_index = torch.tensor([ [0, 1, 2, 3, 4, 1, 2, 3, 4, 0], # source [1, 2, 3, 4, 0, 0, 1, 2, 3, 4] # target ], dtype=torch.long) # 每个节点的特征（这里每个节点3维） x = torch.randn((5, 3)) # 如果是节点分类任务，可加上节点标签 y = torch.tensor([0, 1, 0, 1, 0], dtype=torch.long) # 构建图数据对象 data = Data(x=x, edge_index=edge_index, y=y) print(&quot;图信息：&quot;) print(data) print(&quot;节点特征形状:&quot;, data.x.shape) print(&quot;边数量:&quot;, data.edge_index.shape[1]) # =============================== # 2️⃣ 定义 GCN 模型 # =============================== class GCN(torch.nn.Module): def __init__(self, in_channels, hidden_channels, out_channels): super(GCN, self).__init__() self.conv1 = GCNConv(in_channels, hidden_channels) self.conv2 = GCNConv(hidden_channels, out_channels) def forward(self, x, edge_index): # 第一层：图卷积 + ReLU x = self.conv1(x, edge_index) x = F.relu(x) # 第二层：图卷积 + Softmax 输出（用于分类） x = self.conv2(x, edge_index) return x # =============================== # 3️⃣ 实例化模型并前向传播 # =============================== model = GCN(in_channels=3, hidden_channels=4, out_channels=2) out = model(data.x, data.edge_index) print(&quot;\\n输出特征形状:&quot;, out.shape) print(&quot;输出节点嵌入：\\n&quot;, out) # 若是分类任务： pred = out.argmax(dim=1) print(&quot;\\n节点类别预测：&quot;, pred) "],["dl_8.html", "10.9 Diffusion Model", " 10.9 Diffusion Model 扩散模型（Diffusion Model）是一类生成模型，其核心思想是通过逐步添加噪声再逐步去噪，从而学习到复杂数据的分布并生成新的样本。 10.9.1 原理 在前向过程中，设原始样本为\\(x_0\\)，通过\\(T\\)步逐步加噪，得到\\(x_1, x_2, \\dots, x_T\\)。 每一步定义为： \\[ q(x_t | x_{t-1}) = \\mathcal{N}\\left(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\, \\beta_t I \\right) \\] 即： \\[ x_t = \\sqrt{1-\\beta_t}\\,x_{t-1} + \\sqrt{\\beta_t}\\,\\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0, I) \\] 其中： \\(\\beta_t \\in (0, 1)\\) 为每步噪声强度； \\(I\\) 为单位协方差矩阵。 设上一时刻\\(x_{t-1}\\) 的方差为1，则： \\[ \\text{Var}(x_t) = (1-\\beta_t)\\text{Var}(x_{t-1}) + \\beta_t\\text{Var}(\\epsilon_t) = (1-\\beta_t) + \\beta_t = 1 \\] 因此，若使用系数\\(\\sqrt{1-\\beta_t}\\)，可以保证每一步加噪后整体方差不变， 从而避免分布在时间上传递时发散或塌缩。 \\(\\sqrt{1-\\beta_t}\\)表示保留上一时刻信号的能量比例，\\(\\sqrt{\\beta_t}\\)表示注入噪声的能量比例 记\\(\\alpha_t = 1-\\beta_t, \\quad \\bar{\\alpha}_t= \\prod_{s=1}^t\\alpha_s\\)，通过递归展开 \\[ \\begin{aligned} x_t &amp;= \\sqrt{\\alpha_t}x_{t-1}+\\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\\\ &amp;= \\sqrt{\\alpha_t\\alpha_{t-1}}x_{t-2}+\\sqrt{\\alpha_t(1-\\alpha_{t-1})}\\epsilon_{t-2}+\\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\\\ &amp;= \\cdots \\\\ &amp;= \\sqrt{\\bar{\\alpha}_t}\\,x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\,\\epsilon \\end{aligned} \\] 其中\\(\\epsilon \\sim \\mathcal{N}(0,I)\\)。噪声独立同分布于正态分布，根据正态分布的可加性，可以将所有高斯噪声合并为一个噪声，期望显然为0，注意到方差为 \\[ \\begin{aligned} Var(\\sqrt{\\alpha_t(1-\\alpha_{t-1})}\\epsilon_{t-2}+\\sqrt{1-\\alpha_t}\\epsilon_{t-1}) &amp;= \\alpha_t(1-\\alpha_{t-1})+(1-\\alpha_{t}) \\\\ &amp;=1-\\alpha_t \\alpha_{t-1} \\end{aligned} \\] 通过多步叠加，最终数据会趋近标准高斯分布： \\[ x_T \\sim \\mathcal{N}(0, I) \\] 利用高斯链式性质，可以直接写出任意时刻 \\(t\\) 的显式表达式： \\[ q(x_t | x_0) = \\mathcal{N}\\left(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, \\, (1 - \\bar{\\alpha_t})I \\right) \\] 这意味着我们可以一次性将任意样本 \\(x_0\\) 加噪为第 \\(t\\) 步状态 \\(x_t\\)，无需逐步生成。 之后在反向过程中，从纯噪声 \\(x_T \\sim \\mathcal{N}(0, I)\\) 开始，学习反向马尔可夫链： \\[ p_\\theta(x_{t-1} | x_t) = \\mathcal{N}\\left(x_{t-1}; \\mu_\\theta(x_t, t), \\, \\Sigma_\\theta(x_t, t)\\right) \\] 由于真实的 \\(q(x_{t-1}|x_t)\\) 不可得，我们用神经网络 \\(\\epsilon_\\theta(x_t, t)\\) 来近似噪声分布， 并将均值项重写为： \\[ \\mu_\\theta(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha_t}}} \\, \\epsilon_\\theta(x_t, t) \\right) \\] 训练目标是最小化模型预测噪声与真实噪声之间的均方误差： \\[ L(\\theta) = \\mathbb{E}_{x_0, t, \\epsilon}\\left[ \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 \\right] \\] 即让模型在任意加噪程度下，准确预测噪声成分。 实现逻辑： 随机采样时间步 \\(t\\)； 采样噪声 \\(\\epsilon \\sim \\mathcal{N}(0, I)\\)； 一次性计算加噪样本： \\[ x_t = \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1 - \\bar{\\alpha_t}}\\epsilon \\] 训练模型预测噪声： \\[ \\epsilon_\\theta(x_t, t) \\approx \\epsilon \\] 最小化预测误差。 训练完成后，从标准高斯噪声开始逆向去噪： \\[ x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha_t}}} \\epsilon_\\theta(x_t, t) \\right) + \\sigma_t z, \\quad z \\sim \\mathcal{N}(0, I) \\] 循环从 \\(t = T\\) 到 \\(t = 1\\)，即可生成新的样本 \\(x_0\\)。 10.9.2 示例 对嵌入时间步\\(t\\)，让模型知道此时处于第几个时间步（噪声强度如何） 若是单纯的整数，神经网络不会将其视作连续变量，无法通过梯度下降学习到时间变化趋势 import torch import torch.nn as nn import torch.nn.functional as F import matplotlib.pyplot as plt # ======================== # 1️⃣ 定义超参数 # ======================== device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; T = 1000 # 扩散步数 beta_start, beta_end = 1e-4, 0.02 betas = torch.linspace(beta_start, beta_end, T).to(device) alphas = 1.0 - betas alpha_bars = torch.cumprod(alphas, dim=0) # ======================== # 2️⃣ 构造一个简单数据集（1维高斯分布） # ======================== N = 1000 x0 = torch.randn(N, 1).to(device) * 2 + 3 # 数据分布：N(3, 4) plt.hist(x0.cpu().numpy(), bins=50, density=True) plt.title(&quot;Training data distribution&quot;) plt.show() # ======================== # 3️⃣ 前向加噪函数 q(x_t | x_0) # ======================== def q_sample(x0, t, noise=None): if noise is None: noise = torch.randn_like(x0) sqrt_ab = torch.sqrt(alpha_bars[t]).view(-1, 1) sqrt_one_minus_ab = torch.sqrt(1 - alpha_bars[t]).view(-1, 1) return sqrt_ab * x0 + sqrt_one_minus_ab * noise # ======================== # 4️⃣ 定义去噪网络 ε_θ(x_t, t) # ======================== class DenoiseNet(nn.Module): def __init__(self): super().__init__() self.net = nn.Sequential( nn.Linear(2, 128), nn.ReLU(), nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, 1) ) def forward(self, x_t, t): # 将时间步归一化后拼接输入 t_embed = t.float().unsqueeze(1) / T x_in = torch.cat([x_t, t_embed], dim=1) return self.net(x_in) model = DenoiseNet().to(device) optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) # ======================== # 5️⃣ 训练过程 # ======================== epochs = 1000 for epoch in range(epochs): t = torch.randint(0, T, (N,), device=device) # 随机时间步 noise = torch.randn_like(x0) x_t = q_sample(x0, t, noise) noise_pred = model(x_t, t) loss = F.mse_loss(noise_pred, noise) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 100 == 0: print(f&quot;Epoch {epoch}, Loss: {loss.item():.4f}&quot;) # ======================== # 6️⃣ 采样（从纯噪声逆扩散生成） # ======================== @torch.no_grad() def p_sample(model, x_t, t): beta_t = betas[t] alpha_t = alphas[t] alpha_bar_t = alpha_bars[t] noise_pred = model(x_t, torch.tensor([t]*x_t.shape[0], device=device)) mean = (1 / torch.sqrt(alpha_t)) * ( x_t - (1 - alpha_t) / torch.sqrt(1 - alpha_bar_t) * noise_pred ) if t &gt; 0: z = torch.randn_like(x_t) else: z = 0 return mean + torch.sqrt(beta_t) * z @torch.no_grad() def sample(model, n_samples=1000): x_t = torch.randn(n_samples, 1).to(device) for t in reversed(range(T)): x_t = p_sample(model, x_t, t) return x_t samples = sample(model, n_samples=1000).cpu().numpy() # ======================== # 7️⃣ 可视化结果 # ======================== plt.figure(figsize=(8,5)) plt.hist(x0.cpu().numpy(), bins=50, density=True, alpha=0.5, label=&quot;Real data&quot;) plt.hist(samples, bins=50, density=True, alpha=0.5, label=&quot;Generated data&quot;) plt.legend() plt.title(&quot;DDPM training vs sampling result&quot;) plt.show() "],["dl_10.html", "10.10 Transformer", " 10.10 Transformer Attention Is All You Need 10.10.1 原理 在Transformer之前，自然语言处理的主流方法是RNN等循环神经网络，存在 序列依赖强 → 无法并行训练 长距离依赖弱 → 远距离词之间信息传递困难 训练慢 → 不适合大规模语料 等问题。而Transformer完全使用注意力机制来处理序列，从而解决了这些问题。 图 10.6: Transformer 10.10.1.1 整体结构 Transformer采用编码器-解码器架构。 原始论文中采用6层编码器和6层解码器 编码器 多头注意力机制 前馈神经网络 残差连接+层归一化 解码器 掩码多头自注意力机制 多头自注意力机制 前馈神经网络 残差连接+层归一化 10.10.1.2 注意力机制 自注意力机制 \\[ \\begin{gather} Q = XW_Q, \\quad K = XW_K, \\quad V=XW_V \\\\ \\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V \\end{gather} \\] 自注意力机制中，Q，K，V都是由X线性变换得到。 Q，K，V，X的每一行都是一个词的词向量 Q：Query，当前位置元素的查询请求 K：Key，其他词提供什么样的信息，用于计算与Q的相似度 V：Value，最终实际提供的信息内容 也就是说，\\(QK^T\\)是为了计算Query向量与Key向量之间的相似度，并通过softmax进行归一化，从而得到权重矩阵（行和为1），与V相乘得到差异化的信息（而不是对每个位置一视同仁）。 \\(QK^T\\)的点积结果与维度相关，除以\\(d_k\\)可将方差控制为1，有效避免因数值较大导致softmax过度关注某个位置而忽略其他位置 多头注意力机制 而多头注意力机制则是多个自注意力机制的叠加。 \\[ \\begin{gather} \\text{head}_i = \\text{Attention}(Q_i, K_i, V_i) \\\\ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)W^O \\end{gather} \\] 将每个自注意力机制的输出拼接在一起，再通过\\(W^O\\)重新映射回与X相同的维度。 掩码注意力机制 在解码器部分，当捕捉序列的上下文信息时，我们是没法看到当前元素之后的其他元素信息，因此在计算注意力得分前需要构造Mask矩阵来掩盖当前元素之后的元素仅在当前元素及之前元素上计算注意力得分。 交叉注意力机制 在解码器部分，为了获取编码器输出的上下文信息，交叉注意力机制中设置Q为解码器第一个注意力机制的输出，而K和V都设置为编码器输出的隐状态向量，这样就能获取到与当前序列相关的编码器部分的上下文信息。 10.10.1.3 位置编码 Transformer不像RNN等循环神经网络那样，能够自然而然的体现出先后顺序。为了捕捉元素的先后顺序关系，Transformer采用位置编码来提取位置信息。 \\[ PE_{pos,2i} = \\sin{(\\frac{pos}{10000^{2i/d}})} \\\\ PE_{pos,2i+1} = \\cos{(\\frac{pos}{10000^{2i/d}})} \\] 对于第pos位置的元素，其位置编码为交替的sin和cos函数值，并且维度分为两两一对，每对中的频率是一样的 这样设置能够在较低维度（频率高变化快）捕捉局部位置信息，在较高维度（频率低变化慢）捕捉全局位置信息。 此外，根据正弦函数的性质，\\(PE_{pos+k}\\)能够表示为\\(PE_{pos}\\)的线性函数，也就是能够表示相对位置关系。 10.10.1.4 前馈神经网络 Transformer中的前馈神经网络采取如下形式 \\[ FFN(x) = \\max{(0,xW_1+b1)}W_2+b_2 \\] 两层MLP用于增加模型的非线性表达能力。 两层的维度先升后降 10.10.1.5 残差连接与层归一化 残差连接 残差连接即 \\[ output = x + f(x) \\] 通常用于解决多层网络训练的问题，可以让网络只关注当前差异的部分。 好处： 帮助梯度顺畅流动 让每一层学习增量（差异）信息，而不是完整变换 让深层网络更加稳定 层归一化 层归一化(LN)即 \\[ LN(x) = \\frac{x-\\mu}{\\sigma}\\cdot \\gamma+\\beta \\] 是对每个token的特征维度进行归一化处理。 而批归一化(BN)则是对每个特征在样本维度进行归一化处理。 由于句子有长有短，不适合BN "],["dl_11.html", "10.11 BERT", " 10.11 BERT BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 10.11.1 原理 简而言之，BERT在预训练阶段提取通用的语义特征，再在下游任务处根据需要进行微调。 10.11.1.1 整体结构 BERT只用了Transformer的Encoder部分，并且堆叠多个Encoder。 \\[ x_l&#39; = x_l + \\text{Attention}(\\text{LN}(x_l)) \\\\ x_{l+1} = x_l&#39;+\\text{FFN}(\\text{LN}(x_l&#39;)) \\] Encoder中的注意力机制能够看到上下文信息，体现“双向” 10.11.1.2 模型输入 BERT相较于Transformer，除了词嵌入和位置编码外，还有Segment Embedding。 Token Embedding 采用WordPiece的方法进行词嵌入。 Position Embedding 可学习的绝对位置向量。 对序列长度有要求，不能外推更长的序列 Segment Embedding 可学习的向量，用来标记句子属于第一句还是第二句。 最后模型输入表示为 \\[ E= E_{token}+E_{position}+E_{segment} \\] 图 10.7: input 10.11.1.3 预训练 实际中有大量无标签文本，要从中学习到通用的语义特征，就要采取子监督学习的方法。 Masked Language Modeling 随机抽取一部分token记为[Mask]，对其采取三种策略：(1)记为[Mask]；(2)替换为其他随机词；(3)保留为原词。 这样做是因为在推理时不会存在[Mask]，去掉一部分[Mask]有助于训练-推理保持一致性。 MLM任务就是预测[Mask]处的原始token，即使它是随机token还是原token Mask时能够看到上下文信息，也体现了“双向” Next Sentence Prediction 输入A+B的句子，其中B有50%是真实的A的下一个句子，50%是随机的句子。并使用特殊的隐状态C用于NSP任务的分类预测。 10.11.1.4 微调 根据下游任务的不同，添加对应的输出层与损失函数即可。在实际训练时反向传播BERT与任务层。 以文本的情感分类为例，取[CLS]的隐状态向量，进行一层线性变换即可通过softmax进行分类。 \\[ \\begin{gather} h_{CLS} = H_{[CLS]} \\\\ \\hat{y} = \\text{softmax}(Wh_{CLS}+b) \\end{gather} \\] "],["dl_12.html", "10.12 GPT", " 10.12 GPT 10.12.1 GPT-1 Improving Language Understanding by Generative Pre-Training 图 10.8: GPT-1 10.12.1.1 整体结构 仅采用Transformer的Decoder部分。由于没有编码器部分，所以去除了其中的交叉注意力机制。 10.12.1.2 模型输入 根据下游任务的不同有不同的输入方式，但基本都包含Token Embedding和Position Embedding。 此处的PE采取可学习的绝对位置编码 10.12.1.3 预训练 GPT在无标记文本上进行预训练，采用标准语言模型的目标函数，即似然函数，根据给定的前k个词预测下一个词。 \\[ L_1 = \\sum_i \\log{P(u_i|u_{i-k}, \\cdots,u_{i-1};\\Theta)} \\] 其网络结构为 \\[ \\begin{gather} h_0 = UW_e+W_p \\\\ h_l = \\text{transformer_block}(h_{l-1}) \\\\ P(u) = \\text{softmax}(h_nW_e^T) \\end{gather} \\] 其中\\(W_e\\)和\\(W_p\\)分别表示词嵌入矩阵、位置编码矩阵。 输出的时候也经过\\(W_e\\)的线性变换，是为了找到\\(h_n\\)与已有词表的哪个词最为相似 10.12.1.4 微调 对于有标记的样本对x和y，每次输入序列为x，标签为y，损失函数为 \\[ L_2=\\sum_{x,y}\\log{P(y|x^1, \\dots, x^m)} \\] 则总体损失函数为 \\[ L_3 = L_2+\\lambda \\cdot L_1 \\] 其中\\(\\lambda\\)是调节参数。 不同下游任务可更换不同的输入形式，并适当调整输出结构。 10.12.2 GPT-2 GPT-2除了规模上比GPT-1更大外，主要的改变就是Zero-shot。 Zero-shot，零样本学习，指模型在没有见过该任务训练数据的情况下，仅通过自然语言指令（prompt）就能执行任务。 不用微调 例如 翻译为英文：我喜欢猫 其中“翻译为英文”就是提示词。 为什么提示词有效？ GPT采取语言自回归结构，提示词（Prompt）本身就是一种描述性自然语言上下文 预训练数据规模巨大，任务模式隐含其中 大规模模型具有任务泛化能力 提示词工程:(1)指令提示；(2)零样本提示；(3)少样本提示；(4)思维链提示；(5)角色提示 10.12.3 GPT-3 GPT-3的训练规模进一步扩大。 GPT-2基于Zero-shot理念，而GPT-3除了Zero-shot外还有Few-shot，也称之为In-context Learning。 GPT-3在预训练之后，只要给模型提示词、任务描述、几个示例，模型就能够输出相应的结果，这个过程不会进行梯度更新。 不用微调 "],["survival.html", "11 生存分析", " 11 生存分析 生存分析方法研究一个感兴趣的事件发生的时间。该事件可以是死亡、离婚、戒烟、设备故障等等。因此，“生存”二字不应与狭义上的“死亡”绑定，应当将“生存”视作一种“持续”，“死亡”则对应“事件的发生”。 "],["survival_1.html", "11.1 基本概念", " 11.1 基本概念 在生存分析中，我们最关心的数据是从某个起始点开始到一个特定事件发生所经过的时间间隔，即time to event，也称“生存时间数据”。不妨令\\(X\\)表示观测对象真实的生存时间，令\\(T\\)表示观测到的并记录下来的生存时间。在实际研究中，由于多种因素的影响，我们不一定能够获取完整的生存时间数据，即\\(X \\neq T\\)，因此有必要对数据类型进行区分，以便构建相应的模型。 11.1.1 删失数据 11.1.1.1 右删失 Type I censoring 人为预先设置右删失时间点\\(C_r\\)，则\\(T\\)的表达式为 \\[ T = \\begin{cases} X, &amp;\\text{if } X \\leq C_r \\\\ C_r, &amp;\\text{if } X \\gt C_r \\end{cases} \\tag{11.1} \\] 为方便记录，令\\(\\delta=I\\{X \\leq C_r\\}\\)，则数据对为\\((T, \\delta)\\)。 图 11.1: Type I censoring 图中红点代表目标事件在观察期内发生，蓝点代表右删失数据。 红、蓝点的含义下同 例子：就像一个水桶，我们能观测到的水量肯定小于等于水桶的容量，倘若接的水比水桶的容量还要多，那么就会溢出来，我们也就观测不到了，只会留下一个印象——水满了。 Progressive Type I censoring Progressive Type I censoring相较Type I censoring，人为预先设置了多个右删失时间点，因次也称“逐步删失”。这种设置有个好处，就是能够控制成本。 图 11.2: Progressive Type I censoring 例子：假如你一开始招募了多个志愿者参与研究。让志愿者参与到研究当中是有成本的，而你的预算是有上限的。因此，当试验进行了一段时间后，你发现你的资金不足以支持你同时对多名志愿者进行观察，于是在某天），你决定让某些志愿者退出研究，仅留下部分志愿者继续观察。当然，其中有志愿者会自然地退出研究，因为他们的目标事件已经发生了。 Generalized Type I censoring 此前的右删失数据都有相同的时间起点，而Generalized Type I censoring允许观测对象在不同时间点被纳入到研究当中，但具有统一的删失时间点。 图 11.3: Generalized Type I censoring 由于我们更加关心生存时间的长度，不在乎什么时候开始，因此图11.3可变形为下图 图 11.4: Generalized Type I censoring 注意到该图仅用蓝点表示右删失数据点，而没有统一的竖线表示\\(C_r\\)。 如果你还是想知道观测对象什么时候加入到研究中，图11.3还可变形为下图 图 11.5: Generalized Type I censoring 纵坐标记录了观测对象参与到研究中的时长，即time to event；横坐标记录了观测对象什么时候加入到研究中；最右边的黑色实线表示研究预先设定的右删失时间点。显然，对于Generalized Type I censoring数据，右删失数据点必定落在右边的黑色实线上。 例子：就像一个公司从创立到倒闭，考虑“职工辞职”这个目标事件，那么总会有人中途加入或中途离开。其中“公司创立”对应着研究开始，“公司倒闭”对应着删失时间点\\(C_r\\)。而time to event对应的就是该职工在公司工作的时长，也就是我们感兴趣的“资历”，而不在乎他什么时候加入到这个公司。 Type II censoring Type II相较Type I并没有直接设置右删失时间点，而是预先设置有\\(r\\)个观测对象发生目标事件，将剩余的尚未发生目标事件的观测对象都记为右删失数据。也就是说Type II censoring的右删失时间点与次序统计量\\(T_{(r)}\\)有关。这种设置能够有效地节约时间和控制成本。 若共有5个观测对象，设置\\(r=3\\)，则如下图所示 图 11.6: Type II censoring 例子：就像在一场选拔性比赛中，看谁能最快任务。现在共有100位参赛者，需要筛选出3名种子选手。于是当季军产生时这个比赛就已经结束了。 Progressive Type II censoring Type II表示该类型数据需要预先设置参数\\(r\\)，Progressive表示需要预先设置多个\\(r\\)。考虑两次删失的情形，我们预先设置了\\(r_1\\)和\\(r_2\\)，当总共\\(n\\)个观测对象中有\\(r_1\\)个观测对象率先发生了目标事件，那么这\\(r_1\\)个观测对象的time to event将会被记录下来，同时出于对成本的考虑，我们还会将\\(n_1-r_1\\)个观测对象移除研究，继续观察剩余的\\(n-n_1\\)个观测对象，直到又有\\(r_2\\)个观测对象发生目标事件，此时则有\\(n-n_1-r_2\\)个观测对象被记录为右删失数据。也就是说，Progressive Type II censoring的删失时间点与次序统计量\\(T_{(r_1)}\\)、\\(T_{(n_1+r_2)}\\)有关。 务必与Progressive Type I censoring联系起来 对于10个观测对象，我们设置\\(r_1=r_2=2\\)，同时出于成本的考虑，在出现\\(r_1\\)个目标事件时主动去除2个观测对象，则剩下6个观测对象进入到第二轮中。如下所示 图 11.7: Progressive Type II censoring 例子：想不出了。 Random censoring 出于某些原因，部分观测对象可能会观测不到，也就不知道其具体的生存时间，于是被记为右删失数据。 可能的原因： 观测对象失联，丢失了 观察对象终止于其他事件 … 图 11.8: Random censoring 例子：例如我们对“肺癌”感兴趣，但患者还患有其他疾病，可能因其他疾病而死亡，此时也算是右删失。 11.1.1.2 左删失 对于左删失时间点\\(C_l\\)，则\\(T\\)的表达式为 \\[ T = \\begin{cases} C_l, &amp;\\text{if } X \\lt C_l \\\\ X, &amp;\\text{if } X \\geq C_l \\end{cases} \\tag{11.2} \\] 为方便记录，令\\(\\varepsilon=I\\{X\\geq C_l\\}\\)，则数据对为\\((T,\\varepsilon)\\) 例子：考虑疾病的发生，当一个人进医院进行体检时，若检测出患有某种疾病，则肯定是在体检之前就已经染上了疾病，但不知道是什么时候染上的，那么在医院体检的时间点就是左删失时间点\\(C_l\\)。 11.1.1.3 双删失 根据右删失和左删失的概念，考虑研究中可能同时存在右删失数据和左删失数据，即双删失。 双删失更像是一种现象（右删失与左删失同时在某些研究中出现的现象），而不是数据类型 记数据对为\\((T, \\delta)\\)，其中\\(T=max(min(X, C_r), C_l)\\)。\\(\\delta\\)定义为\\(\\delta=I\\{X \\leq C_r\\} - 2I\\{X \\lt C_l\\}\\)，当\\(\\delta=1\\)时，表示正常的生存时间，当\\(\\delta=0\\)时，表示右删失数据，当\\(\\delta=-1\\)时，表示左删失数据。 如果左删失的定义没错的话，感觉应该是\\(X \\lt C_l\\)而不是\\(X \\leq C_l\\) 例子：假如你问某人什么时候开始打的羽毛球，你可能得到的回答有：1.我20年11月13号开始打的；2.不记得了，至少大学前就开始打了；3.我还没打过。这三个回答分别对应正常的生存时间、左删失数据和右删失数据（还没打过说明目标事件尚未发生，以后可能会打，即为右删失）。 11.1.1.4 区间删失 对于观测对象\\(i\\)，只记得\\(X\\)是落在区间\\((L_i,R_i]\\)中，但不知道具体的时间点，此时即为“区间删失”。 区间删失是右删失、左删失的一般化。例如右删失可表示为\\((C_r,+\\infty)\\)，左删失可表示为\\((0,C_l)\\) 这里貌似也涉及到左删失的定义问题，别再过分纠结区间的开闭问题了 例子：在测核酸的时候，第一次为阴性，第二次为阳性，说明是在第一次和第二次检测之间感染的。 11.1.2 截断数据 对于删失数据，虽然我们不知道具体的生存时间，但至少我们是接触到了观测对象，并且掌握了生存时间的部分信息（可能大于或小于某个值，或者在某个区间内）。而对于截断数据，研究对象并没有进入到我们的观察窗口，更谈不上其携带的生存时间信息。 设置一个观察窗口\\((Y_L,Y_R)\\)，当\\(Y_R=+\\infty\\)时，只有\\(X \\gt Y_L\\)的研究对象才能被纳入到我们的研究中；当\\(Y_L=0\\)时，只有\\(X \\leq Y_R\\)的研究对象才能被纳入到我们的研究中。也就是说，进入到我们的研究中是需要门槛的、有条件的，而且这种条件往往与系统性原因有关（例如研究设计）。 例子1：如果要研究老人的预期寿命，并在退休中心开展调查。该研究忽略了那些年龄未到退休中心准入门槛的老人，但这些老人也应该属于本研究的范畴之中。这属于左截断数据。 例子2：利用天文望远镜观测天体。100年前观测到的天体数量和如今能观测到的天体数量绝对是不一样的。只是以前设备落后，精度有限，观测不到那么多的天体。因此这属于右截断数据。 11.1.3 函数 生存函数 记\\(X\\)为代表生存时间的随机变量，则其密度函数\\(f(x)\\)为 \\[ f(x)=\\lim_{\\Delta x \\rightarrow 0} \\frac{P(x \\leq X \\leq x+\\Delta x)}{\\Delta x} \\tag{11.3} \\] 记其累积分布函数为 \\[ F(x)=P(X \\leq x) \\tag{11.4} \\] 定义生存函数为 \\[ S(x)=P(X \\gt x)=1-F(x)=\\int_x^\\infty f(t)dt \\tag{11.5} \\] \\(T \\gt t\\)具有“生存”的意味 则 \\[ f(x)=-\\frac{dS(x)}{dx} \\tag{11.6} \\] 当\\(X\\)是离散随机变量时，记\\(P(X=x_j)=p(x_j),j=1,2,\\dots\\)。此时的生存函数为 \\[ S(x)=P(X \\gt x)=\\sum_{x_j \\gt x}p(x_j) \\tag{11.7} \\] 风险函数 \\[ \\begin{aligned} b(x)&amp;=\\lim_{\\Delta x \\rightarrow 0}\\frac{P(x \\leq X \\leq x+\\Delta x \\mid X \\geq x)}{\\Delta x} \\\\ &amp;= \\lim_{\\Delta x \\rightarrow 0}\\frac{F(x+\\Delta x)-F(x)}{(1-F(x))\\Delta x} \\\\ &amp;= \\frac{f(x)}{S(x)} \\\\ &amp;= \\frac{d}{dx}(-\\ln S(x)) \\end{aligned} \\tag{11.8} \\] 这种导函数与原函数在分子分母的情况可以考虑为对数函数求导 由定义可知，风险函数代表了目标事件的即时发生率。 根据风险函数可得累积风险函数 \\[ H(x)=\\int_0^x b(u)du=-\\ln S(x) \\tag{11.9} \\] 则 \\[ S(x)=\\exp\\{-H(x)\\}=\\exp\\{-\\int_0^x b(u)du\\} \\tag{11.10} \\] 当\\(X\\)是离散随机变量时，则 \\[ b(x_j)=P(x_j \\leq X \\lt x_{j+1}|X \\geq x_j)=\\frac{p(x_j)}{S(x_{j-1})},\\quad j=1,2,\\dots \\tag{11.11} \\] 注意\\(S(x)=P(X \\gt x)\\)、\\(S(x_0)=1\\)、\\(p(x_j)=S(x_{j-1})-S(x_j)\\)，则 \\[ b(x_j)=1-\\frac{S(x_j)}{S(x_{j-1})} \\tag{11.12} \\] 则 \\[ \\begin{aligned} S(x)&amp;=\\prod_{x_j \\leq x} \\frac{S(x_j)}{S(x_{j-1})} \\\\ &amp;=\\prod_{x_j \\leq x}(1-(1-\\frac{S(x_j)}{S(x_{j-1})})) \\\\ &amp;=\\prod_{x_j \\leq x}(1-b(x_j)) \\end{aligned} \\tag{11.13} \\] 平均剩余寿命 \\[ \\begin{aligned} mrl(x)=E(X-x|X \\gt x)&amp;=\\int_x^\\infty(t-x)f(t|X \\gt x)dt \\\\ &amp;=\\int_x^\\infty\\frac{(t-x)f(t)}{S(x)}dt \\\\ &amp;=\\frac{\\int_x^\\infty(x-t)dS(t)}{S(x)} \\\\ &amp;=\\frac{S(t)(x-t)|^\\infty_x+\\int_x^\\infty S(t)dt}{S(x)} \\\\ &amp;=\\frac{\\int_x^\\infty S(t)dt}{S(x)} \\end{aligned} \\tag{11.14} \\] 注意\\(dS(x)=-f(x)dx\\) 11.1.4 似然函数 对于完整的生存时间，其密度函数为\\(f(x)\\)。而对于删失数据和截断数据，则有另外的表达。 \\[ \\begin{aligned} Right-censored &amp;: S(C_r) \\\\ Left-censored &amp;: 1-S(C_l) \\\\ Interval-censored &amp;: S(L)-S(R) \\\\ Left-truncated &amp;: f(x)/S(Y_L) \\\\ Right-truncated &amp;: f(x)/(1-S(Y_R)) \\\\ Interval-truncated &amp;: f(x)/(S(Y_L)-S(Y_R)) \\end{aligned} \\tag{11.15} \\] 注意\\(f/S\\)的形式是条件概率的含义 以删失数据为例，似然函数可以表示为 \\[ L \\propto \\prod_{i \\in D}f(x_i)\\prod_{i \\in R}S(C_r)\\prod_{i \\in L}(1-S(C_l))\\prod_{i \\in I}[S(L_i)-S(R_i)] \\tag{11.16} \\] 其中\\(D\\)、\\(R\\)、\\(L\\)、\\(I\\)都是对应的数据类型集合。 根据数据类型写出对应的元素即可。下面举个例子。 假设\\(X_i \\stackrel{i.i.d}{\\sim} Exp(\\lambda)\\)，观测数据为\\((T_i,\\delta_i)\\)，其中\\(\\delta_i=I(T_i \\leq C_i)\\)表示是否为右删失数据。则似然函数为 \\[ L(\\lambda)=\\prod_{i=1}^n \\{\\lambda \\exp(-\\lambda T_i)\\}^{\\delta_i}\\{\\exp (-\\lambda T_i)\\}^{(1-\\delta_i)} \\tag{11.17} \\] 可得极大似然估计为 \\[ \\hat \\lambda=\\frac{\\sum_{i=1}^n \\delta_i}{\\sum_{i=1}^nX_i} \\tag{11.18} \\] "],["survival_2.html", "11.2 参数估计", " 11.2 参数估计 这里主要对右删失数据进行讨论，以此来对第6.1.3节中的函数进行估计。因此，数据结构为\\((T,\\delta)\\)，并假设各个对象的删失时间与其time to event无关。 假如有\\(n\\)个观测样本，并且允许重复数据(ties)的出现，那么有\\(D\\)个不同的time to event\\(t_1 \\lt t_2 \\lt \\cdots \\lt t_D\\)。在\\(t_i\\)上有\\(d_i\\)个\\(\\delta=1\\)的个体，并且此时还有\\(Y_i=\\sum_{j=1}^nI(X_j \\geq t_i)=|R_i|\\)个个体存在，则此时的风险函数可估计为\\(b_i(x)=d_i/Y_i\\)。 \\(R_i\\)表示\\(t_i\\)时刻的风险集合，即在时刻\\(t_i\\)时刻还存活的个体 例子如下所示 表 11.1: 原始数据 subject time delta a 2 1 b 5 0 c 3 0 d 4 1 e 2 1 f 3 1 表 11.1: 排序后的数据 subject time delta a 2 1 e 2 1 c 3 0 f 3 1 d 4 1 b 5 0 \\(t_1=2;\\; d_1=2; \\; R(t_1)=\\{a,e,f,c,d,b\\}; \\; Y_1=6\\) \\(t_2=3;\\; d_2=1; \\; R(t_2)=\\{f,c,d,b\\}; \\; Y_2=4\\) \\(t_3=4;\\; d_3=1; \\; R(t_3)=\\{d,b\\}; \\; Y_3=2\\) 11.2.1 KM估计 Kaplan和Meier提出了KM估计，也称Product-Limit estimator。 根据式(11.13)，KM估计给出了生存函数的估计 \\[ \\hat S(t)=\\begin{cases} 1, &amp; \\textrm{if} \\; t \\lt t_1 \\\\ \\prod_{t_i \\leq t}[1-\\frac{d_i}{y_i}], &amp; \\textrm{if} \\; t_1 \\leq t \\end{cases} \\tag{11.19} \\] 注意到，当最大的观测值是完整生存时间时，\\(\\hat S(t)=0\\)；当最大的观测值是右删失时，\\(\\hat S(t)\\)则为一个常数，此时就很难定义取值大于该点时对应的生存函数。 对此，当\\(t \\gt t_{max}\\)时，有如下三种备选方法： \\(\\hat S(t)=0\\) \\(\\hat S(t)=\\hat S(t_{max})\\) \\(\\hat S(t)=\\exp\\{\\frac{t\\ln[\\hat S(t_{max})]}{t_{max}}\\}\\) 第三种方法对应的生存函数是衰减的并趋于零 \\(\\hat S(t)\\)的方差估计为 \\[ \\hat{V}[\\hat S(t)]=\\{\\hat S(t)\\}^2 \\sum_{t_i \\leq t} \\frac{d_i}{Y_i(Y_i-d_i)} \\tag{11.20} \\] 可以直接记 考虑式(11.9)，有 \\[ \\hat H(t)=-\\ln [\\hat S(t)] \\tag{11.21} \\] 在计算\\(\\hat V[\\hat H(t)]\\)前介绍\\(\\delta\\)方法： 已知:\\(\\hat \\beta - \\beta_0 \\stackrel{d}{\\rightarrow} N(0, V(\\hat \\beta))\\)，若\\(f(x)\\)是连续的且一阶导不为0，求\\(f(\\hat \\beta)-f(\\beta_0)\\)的渐近分布。 首先对\\(f(\\hat \\beta)\\)进行一阶泰勒展开 \\[ \\begin{aligned} f(\\hat \\beta) &amp;\\approx f(\\beta_0)+f&#39;(\\beta_0)(\\hat \\beta-\\beta_0) \\\\ f(\\hat \\beta)-f(\\beta_0) &amp; \\approx f&#39;(\\beta_0)(\\hat \\beta-\\beta_0) \\end{aligned} \\tag{11.22} \\] 则 \\[ \\begin{aligned} V(f(\\hat \\beta)-f(\\beta_0))&amp;\\approx V(f&#39;(\\beta_0)(\\hat \\beta-\\beta_0)) \\\\ &amp;\\approx \\{f&#39;(\\beta_0)\\}^2V(\\hat \\beta) \\end{aligned} \\tag{11.23} \\] 故 \\[ f(\\hat \\beta)-f(\\beta_0) \\stackrel{d}{\\rightarrow} N(0, \\{f&#39;(\\beta_0)\\}^2V(\\hat \\beta)) \\tag{11.24} \\] 则 \\[ \\hat V(\\hat H(t))=\\frac{1}{\\{\\hat S(t)\\}^2}\\hat{V}[\\hat S(t)]= \\sum_{t_i \\leq t} \\frac{d_i}{Y_i(Y_i-d_i)} \\tag{11.25} \\] 11.2.2 NA估计 NA估计由Nelson和Aalen提出，该方法首先给出了累积风险函数的估计 \\[ \\tilde H(t)=\\begin{cases} 0, &amp; \\textrm{if} \\; t \\lt t_1 \\\\ \\sum_{t_i \\leq t} \\frac{d_i}{Y_i}, &amp;\\textrm{if} \\; t_1 \\leq t \\end{cases} \\tag{11.26} \\] \\(\\hat b(t_i)=d_i/Y_i\\) 给出该估计量的方差 \\[ \\tilde V(\\tilde H(t))=\\sum_{t_i \\leq t} \\frac{d_i}{Y_i^2} \\tag{11.27} \\] 根据式(11.10)，有 \\[ \\tilde S(t)=\\exp[- \\tilde H(t)] \\tag{11.28} \\] 由\\(\\delta\\)方法可知 \\[ \\tilde V(\\tilde S(t))=\\{-\\exp[-\\tilde H(t)]\\}^2 \\tilde V(\\tilde H(t))=\\{\\tilde S(t)\\}^2\\sum_{t_i \\leq t}\\frac{d_i}{Y_i^2} \\tag{11.29} \\] "],["survival_3.html", "11.3 置信区间与置信带", " 11.3 置信区间与置信带 在这一节，根据KM估计来构建置信区间和置信带。 11.3.1 置信区间 11.3.1.1 Linear CI 记 \\[ \\sigma^2_S(t)=\\frac{\\hat V[\\hat S(t)]}{\\hat S^2(t)} \\tag{11.30} \\] 则 \\[ \\hat V[\\hat S(t)]=\\hat S(t)^2\\sigma^2_S(t) \\tag{11.31} \\] 则可得KM估计的渐近正态分布为 \\[ \\frac{\\hat S(t)-S(t)}{\\sqrt{\\hat V[\\hat S(t)]}} \\sim N(0,1) \\tag{11.32} \\] 易得\\(S(t)\\)在\\(t_0\\)时刻的\\(1-\\alpha\\)置信区间为 \\[ \\hat S(t_0) \\mp Z_{1-\\alpha/2}\\sigma_S(t_0)\\hat S(t_0) \\tag{11.33} \\] 该区间是对称区间 11.3.1.2 Log-Transformed CI 考虑累积风险函数的对数形式\\(\\ln [-\\ln \\hat S(t_0)]\\)，根据\\(\\delta\\)方法，可得其渐近分布为 \\[ \\ln [-\\ln \\hat S(t)]-\\ln [-\\ln S(t)] \\sim N(0,\\frac{\\sigma_S^2(t)}{\\ln^2 \\hat S(t)}) \\tag{11.34} \\] 故\\(t_0\\)时刻的置信区间为 \\[ \\ln [-\\ln\\hat S(t_0)] \\mp Z_{1-\\alpha/2}(-\\sigma_S(t_0)/\\ln \\hat S(t_0)) \\tag{11.35} \\] \\(\\ln \\hat S(t_0) \\lt 0\\) 由于我们关注\\(S(t_0)\\)的置信区间，则需要对该置信区间进行转化，转化后的置信区间为 \\[ \\begin{array}{c} \\ln [-\\ln \\hat S(t_0)]-Z(-\\frac{\\sigma_S(t_0)}{\\ln \\hat S(t_0)}) \\leq \\ln [-\\ln S(t_0)] \\leq \\ln [-\\ln \\hat S(t_0)]+Z(-\\frac{\\sigma_S(t_0)}{\\ln \\hat S(t_0)}) \\\\ -\\ln \\hat S(t_0) \\cdot \\exp\\{\\frac{Z\\sigma_S(t_0)}{\\ln \\hat S(t_0)}\\} \\leq -\\ln S(t_0) \\leq -\\ln \\hat S(t_0) \\cdot \\exp\\{-\\frac{Z\\sigma_S(t_0)}{\\ln \\hat S(t_0)}\\} \\\\ -\\ln \\hat S(t_0) \\cdot \\theta \\leq -\\ln S(t_0) \\leq -\\ln \\hat S(t_0) \\cdot \\theta^{-1} \\\\ \\ln \\hat S(t_0)^\\theta \\geq \\ln S(t_0) \\geq \\ln \\hat S(t_0)^{\\theta^{-1}} \\\\ \\hat S(t_0)^{\\theta^{-1}} \\leq S(t_0) \\leq \\hat S(t_0)^\\theta \\end{array} \\tag{11.36} \\] 其中\\(\\theta=\\exp\\{\\frac{Z\\sigma_S(t_0)}{\\ln \\hat S(t_0)}\\}\\)。 11.3.1.3 Arcsine-Square Root Transformed CI 考虑\\(\\arcsin\\{\\hat S^{\\frac{1}{2}}(x)\\}\\)，则其渐近分布为 \\[ \\arcsin\\{\\hat S^{\\frac{1}{2}}(t)\\}-\\arcsin\\{S^{\\frac{1}{2}}(t)\\} \\sim N(0,\\frac{\\sigma^2_S(t)\\hat S(t)}{4(1-\\hat S(t))}) \\tag{11.37} \\] 同样也可将置信区间进行转化 \\[ \\begin{array}{c} L \\leq \\arcsin\\{S^{\\frac{1}{2}}(t_0)\\} \\leq R \\\\ \\sin \\{\\max [0,L]\\} \\leq S^{\\frac{1}{2}}(t_0) \\leq \\sin \\{\\min [\\frac{\\pi}{2},R]\\} \\\\ \\sin^2 \\{\\max [0,L]\\} \\leq S(t_0) \\leq \\sin^2 \\{\\min [\\frac{\\pi}{2},R]\\} \\end{array} \\tag{11.38} \\] 注意\\(\\arcsin x\\)的定义域与值域 其中 \\[ \\begin{array}{c} L=\\arcsin\\{\\hat S^{\\frac{1}{2}}(t_0)\\}-\\frac{Z\\sigma_S(t_0)}{2}\\sqrt \\frac{\\hat S(t_0)}{1-\\hat S(t_0)} \\\\ R=\\arcsin\\{\\hat S^{\\frac{1}{2}}(t_0)\\}+\\frac{Z\\sigma_S(t_0)}{2}\\sqrt \\frac{\\hat S(t_0)}{1-\\hat S(t_0)} \\end{array} \\tag{11.39} \\] 11.3.2 置信带 置信区间给出了\\(t\\)在某点处的一个\\(1-\\alpha\\)区间。而置信带则给出了\\(t\\)在某个区间内的一个\\(1-\\alpha\\)区间，即\\(1-\\alpha=P(L(t)\\leq S(t) \\leq U(t)), \\; \\forall t \\in (t_L, t_U)\\)，则称区间\\([L(t),U(t)]\\)为置信带。 由于置信带需要查表才能得到，故不多做介绍，仅了解即可。 11.3.3 平均生存时间的置信区间 平均生存时间定义为 \\[ \\mu = E(x)=\\int_0^\\infty tf(t)dt=\\int_0^\\infty S(t)dt \\tag{11.40} \\] 利用分部积分即可转化为\\(S(t)\\) 故区间\\([0, \\tau]\\)上的平均生存时间估计为 \\[ \\hat \\mu_\\tau=\\int_0^\\tau \\hat S(t)dt \\tag{11.41} \\] 其中\\(\\tau\\)是\\(t_{max}\\)或最大的删失值。 而\\(\\hat \\mu_\\tau\\)的方差为 \\[ \\hat V(\\hat \\mu_\\tau)=\\sum_{i=1}^D \\{[\\int_{t_i}^\\tau \\hat S(t)dt]^2\\frac{d_i}{Y_i(Y_i-d_i)}\\} \\tag{11.42} \\] 相应的置信区间为 \\[ [\\hat \\mu_\\tau-Z_{1-\\alpha/2}\\sqrt{\\hat V(\\hat \\mu_\\tau)},\\hat \\mu_\\tau+Z_{1-\\alpha/2}\\sqrt{\\hat V(\\hat \\mu_\\tau)}] \\tag{11.43} \\] 11.3.4 分位数的置信区间 生存函数的分位数定义为\\(x_p = \\inf\\{t:S(t) \\leq 1-p \\}\\)，则\\(x_p\\)的km估计为\\(\\hat x_p=\\inf\\{t:\\hat S(t) \\leq 1-p \\}\\)。 进一步的，Brookmeyer和Crowley给出了分位数置信区间的三种形式。 \\[ \\begin{array}{c} -Z_{1-\\alpha/2} \\leq \\frac{\\hat S(t)-(1-p)}{\\hat V^{1/2}[\\hat S(t)]} \\leq Z_{1-\\alpha/2} \\\\ -Z_{1-\\alpha/2} \\leq \\frac{\\{\\ln [-\\ln (\\hat S(t))]-\\ln [-\\ln(1-p)]\\}\\hat S(t)\\ln (\\hat S(t))}{\\hat V^{1/2}[\\hat S(t)]} \\leq Z_{1-\\alpha/2} \\\\ -Z_{1-\\alpha/2} \\leq \\frac{2[arcsin(\\hat S^{\\frac{1}{2}}(t))-arcsin(1-p)^{\\frac{1}{2}}][\\hat S(t)(1-\\hat S(t))]^{1/2}}{\\hat V^{1/2}[\\hat S(t)]} \\leq Z_{1-\\alpha/2} \\end{array} \\tag{11.44} \\] 11.3.5 左截断数据的置信区间 不妨先举个例子 在这个例子中，A、B都是在一开始就进入到研究中的，而C是中途加进来的。也就是说，我们一开始是观测不到C的。因此在原有的计算规则下，我们有如下结果 \\(t_1=2;\\; d_1=1; \\; R(t_1)=\\{A,B\\}; \\; Y_1=2; \\; \\hat S(t_1)=1/2\\) \\(t_2=3;\\; d_2=1; \\; R(t_2)=\\{B\\}; \\; Y_2=1; \\; \\hat S(t_2)=0\\) \\(t_3=6;\\; d_3=1; \\; R(t_3)=\\{C\\}; \\; Y_3=1; \\; \\hat S(t_3)=0\\) 明明C能够生存到\\(t_2\\)之后，只是之前没有观测到，但生存函数的估计却在\\(t_2\\)时刻估计为0，存在一些问题。 鉴于此，我们需要拓展\\(Y_i\\)的定义：在\\(t_i\\)时刻前进入到研究中并在\\(t_i\\)时刻仍存活的个体，或者在\\(t_i\\)时刻之后进入到研究中并且其生存时长大于等于\\(t_i\\)的个体都会被计入。 \\(t_1=2;\\; d_1=1; \\; R(t_1)=\\{A,B,C\\}; \\; Y_1=3; \\; \\hat S(t_1)=2/3\\) \\(t_2=3;\\; d_2=1; \\; R(t_2)=\\{B,C\\}; \\; Y_2=2; \\; \\hat S(t_2)=1/3\\) \\(t_3=6;\\; d_3=1; \\; R(t_3)=\\{C\\}; \\; Y_3=1; \\; \\hat S(t_3)=0\\) 这样，对于生存函数的估计我们就得考虑条件概率，即\\(P(X \\gt t |X \\geq L)=S(t)/S(L)\\)。当然，当对象一开始就纳入到研究中，那么\\(L=0\\)，按以前的操作来就行。这里只是将研究工具拓展到左截断数据。 \\[ \\hat S_L(t)=S(t)/S(L)=\\prod_{L \\leq t_i \\leq t}[1-\\frac{d_i}{Y_i}] \\tag{11.45} \\] 左截断数据需要对改变生存函数的估计，其他构造置信区间的操作同正常情况。 "],["survival_4.html", "11.4 假设检验", " 11.4 假设检验 11.4.1 单样本检验 在时间段\\(0 &lt; t\\leq \\tau\\)内，考虑如下假设检验 \\[ \\begin{aligned} &amp;H_0: b(t)=b_0(t) \\quad \\textrm{for all} \\; t \\\\ &amp;H_1: b(t) \\neq b_0(t) \\quad \\textrm{for some} \\; t \\end{aligned} \\] \\(\\tau\\)为最大的非右删失生存时间 一个自然的想法就是看看\\(b(t)\\)与\\(b_0(t)\\)之间的差距大不大，如果较大的话就说明应当拒绝原假设。于是给出下面的检验统计量 \\[ \\begin{aligned} Z(\\tau)&amp;=O(\\tau)-E(\\tau) \\\\ &amp;=\\sum_{i=1}^D W(t_i)\\frac{d_i}{Y(t_i)}-\\int_0^\\tau W(t)b_0(t)dt \\end{aligned} \\tag{11.46} \\] 其中\\(O(\\tau)\\)为观测到的加权累积风险函数，\\(E(\\tau)\\)为原假设下的加权累积风险函数，\\(W(t)\\)为权重函数。特别地，当\\(Y(t)=0\\)时\\(W(t)=0\\)。 在原假设下，\\(Z(\\tau)\\)的方差为 \\[ V[Z(\\tau)]=\\int_0^\\tau W^2(t)\\frac{b_0(t)}{Y(t)}dt \\tag{11.47} \\] 在大样本下，当\\(H_0\\)为真时，统计量\\(\\frac{Z(\\tau)^2}{V[Z(\\tau)]}\\)服从卡方分布。 统计量\\(\\frac{Z(\\tau)}{V[Z(\\tau)]^{1/2}}\\)被用于进行\\(b(t)&gt;b_0(t)\\)的单侧检验。当原假设为真且样本量较大时，该统计量就服从标准正态分布。 为什么只能右侧？ Log-Rank Test 设置权重函数为\\(W(t)=Y(t)\\)。不失一般性，考虑可能存在的左截断数据，记\\(T_j\\)为第\\(j\\)个对象在研究中发生目标事件时的时间，\\(L_j\\)为第\\(j\\)个对象进入到研究时的时间。 那么\\(O(\\tau)=\\sum_{i=1}^D d_i\\)表示在\\(\\tau\\)时刻及之前观测到的事件总数，并且有 \\[ \\begin{aligned} E(\\tau) &amp;= \\int_0^\\tau W(t)b_0(t)dt \\\\ &amp;= \\int_0^\\tau Y(t)b_0(t)dt \\\\ &amp;= \\int_0^\\tau \\sum_{j=1}^n I(X_j \\geq t)b_0(t)dt \\\\ &amp;= \\sum_{j=1}^n \\int_{L_j}^{T_j} b_0(t)dt \\\\ &amp;= \\sum_{j=1}^n [H_0(T_j)-H_0(L_j)] \\end{aligned} \\tag{11.48} \\] 对于第\\(j\\)个对象，他不一定存活到\\(\\tau\\)时刻，在\\(T_j\\)到\\(\\tau\\)的时间段里是不存在该对象的，因此直接取\\(T_j\\)。若不是左截断数据，则\\(L_j\\)=0。 同理可得 \\[ V[Z(\\tau)]=\\int_0^\\tau W^2(t)\\frac{b_0(t)}{Y(t)}dt=\\int_0^\\tau W(t)b_0(t)dt=E(\\tau) \\tag{11.49} \\] Fleming-Harrington Test Harrington和Fleming提出了如下权重函数族 \\[ W_{HF}(t)=Y(t)S_0(t)^p [1-S_0(t)]^q, \\;\\; p \\geq 0 \\;\\; \\textrm{and} \\;\\; q \\geq 0 \\tag{11.50} \\] 其中\\(S_0(t)=\\exp[-H_0(t)]\\)是原假设下的生存函数。注意到生存函数是一个递减的函数，因此当\\(p&gt;q\\)时会给予前面的数据较大的权重，当\\(p&lt;q\\)时会给予后面的数据较大的权重，当\\(p=q\\neq0\\)时前后一视同仁，当\\(p=q=0\\)时就是log-rank权重。 权重函数就是一个调节器，可根据实际需要来放大或缩小对应时间段的差异。 11.4.2 多样本检验 对于\\(K(K \\geq 2)\\)个组别，给出如下假设 \\[ H_0: b_1(t)=b_2(t)=\\cdots=b_K(t)=b(t), \\quad \\textrm{for all } t \\leq \\tau \\\\ H_1: \\textrm{at least one of the } b_j(t) \\textrm{ is different for some } t \\leq \\tau \\] 这里的\\(\\tau\\)是各个组别中最大生存时间的最小值，即\\(\\tau = \\min\\{\\tau_1, \\tau_2, \\cdots, \\tau_K\\}\\) 数据处理流程： 混合多组样本，得到\\(t_1 \\lt t_2 \\lt \\cdots \\lt t_D\\) 记录第\\(j\\)组样本中各个时点的事件发生数，得到\\(d_{1j},\\, d_{2j} \\cdots, \\, d_{Dj}\\) 记录第\\(j\\)组样本中各个时点处于风险集的人数，得到\\(Y_{1j},\\, Y_{2j} \\cdots, \\, Y_{Dj}\\) 记录混合样本中各个时点的事件发生数，即\\(d_i=\\sum_{j=1}^K d_{ij}\\) 记录混合样本中各个时点处于风险集的人数，即\\(Y_i=\\sum_{j=1}^K Y_{ij}\\) 据此得到如下检验统计量 \\[ Z_j(\\tau)=\\sum_{i=1}^D W_j(t_i)(\\frac{d_{ij}}{Y_{ij}}-\\frac{d_i}{Y_i}), \\quad j=1, \\cdots, K \\tag{11.51} \\] 这个统计量度量了第\\(j\\)组样本的风险率和总体风险率之间的差距。如果原假设为真的话，那么对于每一组样本而言这个差距应该会很小。同样，当\\(Y_{ij}=0\\)时有\\(W_j(t_i)=0\\)。 一般，权重函数被设置为\\(W_j(t_i)=Y_{ij}W(t_i)\\)，则 \\[ Z_j(\\tau)=\\sum_{i=1}^D W(t_i)[d_{ij}-Y_{ij}(\\frac{d_i}{Y_i})], \\quad j=1, \\cdots, K \\tag{11.52} \\] \\(W_j(t_i)\\)中的\\(j\\)意味着每组有不同的权重函数；\\(W_j(t_i)=Y_{ij}W(t_i)\\)则假设各个组的权重函数有公共的部分，即\\(W(t_i)\\)，不同之处在于\\(Y_{ij}\\) 可得\\(Z_j(\\tau)\\)的方差为 \\[ \\hat \\sigma_{jj}=\\sum_{i=1}^D W(t_i)^2 \\frac{Y_{ij}}{Y_i}(1-\\frac{Y_{ij}}{Y_i})(\\frac{Y_i-d_i}{Y_i-1})d_i, \\; j=1, \\cdots, K \\tag{11.53} \\] \\(Z_j(\\tau)\\)与\\(Z_g(\\tau)\\)的协方差为 \\[ \\hat \\sigma_{jg} = -\\sum_{i=1}^D W(t_i)^2 \\frac{Y_{ij}}{Y_i}\\frac{Y_{ig}}{Y_i}(1-\\frac{Y_{ij}}{Y_i})(\\frac{Y_i-d_i}{Y_i-1})d_i, \\; g\\neq j \\tag{11.54} \\] 注意到不同组的统计量\\(Z(\\tau)\\)之间不独立，因为都用到了\\(d_i\\)和\\(Y_i\\)，而\\(d_i\\)和\\(Y_i\\)又集合了不同组的信息 因此在原假设下有 \\[ \\begin{pmatrix} Z_1(\\tau) \\\\ \\vdots \\\\ Z_K(\\tau) \\end{pmatrix} \\stackrel{d}\\longrightarrow N(0, \\Sigma) \\tag{11.55} \\] 于是我们可以对其进行标准化处理，并求其平方和，根据卡方分布进行检验。但需注意到 \\[ \\begin{aligned} \\sum_{j=1}^K Z_j(\\tau)&amp;=\\sum_{j=1}^K\\sum_{i=1}^D W(t_i)(d_{ij}-\\frac{d_iY_{ij}}{Y_i}) \\\\ &amp;=\\sum_{i=1}^D W(t_i)\\sum_{j=1}^K(d_{ij}-\\frac{d_iY_{ij}}{Y_i}) \\\\ &amp;= \\sum_{i=1}^D W(t_i)(d_i-d_i) \\\\ &amp;= 0 \\end{aligned} \\tag{11.56} \\] 因此\\(\\Sigma\\)是不满秩的，于是我们仅需其中\\(K-1\\)个统计量。方便起见，取其前\\(K-1\\)个统计量来构造卡方分布，则 \\[ \\begin{pmatrix} Z_1(\\tau),\\cdots,Z_{K-1}(\\tau) \\end{pmatrix} \\Sigma_{(-K)}^{-1}\\begin{pmatrix} Z_1(\\tau) \\\\ \\vdots \\\\ Z_{K-1}(\\tau)\\end{pmatrix} \\sim \\chi^2(K-1) \\tag{11.57} \\] 其中\\(\\Sigma_{(-K)}^{-1}\\)表示去掉第\\(K\\)个统计量后的协方差阵。 例如当\\(K=2\\)时，就可以直接选择其中一个\\(Z(\\tau)\\)根据标准正态分布进行假设检验 Log-Rank Test 当\\(W(t)=1\\)时，即为log-rank test。当K个不同群体的风险率彼此成比例时，该方法具有最佳的检测能力。 Gehan’s Test (Wilcoxon-Breslow-Gehan Test) 设置权重函数为\\(W(t_i)=Y_i\\)。不难发现，当时间越早，\\(Y_i\\)越大，对应的权重越大，也就是说该权重函数会对靠前时间段的差异会更为敏感。 需要关注目标事件发生的时间及删失时间的分布情况，但不同组的删失状况不同时，可能会导致错误的结果 Tarone-Ware test 设置权重函数为\\(W(t_i)=Y_i^{\\frac{1}{2}}\\)。和Gehan’s test类似的效果，都会给靠前时间段的差异较大的权重。 Peto-Peto test 设置权重函数为\\(W(t_i)=\\tilde S(t_i)\\)，其中\\(\\tilde S(t)=\\prod_\\limits{t_i \\leq t}(1-\\frac{d_i}{Y_i+1})\\)。由于\\(\\tilde S(t)\\)类似混合后的KM估计生存函数，因此这个权重函数是递减的，会给前面时间段较大的权重。 Modified Peto-Peto test 设置权重函数为\\(W(t_i)=\\frac{\\tilde S(t_i)Y_i}{Y_i+1}\\)。 Fleming-Harrington test 权重函数为\\(W_{p,q}(t_i)=\\hat S(t_{i-1})^p[1-\\hat S(t_{i-1})]^q\\)，其中\\(p , q \\geq 0\\)，\\(\\hat S(t)\\)是混合样本的KM估计生存函数。 当\\(p=q=0\\)时，即为Log-Rank Test。 当\\(p=1, \\;q=0\\)时，即为Mann-Whitney-Wilcoxon Test。 当\\(q=0, \\;p&gt;0\\)时，会给前面时间段较大的权重。 当\\(p=0, \\;q&gt;0\\)时，会给后面时间段较大的权重。 最核心的问题还是权重函数的选择。一般采用Log-Rank Test或Gehan’s Test。但还是得根据实际需要来选择。 11.4.3 趋势性检验 若想要检验各个组别的风险函数是否存在某种趋势，给出如下假设 \\[ H_0: b_1(t)=b_2(t)=\\cdots=b_K(t)=b(t), \\quad \\textrm{for } t \\leq \\tau \\\\ H_A:b_1(t)\\leq b_2(t) \\leq \\cdots \\leq b_K(t), \\quad \\textrm{for } t \\leq \\tau \\] 还是考虑式(11.52)的检验统计量 \\[ Z_j(\\tau)=\\sum_{i=1}^D W(t_i)[d_{ij}-Y_{ij}(\\frac{d_i}{Y_i})], \\quad j=1, \\cdots, K \\] 前面定义的方差、协方差、权重函数都是适用的 若备择假设成立，较小的风险函数对应的\\(Z(\\tau)\\)统计量会更容易为负，因为和混合样本得到的\\(\\frac{d_i}{Y_i}\\)相比，\\(\\frac{d_{ij}}{Y_{ij}}\\)会更小。同理，对于较大的风险函数对应的\\(Z(\\tau)\\)统计量会更容易为正。 现引入一个scores序列\\(a_1 &lt; a_2 &lt; \\cdots &lt; a_K\\)，一般取\\(a_j=j\\)。构造如下检验统计量 \\[ Z=\\frac{\\sum_{j=1}^K a_j Z_j(\\tau)}{\\sqrt{\\sum_{j=1}^K \\sum_{g=1}^K a_ja_g \\hat \\sigma_{jg}}} \\tag{11.58} \\] 分母的存在主要是为了让他为标准正态分布。关键是分子，\\(a_j\\)的作用相当于一个递增的放大器 当原假设为真时，在大样本下检验统计量\\(Z\\)服从标准正态分布。若\\(Z&gt; Z_{1-\\alpha}\\)，则拒绝原假设。正如前面说的，风险函数较小的\\(Z(\\tau)\\)容易为负，风险函数较大的\\(Z(\\tau)\\)容易为正，再加上递增序列\\(a\\)的放大作用，为正的部分放大的倍数要比为负的部分大，因此检验统计量\\(Z\\)总体表现为较大的正值，因此是右侧检验。 趋势性检验仅在已知备择假设具体大小顺序的时候才能适用 11.4.4 分层检验 考虑“辛普森悖论”的影响，有时需要根据分层变量来对样本先进行分层，再进行假设检验。 假设样本可根据某分层变量分成M个水平，给出如下假设 \\[ H_0: b_{1s}(t)=b_{2s}(t)=\\cdots=b_{Ks}(t), \\; \\textrm{for } s=1, \\cdots, M, \\; t &lt; \\tau \\\\ H_A: \\textrm{at least one of the } h_{js}(t) \\textrm{ is different for some } s \\textrm{ and } t &lt; τ \\] 于是每一层的检验统计量、方差、协方差分别为 \\[ Z_{js}(\\tau)=\\sum_{i=1}^D W(t_i)[d_{ijs}-Y_{ijs}(\\frac{d_{is}}{Y_{is}})] \\tag{11.59} \\] \\[ \\hat \\sigma_{jjs}=\\sum_{i=1}^D W(t_i)^2 \\frac{Y_{ijs}}{Y_{is}}(1-\\frac{Y_{ijs}}{Y_{is}})(\\frac{Y_{is}-d_{is}}{Y_{is}-1})d_{is}, \\; j=1, \\cdots, K \\tag{11.60} \\] \\[ \\hat \\sigma_{jgs}=-\\sum_{i=1}^D W(t_i)^2 \\frac{Y_{ijs}}{Y_{is}}\\frac{Y_{igs}}{Y_{is}}(\\frac{Y_{is}-d_{is}}{Y_{is}-1})d_{is}, \\; j \\neq g \\tag{11.61} \\] 据此得到总的检验统计量、方差和协方差 \\[ Z_{j \\cdot}(\\tau) = \\sum_{s=1}^M Z_{js}(\\tau) \\tag{11.62} \\] \\[ \\hat \\sigma_{jj \\cdot}=\\sum_{s=1}^M \\hat \\sigma_{jjs} \\tag{11.63} \\] \\[ \\hat \\sigma_{jg \\cdot} = \\sum_{s=1}^M \\hat \\sigma_{jgs} \\tag{11.64} \\] 同理构造卡方统计量进行检验 \\[ \\begin{pmatrix} Z_{1\\cdot}(\\tau), \\cdots, Z_{K-1 \\cdot}(\\tau) \\end{pmatrix} \\Sigma_{\\cdot}^{-1} \\begin{pmatrix} Z_{1\\cdot}(\\tau) \\\\ \\vdots \\\\ Z_{K-1 \\cdot}(\\tau) \\end{pmatrix} \\sim \\chi^2(K-1) \\tag{11.65} \\] 在原假设为真且为大样本的条件下 "],["survival_5.html", "11.5 Cox比例风险模型", " 11.5 Cox比例风险模型 对于数据\\(X_i=(X_{i1},\\dots, X_{ip})^T\\)，风险函数被定义为 \\[ h_i(t)=h_0(t)\\exp\\{x_{i1}\\beta_1+\\dots+x_{ip}\\beta_p\\}=h_0(t)e^{X_i^T\\beta} \\tag{11.66} \\] 其中\\(h_0(t)\\)被称为基线风险函数。 截距项被归到\\(h_0(t)\\)里了 考虑\\(h_i(t|x)=h_0(t)e^{x_{i1}\\beta_1}\\)，称 \\[ \\frac{h_i(t|x+1)}{h_i(t|x)}=\\frac{h_0(t)e^{x_{i1}\\beta_1}e^{\\beta_1}}{h_0(t)e^{x_{i1}\\beta_1}}=e^{\\beta_1} \\tag{11.67} \\] 这样的关系为“比例”关系。 11.5.1 偏对数似然函数 现有数据\\((T_i,\\delta_i, Z_i)\\)，其中\\(T_i\\)表示记录的生存时间，\\(\\delta_i\\)表示删失状态（1为非删失，0为删失），\\(Z_i\\)表示协变量。 回顾式(11.15)，写出Cox比例风险模型的似然函数。 \\[ \\begin{aligned} L(\\theta)&amp;=\\prod_{i=1}^n \\{f(\\tau_i)\\}^{\\delta_i}\\{S(\\tau_i)\\}^{1-\\delta_i} \\\\ &amp;= \\prod_{i=1}^n \\{h(\\tau_i)\\}^{\\delta_i}S(\\tau_i) \\\\ &amp;= \\prod_{i=1}^n [\\{h_0(\\tau_i)\\exp(\\beta^TZ_i)\\}^{\\delta_i}\\exp\\{-H_0(\\tau_i)\\exp(\\beta^TZ_i)\\}] \\\\ &amp;= [\\prod_{i=1}^n \\{h_0(\\tau_i)\\exp(\\beta^TZ_i)\\}^{\\delta_i}][\\prod_{i=1}^n\\exp\\{-H_0(\\tau_i)\\exp(\\beta^TZ_i)\\}] \\\\ &amp;= [\\prod_{j=1}^D h_0(\\tau_j)\\exp(\\beta^TZ_{(j)})] \\exp\\{-\\sum_{i=1}^n\\sum_{t_j \\leq T_i}h_0(\\tau_j)\\exp(\\beta^TZ_i)\\} \\\\ &amp;= [\\prod_{j=1}^D h_0(\\tau_j)\\exp(\\beta^TZ_{(j)})] \\exp\\{-\\sum_{j=1}^D\\sum_{i \\in R_j}h_0(\\tau_j)\\exp(\\beta^TZ_i)\\} \\end{aligned} \\] \\(h(t)=\\frac{f(t)}{S(t)},\\; S(t)=\\exp\\{-H(t)\\}\\) D表示非删失的对象数，指数处的\\(\\delta_i\\)等价于就是对非删失的对象累乘 \\(H_0(t)=\\int_0^t h_0(x)dx\\)，在实际中就是对不同死亡时刻的\\(h_0(\\tau)\\)累加 \\(R_j\\)表示\\(T_i \\geq T_j\\)的\\(i\\)的集合 11.5.2 Regularization Paths for Cox’s Proportional Hazards Model via Coordinate Descent 下面的内容是对Simon等人[1]论文的复现。 观测对象的数据结构为\\((y_i,x_i,\\delta_i)\\)，分别表示生存时间、自变量向量、生存结局。其中\\(\\delta_i\\)取值为1: failure time或0: censoring time。将\\(\\delta=1\\)的failure time进行排序得到\\(t_1 \\leq t_2 \\leq \\cdots \\leq t_m\\)。 在有结点情况时才有可能取到等号 11.5.2.1 推导 以下内容为个人推导，与原文略有出入 1. 无结点情况 \\[ L(\\beta)=\\prod_{i=1}^m \\frac{e^{x_{j(i)}^T\\beta}}{\\sum_{j \\in R_i }e^{x_j^T\\beta}} \\] 其中\\(R_i\\)表示\\(t_i \\leq y_j\\)的\\(j\\)的集合，\\(j(i)\\)表示第\\(i\\)个failure time对应的观测对象\\(j\\)。 \\[ \\frac{2}{n}l(\\beta)=\\frac{2}{n}\\begin{bmatrix}\\sum_{i=1}^mx_{j(i)}^T\\beta-\\sum_{i=1}^m\\log(\\sum_{j \\in R_i}e^{x_j^T\\beta})\\end{bmatrix} \\] 这里的1/n相当于是权重，2是为了消掉泰勒展开中的1/2 令\\(\\eta=X\\beta\\)，对对数似然函数进行二阶泰勒展开 \\[ \\begin{aligned} l(\\beta)&amp;\\approx l(\\tilde \\beta)+(\\beta-\\tilde \\beta)^T\\dot l(\\tilde \\beta)+(\\beta-\\tilde \\beta)^T\\ddot l(\\tilde \\beta)(\\beta-\\tilde \\beta)/2 \\\\ &amp;=l(\\tilde \\beta)+(X\\beta-\\tilde \\eta)^Tl&#39;(\\tilde \\eta)+(X\\beta-\\tilde \\eta)^Tl&#39;&#39;(\\tilde \\eta)(X\\beta-\\tilde \\eta)/2 \\end{aligned} \\] 其中 \\[ \\frac{\\partial l(\\eta)}{\\partial \\beta}=(X^T)_{p \\times n}l&#39;(\\eta)_{n \\times 1}=\\dot l(\\beta)_{p \\times1} \\] \\((X_{\\cdot k})^Tl&#39;(\\eta)=\\dot l(\\beta)_k\\) 整理可得 \\[ l(\\beta)\\approx\\frac{1}{2}(z(\\tilde \\eta)-X\\beta)^Tl&#39;&#39;(\\tilde \\eta)(z(\\tilde \\eta)-X\\beta)+C(\\tilde \\eta,\\tilde \\beta)\\\\ z(\\tilde \\eta)=\\tilde \\eta-l&#39;&#39;(\\tilde \\eta)^{-1}l&#39;(\\tilde \\eta) \\] 经检验，原文该表达式没错 则 \\[ \\frac{2}{n}l(\\beta)\\approx\\frac{1}{n}(z(\\tilde \\eta)-X\\beta)^Tl&#39;&#39;(\\tilde \\eta)(z(\\tilde \\eta)-X\\beta)+\\frac{2}{n}C(\\tilde \\eta,\\tilde \\beta) \\] 原文指出，为了计算方便，仅取黑塞矩阵的对角线元素而无视其他元素，其余元素对最终结果的影响也较小。 故目标函数为 \\[ M(\\beta)=-\\frac{1}{n}\\sum_{i=1}^nw(\\tilde \\eta)_i(z(\\tilde \\eta)_i-x_i^T\\beta)^2+\\lambda(\\alpha\\sum_{k=1}^p|\\beta_k|+\\frac{1}{2}(1-\\alpha)\\sum_{k=1}^p\\beta_k^2) \\] 其中\\(w(\\tilde \\eta)_i\\)是\\(l&#39;&#39;(\\tilde \\eta)\\)的第\\(i\\)个对角线元素。 由于是最小化，对数似然函数得添负号 下面推导\\(w(\\eta)_i\\)及\\(z(\\eta)_i\\)的具体表达式 \\[ l(\\eta)=\\sum_{i=1}^m (\\eta_{j(i)}-\\log(\\sum_{j \\in R_i}e^{\\eta_j}))=\\sum_{i=1}^m \\eta_{j(i)}-\\sum_{i=1}^m \\log(\\sum_{j \\in R_i}e^{\\eta_j}) \\] 对\\(\\eta_k\\)求偏导 \\[ l&#39;(\\eta)_k=\\delta_k-\\sum_{i=1}^m \\frac{e^{\\eta_k}\\textrm{I}_{\\{y_k \\geq t_i\\}}}{\\sum_{j \\in R_i}e^{\\eta_j}}=\\delta_k-\\sum_{i \\in C_k}(\\frac{e^{\\eta_k}}{\\sum_{j \\in R_i}e^{\\eta_j}}) \\] 由于是对\\(\\eta_k\\)求偏导，因此在\\(\\sum_{i=1}^m \\eta_{j(i)}\\)中，若有\\(\\eta_k\\)则为1，反之为0。也就是说，只要\\(y_k\\)是failure time就为1，是删失数据即为0，等价于\\(\\delta_k\\)。而对于给定的\\(i\\)，\\(\\eta_k\\)不一定在\\(\\sum_{j \\in R_i}e^{\\eta_j}\\)中，因此可根据\\(R_i\\)的定义添加示性函数。综合考虑\\(\\sum_{i=1}^m\\)和\\(\\textrm{I}_{\\{y_k \\geq t_i\\}}\\)即可发现，\\(\\eta_k\\)仅出现在\\(y_k \\geq t_i\\)的\\(i\\)的集合中，也就是\\(C_k\\)的定义。 \\[ \\begin{aligned} l&#39;&#39;(\\eta)_{kk}&amp;=-[e^{\\eta_k} \\cdot \\sum\\limits_{i \\in C_k}\\frac{1}{\\sum_{j \\in R_i}e^{\\eta_j}}+e^{\\eta_k}(-\\sum_{i \\in C_k}\\frac{e^{\\eta_k}}{(\\sum_{j \\in R_i}e^{\\eta_j})^2})] \\\\ &amp;=-\\sum_{i \\in C_k}\\frac{e^{\\eta_k}\\sum_{j \\in R_i}e^{\\eta_j}-(e^{\\eta_k})^2}{(\\sum_{j \\in R_i}e^{\\eta_j})^2} \\end{aligned} \\] 当\\(\\sum_{i=1}^m\\)转化为\\(\\sum_{i \\in C_k}\\)后，此时\\(i\\)对应的\\(R_i\\)中必定包含\\(\\eta_k\\)，因此不用再加示性函数。 \\(w(\\tilde \\eta)_k\\)显然小于等于0，因为\\(R_i\\)中必定包含索引\\(k\\) \\[ z(\\tilde \\eta)_k=\\tilde \\eta_k-\\frac{l&#39;(\\tilde \\eta)_k}{l&#39;&#39;(\\tilde \\eta)_{kk}}=\\tilde \\eta_k-\\frac{\\delta_k-\\sum\\limits_{i \\in C_k}(\\frac{e^{\\eta_k}}{\\sum_{j \\in R_i}e^{\\eta_j}})}{w(\\tilde \\eta)_k} \\] 事实上，这里有一个致命的错误。当\\(y_k \\lt t_1\\)时，\\(C_k\\)为空集，对应的\\(w(\\tilde \\eta)_k=0\\)，不能取倒数！ 之后，对\\(\\beta_k\\)求偏导 \\[ \\frac{\\partial M}{\\partial \\beta_k}=\\frac{2}{n}\\sum_{i=1}^nw(\\tilde \\eta)_ix_{ik}(z(\\tilde \\eta)_i-x_i^T\\beta)+\\lambda\\alpha\\cdot\\textrm{sgn}(\\beta_k)+\\lambda(1-\\alpha)\\beta_k \\] 对于这个分子上的2，可以将其并到\\(\\lambda\\)中从而忽略掉这个2，毕竟\\(\\lambda\\)是数据驱动的参数。 令偏导为0，可得 \\[ \\frac{1}{n}\\sum_{i=1}^nw_ix_{ik}(z_i-\\sum_{j \\neq k}x_{ij}\\beta_j)-\\frac{1}{n}\\sum_{i=1}^nw_ix_{ik}^2\\beta_k+\\lambda\\alpha\\cdot \\textrm{sgn}(\\beta_k)+\\lambda(1-\\alpha)\\beta_k=0 \\] 行宽有限，这里简记\\(w(\\tilde \\eta)_i=w_i\\)、\\(z(\\tilde \\eta)_i=z_i\\) 再次强调，这里的1/n事实上就是权重，应该把\\(w_i/n\\)看出一个整体 此时可将不含\\(\\beta_k\\)的第一项记作常数\\(C\\)，把后面的三项记作关于\\(\\beta_k\\)的函数\\(f(\\beta_k)\\) \\[ f(\\beta_k)=(\\lambda(1-\\alpha)-\\frac{1}{n}\\sum_{i=1}^nw_ix_{ik}^2)\\beta_k+\\lambda\\alpha\\cdot\\textrm{sgn}(\\beta_k) \\] 由于\\(w_i \\leq 0\\)，且\\(f(\\beta_k)\\)为奇函数，则其图像大概为 图 11.9: 函数图 则该问题就转化为对\\(C\\)进行分类讨论，看看\\(f(\\beta_k)\\)什么时候和横轴相交，求出相交时的横坐标即可。思路已经有了，这里就不展开说了，得到结果如下所示 \\[ \\hat\\beta_k=-\\frac{\\textrm{S}(\\frac{1}{n}\\sum_{i=1}^nw_ix_{ik}(z_i-\\sum_{j \\neq k}x_{ij}\\beta_j),\\lambda\\alpha)}{-\\frac{1}{n}\\sum_{i=1}^nw_ix_{ik}^2+\\lambda(1-\\alpha)} \\] 其中\\(\\textrm{S}(x,\\lambda)=\\textrm{sgn}(x)(|x|-\\lambda)_+\\)。 2. 有结点情况 有结点情况相较无结点情况就是多了权重，其余步骤都是一样的。 \\[ L(\\beta)=\\prod_i^m\\frac{\\exp{(\\sum_{j \\in D_i}\\omega_j\\eta_j})}{(\\sum_{j \\in R_i}\\omega_je^{\\eta_j})^{d_i}} \\] 其中\\(D_i\\)表示结点为\\(t_i\\)的集合，\\(\\omega_j\\)表示权重，\\(d_i=\\sum_{j \\in D_i}\\omega_j\\)。 对数似然函数为 \\[ l(\\beta)=\\sum_i^{m}[(\\sum_{j \\in D_i}\\omega_j\\eta_j)-d_i\\log(\\sum_{j \\in R_i}\\omega_je^{\\eta_j})] \\] 对\\(\\eta_k\\)求一阶导及二阶导 \\[ l&#39;(\\eta)_k=\\delta_k\\omega_k-\\sum_{i \\in C_k}d_i\\frac{\\omega_ke^{\\eta_k}}{\\sum_{j \\in R_i}\\omega_je^{\\eta_j}} \\] \\[ l&#39;&#39;(\\eta)_{kk}=-\\sum_{i \\in C_k}d_i\\frac{\\omega_ke^{\\eta_k}(\\sum_{j \\in R_i}\\omega_je^{\\eta_j})-(\\omega_ke^{\\eta_k})^2}{(\\sum_{j \\in R_i}\\omega_je^{\\eta_j})^2} \\] 同样没有解决\\(w(\\eta)_k\\)可能为0的问题。 你可以试着将其代入到无结点的情况下，也就是把\\(\\omega_j=d_i=1/n\\)带进去，就会发现无结点情况下的那个1/n就是权重，应该把那个1/n并到\\(l&#39;&#39;(\\tilde \\eta)\\)中，这样无结点和有结点就一致了 则 \\[ z(\\tilde \\eta)_k=\\tilde \\eta_k-\\frac{\\delta_k\\omega_k-\\sum_{i \\in C_k}d_i\\frac{\\omega_ke^{\\eta_k}}{\\sum_{j \\in R_i}\\omega_je^{\\eta_j}}}{w(\\tilde \\eta)_k} \\] \\(\\hat\\beta_k\\)的表达式同无结点情形。 3. 收敛条件 定义\\(D(\\beta)\\)为 \\[ D(\\beta)=2(l_{saturated}-l(\\beta)) \\] 则 \\[ D_{null}=D(0)=2(l_{saturated}-l_{null}) \\] 收敛条件为 \\[ D(\\beta_{current})-D_{null} \\geq 0.99D_{null} \\] 特别地，原文提供了简单的计算公式。在无结点情况下 \\[ l_{saturated}=0\\\\ \\] \\[ l_{null}=-\\sum_{i=1}^m \\log|R_i| \\] 在有结点情况下 \\[ l_{null}=-\\sum_{i=1}^m d_i \\log(\\sum_{j \\in R_i}w_j) \\] \\[ l_{saturated}=-\\sum_{i=1}^m d_i \\log(d_i) \\] 11.5.2.2 自定义算法 由于当前技术难以缩减计算时间，故自定义算法暂且放弃“正则化路径”功能 # **************参数输入************** # y：矩阵，要求第一列为观测时间，第二列为状态 # X：自变量矩阵 # weight：权重向量，长度同样本量 # beta_0：迭代的初始值 # lambda/alpha：正则化参数 # max.iter：最大迭代次数 # trace：是否展示迭代过程 cox_cd &lt;- function(y, X, weight=NULL, beta_0=NULL, lambda, alpha, max.iter=100, trace = FALSE){ # 设置输出对象 outcome = list( weight = NULL, lambda = NULL, alpha = NULL, beta = NULL, D_null = NULL, D_current = NULL ) outcome$lambda = lambda outcome$alpha = alpha status = y[,2] y = y[,1] n = length(y) failure_t = y[status==1] %&gt;% unique() %&gt;% sort() R = map(failure_t, ~which(y&gt;=.)) #R_i C = map(y, ~which(failure_t&lt;=.)) #C_k # 根据是否有ties运行不同代码 if(length(y)==length(unique(y))){ # 无结点 weight = 1/n #原文无ties情况的1/n就是有ties情况下权重为1/n的情形 outcome$weight = weight # log_likelihood_beta用于精度判断 log_likelihood_beta &lt;- function(beta){ term_1 = as.numeric(status %*% X %*% beta) #在无结点情况下，j(i)与status等价 term_2 = map_vec(R, function(R_i){ map_vec(R_i, ~exp(X[.,] %*% beta)) %&gt;% sum() %&gt;% log() }) %&gt;% sum() result = term_1 - term_2 result } # 初始化beta if(is.null(beta_0)){ beta = rep(0,dim(X)[2]) }else{ beta = beta_0 } D_null = 2 * map_vec(R, ~length(.) %&gt;% log()) %&gt;% sum() #用于判断精度 outcome$D_null = D_null for (i in 1:max.iter) { if(trace == TRUE) cat(paste0(&quot;-----第&quot;, i, &quot;次迭代-----\\n&quot;)) eta = X %*% beta eta = scale(eta, TRUE, FALSE) # 源码有这个，为了保持一致我也加上去了 hessian = map2_vec(C, c(1:length(C)), function(C_k, k){ # 计算w_k C_k = C_k k = k # .y提供位置索引 eta_k = as.numeric(eta[k,]) exp_eta_k = exp(eta_k) exp_eta_k_2 = exp_eta_k^2 w_k = map_vec(C_k, function(i){ sum_exp_eta_Ri = map_vec(R[[i]], ~exp(eta[.,])) %&gt;% sum() sum_exp_eta_Ri_2 = sum_exp_eta_Ri^2 value = (exp_eta_k * sum_exp_eta_Ri - exp_eta_k_2) / sum_exp_eta_Ri_2 value }) %&gt;% sum() w_k = -weight * w_k # 无结点情况的1/n就相当于是权重 w_k }) grad = map2_vec(C, c(1:length(C)), function(.x, .y){ # 计算w_k C_k = .x k = .y # .y提供位置索引 eta_k = as.numeric(eta[k,]) exp_eta_k = exp(eta_k) w_prior_k = map_vec(C_k, function(i){ sum_exp_eta_Ri = map_vec(R[[i]], ~exp(eta[.,])) %&gt;% sum() value = exp_eta_k / sum_exp_eta_Ri value }) %&gt;% sum() w_prior_k }) grad = weight * (status - grad) if(any(hessian==0)){ if(trace == TRUE) cat(&#39;=====w_k中有零=====\\n&#39;) hessian[which(hessian == 0)] = 0.0000001 } z = eta - grad / hessian last_beta = beta # 坐标下降法 for (k in 1:length(beta)) { denominator = as.numeric(hessian %*% X[,k]^2 + lambda * (1-alpha)) numerator = as.numeric(t(diag(hessian) %*% X[,k]) %*% (z - X[,-k] %*% beta[-k])) numerator = sign(numerator) * max(abs(numerator), lambda * alpha) beta[k] = numerator/denominator } # 精度判断 # 无ties情况下，l_saturated = 0 D_current = -2 * log_likelihood_beta(beta) outcome$D_current = D_current # 和原文的收敛条件不同，感觉这样更符合逻辑，详见“自定义算法检验” if(D_null - D_current &gt;= 0.99 * D_null){ if(trace == TRUE){ cat(paste0(beta, &#39;\\n&#39;)) cat(&#39;&lt;&lt;&lt;&lt;&lt;满足精度要求&gt;&gt;&gt;&gt;&gt;\\n&#39;) } break } if(all(round(last_beta, 7) == round(beta, 7))){ if(trace == TRUE){ cat(paste0(beta, &#39;\\n&#39;)) cat(&#39;&lt;&lt;&lt;&lt;&lt;系数不再更新&gt;&gt;&gt;&gt;&gt;\\n&#39;) } break } if(trace == TRUE) cat(paste0(beta,&#39;\\n&#39;)) } outcome$beta = beta return(outcome) }else{ # 有ties if(is.null(weight)){ weight = rep(1, n)/n #若不指定权重，则默认为1/n }else{ if(length(weight) == n){ if(sum(weight) == 1){ weight #若权重和为1，则可以 }else{ weight = weight/sum(weight) #若权重和不为1，则标准化 } }else{ cat(&#39;权重向量长度不匹配\\n&#39;) } } outcome$weight = weight D = map(failure_t, ~which(y == . &amp; status==1)) d = map_vec(D, ~sum(weight[.])) # log_likelihood_beta用于精度判断 log_likelihood_beta &lt;- function(beta){ term_1 = as.numeric(status %*% diag(weight) %*% X %*% beta) #第一项等价于所有的failure time的加权和 term_2 = map_vec(R, function(R_i){ map_vec(R_i, ~weight[.] * exp(X[.,] %*% beta)) %&gt;% sum() %&gt;% log() }) %*% d result = term_1 - as.numeric(term_2) result } # 初始化beta if(is.null(beta_0)){ beta = rep(0,dim(X)[2]) }else{ beta = beta_0 } # 用于精度判断 l_null = -map_vec(R, function(R_i){ map_vec(R_i, ~weight[.]) %&gt;% sum() %&gt;% log() }) %*% d %&gt;% as.numeric() l_saturated = -as.numeric(d %*% log(d)) D_null = 2 * (l_saturated - l_null) outcome$D_null = D_null for (i in 1:max.iter) { if(trace == TRUE) cat(paste0(&quot;-----第&quot;, i, &quot;次迭代-----\\n&quot;)) eta = X %*% beta eta = scale(eta, TRUE, FALSE) #源码有这个，为了保持一致我也加上去了 hessian = map2_vec(C, c(1:length(C)), function(C_k, k){ # 计算w_k C_k = C_k k = k # .y提供位置索引 eta_k = as.numeric(eta[k,]) weight_exp_eta_k = weight[k] * exp(eta_k) weight_exp_eta_k_2 = (weight_exp_eta_k)^2 w_k = map_vec(C_k, function(i){ weight_sum_exp_eta_Ri = map_vec(R[[i]], ~weight[.] * exp(eta[.,])) %&gt;% sum() weight_sum_exp_eta_Ri_2 = weight_sum_exp_eta_Ri^2 value = d[i] * (weight_exp_eta_k * weight_sum_exp_eta_Ri - weight_exp_eta_k_2) / weight_sum_exp_eta_Ri_2 value }) %&gt;% sum() w_k = -w_k #权重已经包含在w_k里面了 w_k }) grad = map2_vec(C, c(1:length(C)), function(.x, .y){ # 计算w_k C_k = .x k = .y # .y提供位置索引 eta_k = as.numeric(eta[k,]) weight_exp_eta_k = weight[k] * exp(eta_k) w_prior_k = map_vec(C_k, function(i){ weight_sum_exp_eta_Ri = map_vec(R[[i]], ~weight[.] * exp(eta[.,])) %&gt;% sum() value = d[i] * weight_exp_eta_k / weight_sum_exp_eta_Ri value }) %&gt;% sum() w_prior_k = status[k] * weight[k]-w_prior_k w_prior_k }) if(any(hessian==0)){ if(trace == TRUE) cat(&#39;=====w_k中有零=====\\n&#39;) hessian[which(hessian == 0)] = 0.0000001 } z = eta - grad / hessian last_beta = beta for (k in 1:length(beta)) { denominator = as.numeric(hessian %*% X[,k]^2 + lambda * (1-alpha)) numerator = as.numeric(t(diag(hessian) %*% X[,k]) %*% (z - X[,-k] %*% beta[-k])) numerator = sign(numerator) * max(abs(numerator), lambda * alpha) beta[k] = numerator/denominator } # 精度判断 D_current = 2 * (l_saturated - log_likelihood_beta(beta)) outcome$D_current = D_current if(D_null - D_current &gt;= 0.99 * D_null){ if(trace == TRUE){ cat(paste0(beta, &#39;\\n&#39;)) cat(&#39;&lt;&lt;&lt;&lt;&lt;满足精度要求&gt;&gt;&gt;&gt;&gt;\\n&#39;) } break } if(all(round(last_beta, 7) == round(beta, 7))){ if(trace == TRUE){ cat(paste0(beta, &#39;\\n&#39;)) cat(&#39;&lt;&lt;&lt;&lt;&lt;系数不再更新&gt;&gt;&gt;&gt;&gt;\\n&#39;) } break } if(trace == TRUE) cat(paste0(beta,&#39;\\n&#39;)) } outcome$beta = beta return(outcome) } } 11.5.2.3 自定义算法检验 模拟所用数据集来自glmnet包的data(CoxExample)数据集。 内容总结： 自定义算法的梯度向量、黑塞矩阵对角线元素与源码计算结果基本一致。其中黑塞矩阵对角线元素可能会出现0，因此为其加上非常小的数(0.0000001)。 对于收敛条件，对于无结点情况，自定义算法与原函数的结果完全一致，但在有结点情况则存在差异。同时有理由怀疑除了原文给出的收敛条件外，还有其他的收敛条件，并且原文的收敛条件可能有误。 对自定义算法随机选取初始值，发现均能收敛到相同结果，表明自定义算法具有一定的稳健性。但与原函数的结果还存在差异。 综上，自定义算法是对论文内容的复刻，因此在未提及的细节处必定与原函数存在差异，从而导致结果的差异。但此次复刻不失为一次有益的探索。 1. 梯度向量与黑塞矩阵 library(glmnet) data(CoxExample) X &lt;- CoxExample[[1]][1:50,1:5] y &lt;- CoxExample[[2]][1:50,] head(X) ## [,1] [,2] [,3] [,4] [,5] ## [1,] -0.8767670 -0.6135224 -0.56757380 0.6621599 1.82218019 ## [2,] -0.7463894 -1.7519457 0.28545898 1.1392105 0.80178007 ## [3,] 1.3759148 -0.2641132 0.88727408 0.3841870 0.05751801 ## [4,] 0.2375820 0.7859162 -0.89670281 -0.8339338 -0.58237643 ## [5,] 0.1086275 0.4665686 -0.57637261 1.7041314 0.32750715 ## [6,] 1.2027213 -0.4187073 -0.05735193 0.5948491 0.44328682 head(y) ## time status ## [1,] 1.76877757 1 ## [2,] 0.54528404 1 ## [3,] 0.04485918 0 ## [4,] 0.85032298 0 ## [5,] 0.61488426 1 ## [6,] 0.29860939 0 status = y[,2] y = y[,1] n = length(y) failure_t = y[status==1] %&gt;% sort() R = map(failure_t, ~which(y&gt;=.)) #R中每个元素对应原文的R_i C = map(y, ~which(failure_t&lt;=.)) # 无ties weight = 1/n #原文无ties情况的1/n就是有ties情况下权重为1/n的情形 # 初始化beta beta = rep(0,dim(X)[2]) eta = X %*% beta eta = scale(eta, TRUE, FALSE) #源码有这个中心化的操作，为了保持一致，这里也加上 # 梯度向量 my_grad = map2_vec(C, c(1:length(C)), function(.x, .y){ # 计算w_k C_k = .x k = .y # .y提供位置索引 eta_k = as.numeric(eta[k,]) exp_eta_k = exp(eta_k) w_prior_k = map_vec(C_k, function(i){ sum_exp_eta_Ri = map_vec(R[[i]], ~exp(eta[.,])) %&gt;% sum() value = exp_eta_k / sum_exp_eta_Ri value }) %&gt;% sum() w_prior_k }) my_grad = weight * (status - my_grad) # 黑塞矩阵对角线元素 hessian = map2_vec(C, c(1:length(C)), function(C_k, k){ # 计算w_k C_k = C_k k = k # .y提供位置索引 eta_k = as.numeric(eta[k,]) exp_eta_k = exp(eta_k) exp_eta_k_2 = exp_eta_k^2 w_k = map_vec(C_k, function(i){ sum_exp_eta_Ri = map_vec(R[[i]], ~exp(eta[.,])) %&gt;% sum() sum_exp_eta_Ri_2 = sum_exp_eta_Ri^2 value = (exp_eta_k * sum_exp_eta_Ri - exp_eta_k_2) / sum_exp_eta_Ri_2 value }) %&gt;% sum() w_k = -weight * w_k # 无结点情况的1/n就相当于是权重 w_k }) my_hessian = hessian 下面是源码中关于梯度向量和黑塞矩阵的计算。 X &lt;- CoxExample[[1]][1:50,1:5] y &lt;- CoxExample[[2]][1:50,] fid &lt;- function(x,index) { idup=duplicated(x) if(!any(idup)) list(index_first=index,index_ties=NULL) else { ndup=!idup xu=x[ndup]# first death times index_first=index[ndup] ities=match(x,xu) index_ties=split(index,ities) nties=sapply(index_ties,length) list(index_first=index_first,index_ties=index_ties[nties&gt;1]) } } w=rep(1,length(eta)) w=w/sum(w) nobs &lt;- nrow(y) time &lt;- y[, &quot;time&quot;] d &lt;- y[, &quot;status&quot;] eta &lt;- scale(eta, TRUE, FALSE) o &lt;- order(time, d, decreasing = c(FALSE, TRUE)) exp_eta &lt;- exp(eta)[o] time &lt;- time[o] d &lt;- d[o] w &lt;- w[o] rskden &lt;- rev(cumsum(rev(exp_eta*w))) dups &lt;- fid(time[d == 1],seq(length(d))[d == 1]) dd &lt;- d ww &lt;- w rskcount=cumsum(dd) rskdeninv=cumsum((ww/rskden)[dd==1]) rskdeninv=c(0,rskdeninv) grad &lt;- w * (d - exp_eta * rskdeninv[rskcount+1]) grad[o] &lt;- grad #源码的梯度向量 rskdeninv2 &lt;- cumsum((ww/(rskden^2))[dd==1]) rskdeninv2 &lt;- c(0, rskdeninv2) w_exp_eta &lt;- w * exp_eta diag_hessian &lt;- w_exp_eta^2 * rskdeninv2[rskcount+1] - w_exp_eta * rskdeninv[rskcount+1] diag_hessian[o] &lt;- diag_hessian #源码的黑塞矩阵对角线 梯度向量对比。 my_grad #我的梯度向量 ## [1] -0.0014031776 0.0156521479 -0.0008699764 -0.0093888607 0.0141421764 ## [6] -0.0029439112 0.0000000000 -0.0404031776 0.0124726112 -0.0214031776 ## [11] 0.0056201324 0.0163664336 0.0176275174 -0.0008699764 -0.0214031776 ## [16] -0.0116179628 -0.0066578236 0.0024150042 0.0133421764 -0.0029439112 ## [21] 0.0000000000 -0.0104414923 0.0186538332 -0.0029439112 0.0149114072 ## [26] -0.0018461668 -0.0214031776 0.0195744681 -0.0054031776 0.0115635203 ## [31] -0.0013461668 -0.0023724826 0.0106111393 0.0000000000 0.0070487038 ## [36] 0.0191300236 -0.0214031776 -0.0029439112 -0.0004255319 -0.0204031776 ## [41] 0.0170560888 0.0040816709 -0.0104031776 0.0005968224 -0.0304031776 ## [46] 0.0095585077 -0.0023724826 0.0181538332 0.0083820372 -0.0029439112 grad #源码的梯度向量 ## [1] -0.0014031776 0.0156521479 -0.0008699764 -0.0093888607 0.0141421764 ## [6] -0.0029439112 0.0000000000 -0.0404031776 0.0124726112 -0.0214031776 ## [11] 0.0056201324 0.0163664336 0.0176275174 -0.0008699764 -0.0214031776 ## [16] -0.0116179628 -0.0066578236 0.0024150042 0.0133421764 -0.0029439112 ## [21] 0.0000000000 -0.0104414923 0.0186538332 -0.0029439112 0.0149114072 ## [26] -0.0018461668 -0.0214031776 0.0195744681 -0.0054031776 0.0115635203 ## [31] -0.0013461668 -0.0023724826 0.0106111393 0.0000000000 0.0070487038 ## [36] 0.0191300236 -0.0214031776 -0.0029439112 -0.0004255319 -0.0204031776 ## [41] 0.0170560888 0.0040816709 -0.0104031776 0.0005968224 -0.0304031776 ## [46] 0.0095585077 -0.0023724826 0.0181538332 0.0083820372 -0.0029439112 黑塞矩阵对角线元素对比。 my_hessian #我的黑塞矩阵对角线元素 ## [1] -0.0201293825 -0.0042256154 -0.0008510459 -0.0090531224 -0.0056785663 ## [6] -0.0028709660 0.0000000000 -0.0320793825 -0.0072783243 -0.0201293825 ## [11] -0.0137285938 -0.0035368399 -0.0023158639 -0.0008510459 -0.0201293825 ## [16] -0.0111576188 -0.0064465663 -0.0166764899 -0.0064465663 -0.0028709660 ## [21] 0.0000000000 -0.0100503523 -0.0013158986 -0.0028709660 -0.0049389213 ## [26] -0.0018033986 -0.0201293825 -0.0004164780 -0.0233293825 -0.0081460929 ## [31] -0.0013158986 -0.0023158639 -0.0090531224 0.0000000000 -0.0124020632 ## [36] -0.0008510459 -0.0201293825 -0.0028709660 -0.0004164780 -0.0320793825 ## [41] -0.0028709660 -0.0151487122 -0.0270793825 -0.0183293825 -0.0270793825 ## [46] -0.0100503523 -0.0023158639 -0.0018033986 -0.0111576188 -0.0028709660 diag_hessian #源码的黑塞矩阵对角线元素 ## [1] -0.0201293825 -0.0042256154 -0.0008510459 -0.0090531224 -0.0056785663 ## [6] -0.0028709660 0.0000000000 -0.0320793825 -0.0072783243 -0.0201293825 ## [11] -0.0137285938 -0.0035368399 -0.0023158639 -0.0008510459 -0.0201293825 ## [16] -0.0111576188 -0.0064465663 -0.0166764899 -0.0064465663 -0.0028709660 ## [21] 0.0000000000 -0.0100503523 -0.0013158986 -0.0028709660 -0.0049389213 ## [26] -0.0018033986 -0.0201293825 -0.0004164780 -0.0233293825 -0.0081460929 ## [31] -0.0013158986 -0.0023158639 -0.0090531224 0.0000000000 -0.0124020632 ## [36] -0.0008510459 -0.0201293825 -0.0028709660 -0.0004164780 -0.0320793825 ## [41] -0.0028709660 -0.0151487122 -0.0270793825 -0.0183293825 -0.0270793825 ## [46] -0.0100503523 -0.0023158639 -0.0018033986 -0.0111576188 -0.0028709660 可见，二者基本一致。 当用my_grad==grad及my_hessian==diag_hessian比较是否一致时，小部分是FALSE，但单从展示的数值上来看，几乎是一致的，可见差异微乎其微。 需要注意的是，正如前面提到的，\\(w(\\tilde \\eta)_k\\)可能为0。无论是自定义算法还是源码都输出了0（第7,21,34个对角线元素），但源码的函数能够运行下去，而自定义算法则不行，说明原文遗漏了一些细节。 当\\(y_k \\lt t_1\\)时，\\(C_k\\)即为空集，则对应的\\(w(\\tilde \\eta)_k=0\\) 即使删掉\\(C_k\\)为空集的观测对象，结果也与原函数不一致 鉴于此，为\\(w(\\tilde \\eta)_k=0\\)的元素加上非常小的数(0.0000001)以确保代码能够正确运行。 2. 收敛条件 原文中使用\\(D(0)\\)与\\(D(\\beta_{current})\\)作为收敛条件。\\(D(\\cdot)\\)的内核就是对数似然函数，不妨先确定自定义算法中关于对数似然函数的定义是否正确。 先考虑无结点的情况。此时\\(l_{saturated}=0\\) 用自定义算法中的log_likelihood_beta()函数计算\\(D(0)\\) X &lt;- CoxExample[[1]][1:50,1:5] y &lt;- CoxExample[[2]][1:50,] log_likelihood_beta &lt;- function(beta){ term_1 = as.numeric(status %*% X %*% beta) #在无结点情况下，j(i)与status等价 term_2 = map_vec(R, function(R_i){ map_vec(R_i, ~exp(X[.,] %*% beta)) %&gt;% sum() %&gt;% log() }) %&gt;% sum() result = term_1 - term_2 result } status = y[,2] y = y[,1] n = length(y) failure_t = y[status==1] %&gt;% unique() %&gt;% sort() #t_i R = map(failure_t, ~which(y&gt;=.)) #R_i D_null &lt;- (-2) * log_likelihood_beta(rep(0,5)) D_null ## [1] 145.1673 原函数也会输出\\(D(0)\\) X &lt;- CoxExample[[1]][1:50,1:5] y &lt;- CoxExample[[2]][1:50,] source_result &lt;- glmnet(X,y,family = &#39;cox&#39;, lambda=0.02, alpha=0.5) source_result$nulldev ## [1] 145.1673 可见二者是一致的。 再看看\\(D(\\beta_{current})\\)。原函数输出的结果中有dev.ratio一项，其值为\\(dev.ratio = 1-D(\\beta_{current})/D(0)\\)。因此可根据dev.ratio及nulldev两项输出值计算\\(D(\\beta_{current})\\) # 原函数输出的D_current source_result$nulldev - source_result$dev.ratio * source_result$nulldev ## [1] 132.0468 # 自定义算法log_likelihood_beta()函数输出的D_current -2 * log_likelihood_beta(source_result$beta@x) ## [1] 132.0468 因此无论是\\(D(0)\\)还是\\(D(\\beta_{current})\\)，均表明自定义算法中的log_likelihood_beta()是正确的。 注意，\\(D(\\beta_{current})=132\\)，\\(D(0)=145\\)，根本没法满足原文的收敛条件。进一步地，对数据集进行随机抽样，看看原函数是否有按原文的收敛条件停止迭代。 set.seed(111) result &lt;- rep(NA, 100) for (i in 1:100) { obs &lt;- sample(1:1000,500) index &lt;- sample(1:30,15) X &lt;- CoxExample[[1]][obs,index] y &lt;- CoxExample[[2]][obs,] source_result &lt;- glmnet(X,y,family = &#39;cox&#39;, lambda=0.02, alpha=0.5) D_beta = source_result$nulldev - source_result$dev.ratio * source_result$nulldev D_null = source_result$nulldev convergence = D_beta-D_null &gt;= 0.99*D_null result[i] = convergence } sum(result) ## [1] 0 可见，在100次的随机抽样中，原函数没有一次根据原文所给的收敛条件停止迭代。下面，再来细看原文的收敛条件 \\[ \\begin{aligned} D(\\beta_{current})-D_{null} &amp;\\geq 0.99D_{null} \\\\ 2(l_{saturated}-l(\\beta))-2(l_{saturated}-l_{null}) &amp;\\geq 0.99 \\ast 2(l_{saturated}-l_{null}) \\\\ l_{null}-l(\\beta) &amp;\\geq 0.99(l_{saturated}-l_{null}) \\\\ \\frac{l_{null}-l(\\beta)}{l_{saturated}-l_{null}} &amp;\\geq 0.99 \\end{aligned} \\] 注意到，\\(l_{saturated}\\)是对数似然函数在理论上的最大值，故\\(l_{saturated}-l_{null} \\gt 0\\)。一般来说，如果引入\\(X\\beta\\)有助于解释的话，那么\\(l(\\beta) \\gt l_{null}\\)，则原文的收敛条件必不可能满足。若分子改为\\(l(\\beta)-l_{null}\\)则较为合理，表示引入\\(X\\beta\\)后多解释的那一部分信息，那么收敛条件就变为考察这部分信息是否占\\(l_{saturated}-l_{null}\\)的绝大部分。 自定义算法的收敛条件已按\\(D_{null}-D(\\beta_{current}) \\geq 0.99D_{null}\\)设置 既然原文给出的收敛条件有问题，并且随机抽样的结果也表明不符合给出的收敛条件，但原函数却收敛了，说明原函数还设置了其他收敛条件。鉴于此，自定义算法会在前后两次\\(\\hat \\beta\\)的变化微乎其微时停止迭代。 “微乎其微”是指保留7位小数后相等 而对于有结点的情况，则有点差异。 X &lt;- CoxExample[[1]][1:50,1:5] y &lt;- CoxExample[[2]][1:50,] y[36:50,] &lt;- y[1:15,] #设置重复数据 status = y[,2] y = y[,1] n = length(y) weight = rep(1, n)/n failure_t = y[status==1] %&gt;% unique() %&gt;% sort() R = map(failure_t, ~which(y&gt;=.)) #R_i D = map(failure_t, ~which(y == . &amp; status==1)) d = map_vec(D, ~sum(weight[.])) # log_likelihood_beta用于精度判断 log_likelihood_beta &lt;- function(beta){ term_1 = as.numeric(status %*% diag(weight) %*% X %*% beta) #第一项等价于所有的failure time的加权和 term_2 = map_vec(R, function(R_i){ map_vec(R_i, ~weight[.] * exp(X[.,] %*% beta)) %&gt;% sum() %&gt;% log() }) %*% d result = term_1 - as.numeric(term_2) result } # 自定义算法 l_null = log_likelihood_beta(rep(0,5)) l_null ## [1] 0.5402271 # 原文提到的简化算法 l_null_simple = -map_vec(R, function(R_i){ map_vec(R_i, ~weight[.]) %&gt;% sum() %&gt;% log() }) %*% d %&gt;% as.numeric() l_null_simple ## [1] 0.5402271 原文提到的关于\\(l_{null}\\)的快速算法和自定义算法中的log_likelihood_beta(0)函数结果一致。接着计算\\(D_{null}\\) # 自定义算法 l_saturated = -as.numeric(d %*% log(d)) D_null = 2 * (l_saturated - l_null) D_null ## [1] 2.387955 D_null * 50 ## [1] 119.3977 和原函数的\\(D_{null}\\)进行对比 X &lt;- CoxExample[[1]][1:50,1:5] y &lt;- CoxExample[[2]][1:50,] y[36:50,] &lt;- y[1:15,] #设置重复数据 source_result &lt;- glmnet(X,y,family = &#39;cox&#39;, lambda=0.02, alpha=0.5) source_result$nulldev ## [1] 119.3977 source_result$beta ## 5 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s0 ## V1 0.1261175 ## V2 -0.3690040 ## V3 . ## V4 0.1141002 ## V5 -0.3313418 自定义算法输出的\\(\\beta\\)向量为 0.18386651 -0.43987813 -0.03533338 0.13130503 -0.36330350 自定义算法得到的\\(D_{null}\\)与原函数输出的结果差了50倍。既然原文提到的关于\\(l_{null}\\)的快速算法和自定义算法中的log_likelihood_beta(0)函数结果一致，那么说明原函数暗中调整了倍数。因此，对于收敛条件的判定，如果\\(D_{null}\\)与\\(D(\\beta_{current})\\)都做了倍数调整的话，那么结果也是不变的，所以无需过分在意这里的倍数差异。另外，原函数没有输出第三个变量的\\(\\hat \\beta\\)，至少能看出来自定义算法和原函数还是存在差异（毕竟原函数没有输出第三个变量的系数，但自定义函数可以），归根结底还是论文提供的细节太少了。 3. 随机化初始值 上述自定义算法的结果都是基于\\(\\beta=0\\)的初始值开始迭代，下面通过随机化初始值看看自定义算法的稳健性。 X &lt;- CoxExample[[1]][1:50,1:5] y &lt;- CoxExample[[2]][1:50,] set.seed(111) for (i in 1:10) { cat(paste0(&quot;----------第&quot;, i, &quot;次迭代----------&quot;)) cat(&#39;\\n&#39;) beta_0 = runif(5,min=0,max=1) my_result = cox_cd(y, X, beta_0 = beta_0, lambda=0.02, alpha=0.5, trace = FALSE) cat(my_result$beta) cat(&#39;\\n&#39;) } ## ----------第1次迭代---------- ## 0.395017 -0.8420384 -0.1444031 0.363653 -0.5357859 ## ----------第2次迭代---------- ## 0.395017 -0.8420384 -0.1444031 0.363653 -0.5357859 ## ----------第3次迭代---------- ## 0.395017 -0.8420384 -0.1444031 0.363653 -0.5357859 ## ----------第4次迭代---------- ## 0.395017 -0.8420384 -0.1444031 0.363653 -0.5357859 ## ----------第5次迭代---------- ## 0.395017 -0.8420384 -0.1444031 0.363653 -0.5357859 ## ----------第6次迭代---------- ## 0.395017 -0.8420385 -0.1444031 0.363653 -0.5357859 ## ----------第7次迭代---------- ## 0.395017 -0.8420385 -0.1444031 0.363653 -0.5357859 ## ----------第8次迭代---------- ## 0.395017 -0.8420384 -0.1444031 0.363653 -0.5357859 ## ----------第9次迭代---------- ## 0.395017 -0.8420384 -0.1444031 0.363653 -0.5357859 ## ----------第10次迭代---------- ## 0.395017 -0.8420384 -0.1444031 0.363653 -0.5357859 source_result &lt;- glmnet(X,y,family = &#39;cox&#39;, lambda=0.02, alpha=0.5) source_result$beta@x #原函数的结果 ## [1] 0.2800404 -0.6870509 -0.1062398 0.3150430 -0.4588579 据此可知自定义算法具有一定的稳健性，但与原函数的结果存在一定差异。 References [1] SIMON N. FRIEDMAN J H. HASTIE T. 等. Regularization paths for Cox’s proportional hazards model via coordinate descent[J]. Journal of Statistical Software, 2011, 39: 1-13. "],["da.html", "12 数据分析 ", " 12 数据分析 "],["da_1.html", "12.1 ABtest", " 12.1 ABtest 流程： 明确实验目标 要优化什么指标，如支付转化率、加购率、点击率等，并定义核心指标、辅助指标与安全指标。 实验设计 确定实验组和对照组、流量分配、实验周期、随机化分流方式，并考虑样本独立性、防止干扰。 样本量计算 根据基准转化率、期望提升幅度、置信度和检验功效计算样本量，避免实验结果不显著或成本浪费。 实验执行与监控 实时监控数据质量、流量均匀性、指标波动，避免脏数据干扰实验。 统计检验与分析 对结果进行显著性分析，如t-test、卡方检验、Mann-Whitney、Bootstrap，根据数据分布选择正确方法，同时关注稳定性与用户异质性。 结论与策略落地 判断是否全量上线、继续实验还是放弃，并记录实验文档，纳入知识库。 12.1.1 确定目标与指标选取 考虑：业务要改变什么 ——&gt; 哪些指标能量化这种改变 ——&gt; ABtest能拉动哪些环节的指标。 据此确定核心指标、辅助指标与安全指标。 其中核心指标与业务需求密切相关，用来判定实验是否成功；辅助指标则是与核心指标相关联的其余指标，用户辅助分析变化原因；安全指标是我们的红线，我们总是希望在安全指标不会恶化的前提下尽可能地提升核心指标。 注意所选指标的短期效应与长期效应 举例： 目标：更改结算页面的UI设计以提升用户支付意愿 核心指标：支付转化率（下单—&gt;支付成功） 辅助指标：下单数、支付成功笔数、平均支付时长 安全指标：退款率、重复下单率 12.1.2 实验设计 构造原始假设与备择假设 原始假设应为新操作不会带来显著差异，备择假设为新操作会让目标变好或者变坏。 确定实验对象 可能不同的实验针对的用户群体不一样，需将用户细分后再进行实验。 例如首充激励应该针对尚未进行过充值的用户而言，而非面向全体用户。 流量分配 确保采取随机分配的原则均匀分配流量，确保实验组和对照组用户画像一致，避免辛普森悖论（控制其余变量）。 可采用两样本t检验、卡方检验或者方差分析来判断两组的指标是否具有显著差异 可采取50:50的比例进行分配。若实验风险较大，可采取逐步灰度的方法不断调高实验流量所占的比例，在过程中持续监控相关指标及实验效果。 样本量计算 核心思想是在给定显著性水平\\(\\alpha\\)、检验功效\\(1-\\beta\\)及预期差异大小\\(\\Delta\\)时，构造出不同假设下对同一临界值的描述，从而解出样本量\\(n\\)。 检验功效表示当总体分布确实存在差异时，统计检验能够发现这种差异的概率 在实验设计阶段，最小可检验效应MDE即\\(\\Delta\\) 故有 单边检验 比例指标： \\[ \\begin{gather} n = \\frac{(Z_{1-\\alpha}\\sqrt{2\\bar{p}(1-\\bar{p})}+Z_{1-\\beta}\\sqrt{p_1(1-p_1)+p_2(1-p_2)})^2}{(p_2-p_1)^2} \\\\ \\bar{p}=\\frac{p_1+p_2}{2} \\end{gather} \\] 若假设两组方差相等，则可简化为 \\[ \\begin{gather} n = \\frac{2p(1-p)(Z_{1-\\alpha}+Z_{1-\\beta})^2}{\\Delta^2} \\\\ p \\approx{p_1} \\end{gather} \\] 连续指标： \\[ n = \\frac{2\\sigma^2(Z_{1-\\alpha}+Z_{1-\\beta})^2}{\\Delta^2} \\] 注意\\(n\\)为每组所需样本量，在均分流量的情形下总样本量为2\\(n\\) 当显著性水平越低，检验功效越高，波动越大，差异越小，所需的样本量就越大 实验周期计算 当确定了实验所需的样本量后，那么实验周期为样本量除以每日能够纳入到实验中的样本量。但并非每日获取到的所有流量都会进入到实验中。 记每日符合研究条件的流量总数为\\(U\\)，对照组和实验组所需的样本量分别为\\(n_1\\)和\\(n_2\\)，流量进入到实验的比例为\\(g\\)，流量分配到对照组和实验组的比例分别为\\(p_1\\)和\\(p_2\\)。 则 \\[ \\begin{gather} T_{control} = \\frac{n_1}{U*g*p_1} \\\\ T_{treat} = \\frac{n_2}{U*g*p_2} \\\\ T_{min} = \\max{(T_{control}, T_{treat})} \\end{gather} \\] 事实上，实际的实验周期要比理论实验周期要长，理由如下： 实验周期需要覆盖完整的业务周期 用户行为可能存在滞后性，需要延长一段时间 避免“新鲜感效应”，核心指标可能在前期很好看，但当新鲜感褪去后又回归平淡 其他事件导致流量波动 12.1.3 实验上线 确保埋点采集的数据与实验所需数据口径一致 可在小范围内进行实验用于检验流量、分组、埋点都没问题 持续监控每日流量、核心指标与安全指标，检查分组是否均衡 不能因为短期波动而提前结束实验，若安全指标持续恶化则考虑停止实验 12.1.4 实验结束 计算各指标在实验组和对照组中的均值或比例 进行显著性检验，输出效果差异、P值或置信区间、效应量 效果差异为业务场景的绝对差异或相对差异；效应量表示统计视角的差异大小，如Cohen统计量 做必要的分层分析，看看指标在不同群体中是否有差异 看看实验组和对照组核心指标的差异是否满足预先设定的变化量MDE。 以置信区间为评价指标，若指标显著且置信区间下界大于MDE，则可以上线； 若指标显著，但置信区间包含MDE，则统计显著但效果不确定。（也就是实验操作会带来差异，但这种差异不一定符合预期）。此时考虑是否需要延长实验、增加样本或改动方案。 若指标显著，但置信区间上界小于MDE，则统计显著但效果不显著。（也就是实验操作会带来差异，但这种差异不符合预期）。此时考虑是否需要延长实验、增加样本或改动方案。 若指标不显著那就GG了。 "],["da_2.html", "12.2 异动归因", " 12.2 异动归因 找到出现问题的环节，并分析原因。 流程： 验真：确定该异动是真实异动，而非虚假异动 维度拆解：将指标进行多维度拆分，并定位问题所在之处 从数据原因到业务原因 12.2.1 验真 指标变动是正常的，关键得看变动幅度和持续时间，从而区别出正常变动与异常变动。 绝对变化、相对变化、发生时间与持续时间 首先考虑变动幅度，如果是小幅度变动，那么可能是由随机因素引起，属于正常范畴。 其次，对于突变点（也就是突然大幅度变动后又平稳）则考虑是否数据采集过程中出了问题，例如统计口径变化、数据未及时更新等等。 最后考虑指标的周期性，在特定周期内指标的大幅度变动也可能是正常的，例如十一前后车票销量暴涨。 综合来说，可根据如下方法判断是否是异常变动： 移动平均 3\\(\\sigma\\)法则 环比/同比双重比较，设定阈值 12.2.2 维度拆解 当确定指标异动是真实的之后，应当将指标进行拆解，从而定位问题所在。 将指标分为总体指标与平均指标。 说来惭愧，直到这里，我才想起来本科的时候学过这方面的知识——“指标的因素分解” 12.2.2.1 总数指标 对于总数指标可以考虑加法模型和乘法模型。 加法模型（分层拆解） 选择合适的分类型变量作为分层标准，看看不同水平对应的核心指标如何变化，例如 \\[ \\begin{aligned} GMV &amp;= GMV_{新客户}+GMV_{老客户} \\\\ &amp;= \\sum_{i\\in 地区} GMV_i \\\\ &amp;= \\sum_{i\\in 品类} GMV_i \\end{aligned} \\] GMV：商品交易总额 总而言之，尝试着从人/物品/场景三个角度分割核心指标。 人 新老客户 城市 性别 年龄 物品 商品品类 商品价格带 场景 首页/搜索/广告/活动会场 设备（安卓/苹果/PC端/小程序） 乘法模型（链路拆解） 从事件的完整性入手，核心指标不是一蹴而就的，而是经过多轮环节之后“沉淀”下来的，例如 \\[ GMV = \\text{流量} \\times \\text{加购转化率} \\times \\text{下单转化率} \\times \\text{支付转化率} \\times \\text{客单价} \\] 考虑漏斗分析 加法+乘法模型 形如 \\[ GMV = \\sum_{i \\in 新老用户} {\\text{流量}_i \\times \\text{加购转化率}_i \\times \\text{下单转化率}_i \\times \\text{支付转化率}_i \\times \\text{客单价}_i} \\] 12.2.2.2 平均指标与相对指标 平均指标 对于平均指标考虑除法模型，也就是从指标的定义出发，即 \\[ \\bar{X} = \\frac{\\sum{Xf}}{\\sum f} \\] 称\\(X\\)为水平值（质量指标），\\(f/{\\sum f}\\)为结构值（数量指标）。 因此从水平和结构两个角度出发看看平均指标变动的地方。 例如\\(\\bar{X}\\)表示整体付费率，\\(X\\)表示不同维度的付费率，\\(f\\)表示不同维度的用户数，\\(\\sum{f}\\)表示总用户数。则整体付费率即可表示为总付费用户数/总用户数，其中总付费用户数为各个维度的付费用户数之和。 相对指标 \\[ X = \\frac{A}{B} \\] 相对指标直接由分子和分母处的指标计算得到，相对较为简单。 12.2.3 贡献度分析 当核心指标拆解为多个子指标的组合后，那么又该如何度量子指标对核心指标变动的影响？ 不妨记核心指标为\\(X\\)，子指标为\\(X_i\\)，报告期与基期分别以上标1和0表示。 12.2.3.1 加法模型 \\[ X = \\sum X_i \\] 绝对变动： \\[ X^1 - X^0 &amp;= \\sum{X_i^1} - \\sum{X_i^0}= \\sum{(X_i^1 - X_i^0)} \\] 相对变动： \\[ \\frac{X^1 - X^0}{X^1 - X^0} = 1 = \\sum{\\frac{(X_i^1 - X_i^0)}{X^1 - X^0}} \\] 这里的相对变动指的是子指标的变动对总指标变动的贡献度 12.2.3.2 乘法模型 \\[ X=\\prod X_i \\] 绝对变动： \\[ X^1 - X^0 = \\prod{X_i^1} - \\prod{X_i^0} \\] 以两因素（总体指标能够拆解为两个指标之积）为例，绝对变动可进一步拆分为 \\[ X^1 - X^0 = X_1^1 \\cdot X_2^1 - X_1^0 \\cdot X_2^0 = (X_1^1 \\cdot X_2^0 - X_1^0 \\cdot X_2^0 ) + (X_1^1 \\cdot X_2^1 - X_1^1 \\cdot X_2^0) \\] 其中称\\(X_1\\)为数量指标，\\(X_2\\)为质量指标。根据连环替换法，先将质量指标固定在基期，再逐渐将数量指标替换为报告期。那么\\((X_1^1 \\cdot X_2^0 - X_1^0 \\cdot X_2^0 )\\)度量了因数量变动而带来的绝对值变化，\\((X_1^1 \\cdot X_2^1 - X_1^1 \\cdot X_2^0)\\)度量了因质量变动带来的绝对值变化。 数量指标和质量指标是一个相对概念而不是绝对概念，习惯上将数量指标写在前面，质量指标写在后面 例如总产值可以分解为工人数、劳动生产率、产品价格三者之积 此时工人数相对于劳动生产率是数量指标，而劳动生产率相对于工人数是质量指标 相对的，劳动生产率是产品价格的数量指标，产品价格是劳动生产率的质量指标 相对变动： \\[ \\frac{X^1}{X^0} = \\frac{\\prod{X_i^1}}{\\prod{X_i^0}}=\\prod{\\frac{X_i^1}{X_i^0}} \\] 也就是说总体指标的相对变动可以分解为子指标相对变动之积。 以两因素为例，相对变动可进一步拆分为 \\[ \\frac{X^1}{X^0} = \\frac{X_1^1 \\cdot X_2^1}{X_1^0 \\cdot X_2^0} = \\frac{X_1^1 \\cdot X_2^0}{X_1^0 \\cdot X_2^0}\\cdot \\frac{X_1^1 \\cdot X_2^1}{X_1^1 \\cdot X_2^0} \\] 这进一步拆分是为了形式上的统一（基期与报告期），同时也是为后续的加法+乘法模型做铺垫 当然，也可通过取对数将乘法模型转化为加法模型，则 \\[ \\log{X} = \\sum{\\log{X_i}} \\] 那么可根据加法模型计算子指标变动对总指标的贡献度。 12.2.3.3 乘法+加法模型 事实上加法+乘法模型就涉及到同度量因素、报告期及基期的概念，可以了解下拉氏指数与帕氏指数 以两因素为例 \\[ X = \\sum{X_1 \\cdot X_2} \\] 绝对变动： \\[ X^1-X^0 = (\\sum{X_1^1X_2^0}- \\sum{X_1^0X_2^0}) + (\\sum{X_1^1X_2^1} - \\sum{X_1^1X_2^0}) \\] 相对变动： \\[ \\frac{X^1}{X^0} = \\frac{\\sum{X_1^1X_2^0}}{\\sum{X_1^0X_2^0}}\\cdot\\frac{\\sum{X_1^1X_2^1}}{\\sum{X_1^1X_2^0}} \\] 12.2.3.4 除法模型 \\[ X = \\frac{A}{B} \\] 绝对变动： \\[ X^1 - X^0 = \\frac{A^1}{B^1} - \\frac{A^0}{B^0} = (\\frac{A^1}{B^0} - \\frac{A^0}{B^0}) + (\\frac{A^1}{B^1}-\\frac{A^1}{B^0}) \\] 相对变动： \\[ \\frac{X^1}{X^0}=\\frac{A^1/B^1}{A^0/B^0} = \\frac{A^1/B^0}{A^0/B^0}\\cdot \\frac{A^1/B^1}{A^1/B^0} \\] 12.2.3.5 乘法+加法+除法模型 \\[ \\bar{X} = \\frac{\\sum{Xf}}{\\sum f} \\] 绝对变动： \\[ \\bar{X}^1-\\bar{X}^0=\\frac{\\sum{X^1f^1}}{\\sum f^1}-\\frac{\\sum{X^0f^0}}{\\sum f^0} = \\begin{pmatrix} \\frac{\\sum{X^0f^1}}{\\sum f^1}-\\frac{\\sum{X^0f^0}}{\\sum f^0}\\end{pmatrix}+\\begin{pmatrix} \\frac{\\sum{X^1f^1}}{\\sum f^1}-\\frac{\\sum{X^0f^1}}{\\sum f^1}\\end{pmatrix} \\] 记\\(\\bar{X}^*\\)为\\((\\sum{X^0f^1})/(\\sum f^1)\\)，则\\(\\bar{X}^*-\\bar{X}^0\\)度量了因结构变化导致的绝对值变化，\\(\\bar{X}^1-\\bar{X}^*\\)度量了因水平变化导致的绝对值变化。 相对变动： \\[ \\frac{\\bar{X}^1}{\\bar{X}^0} = \\frac{\\frac{\\sum{X^1f^1}}{\\sum f^1}}{\\frac{\\sum{X^0f^0}}{\\sum f^0}}=\\frac{\\frac{\\sum{X^0f^1}}{\\sum f^1}}{\\frac{\\sum{X^0f^0}}{\\sum f^0}}\\cdot\\frac{\\frac{\\sum{X^1f^1}}{\\sum f^1}}{\\frac{\\sum{X^0f^1}}{\\sum f^1}} \\] 同理，\\(\\bar{X}^*/\\bar{X}^0\\)度量了因结构变化导致的相对变化，\\(\\bar{X}^1/\\bar{X}^*\\)度量了因水平变化导致的相对变化。 12.2.4 从数据原因到业务原因 当找到异动所在之处，并度量了不同子指标对核心指标的影响时，这就是数据层面造成异动的原因。 那么又是怎样的业务原因会导致这样的数据原因呢？ 可以尝试如下方法： 从数据开始发生异动的时间入手，寻找与其可能相关的业务变动 业务原因对不同维度的影响是结构化的，从这种结构化差异寻找可能的业务变动 宏观因素的影响 例如在AI时代，考虑员工工资总额这一指标，其影响因素为员工数与人均工资。在AI时代（宏观因素），企业会加大AI的研发投入，因此在AI部门的招聘人数要明显多于其他部门（结构化差异），同时AI部门的员工的薪资往往高于其他部门，最终导致员工工资总额大幅上升。 12.2.5 案例分析 12.2.5.1 GMV 考虑GMV下降问题。 验真 数据变动是否属于正常范围？是否处于周期性变动中？统计口径是否变化？数据是否及时更新？ 维度拆解 \\[ GMV = UV \\times \\text{转化率} \\times \\text{客单价} \\] UV：不同城市（区域）？不同价值层级的用户？新老用户？年龄段？ 转化率：哪个环节的转化率下降最多？首页-&gt;搜索页-&gt;详情页-&gt;购物车-&gt;提交订单-&gt;支付成功 客单价：不同品类？新老客户的优惠力度？ 业务原因 新客户访问量下降，拉新或者流量曝光存在问题？ 客单价下降，促销力度是否减小？ 订单页转化率下降，是否出现卡顿等因素影响用户支付体验？ 12.2.5.2 DAU 验真 数据变动是否属于正常范围？是否处于周期性变动中？统计口径是否变化？数据是否及时更新？ 维度拆解 \\[ \\begin{aligned} DAU &amp;= \\text{新用户} + \\text{老用户} \\end{aligned} \\] 对于新用户，可以看看是从哪些渠道注册的，例如城市/广告/第三方渠道 对于老用户，可以看看行为路径，打开-&gt;浏览-&gt;点击-&gt;停留，看看哪个环节有问题 业务原因 产品迭代：UI调整/入口改版 技术问题：登录失败/页面加载延迟/崩溃率上升 运营因素：活动停止/内容更新少 外部因素：竞品/节日/天气 "],["da_3.html", "12.3 用户分析", " 12.3 用户分析 12.3.1 用户画像 用户画像通过标签体系描述用户是谁，长什么样子。 可以从基础信息、用户行为、业务场景三个角度提取特征，用于描述用户。 所提取特征应当与业务目标具有一致性，不需要冗余特征 基础信息无非年龄/性别/学历/收入/职业/城市/设备及其他静态特征，这类信息基本上不会变动。下面考虑部分场景应当如何设计画像标签。 12.3.1.1 电商 用户行为 访问频次 浏览深度 搜索 购物频率 评论 业务场景 消费金额 优惠券 以本人PDD羽毛球消费行为为例。 基础信息： 男/学生/珠海 男生，对抗较为激烈，羽毛球需求大 学生，收入有限 珠海，南方城市，羽毛球适合76速 用户行为： 每天登录 关注商品价格信息 搜索关键词为羽毛球（主要）、羽毛球拍、羽毛球鞋 爱好羽毛球 多次点击不同详情页的羽毛球商品 货比三家，不想买亏了 羽毛球消费间隔有长有短 间隔不固定，说明会在有合适的价格时囤球 羽毛球拍和羽毛球鞋消费间隔较长 高消费能力不足 业务场景： 只在有优惠券时下单 消费能力有限，不能说买就买TT 羽毛球消费金额在120~180区间/羽毛球拍在1000内/羽毛球鞋在600左右 羽毛球消费金额区间跨度较大，既有口粮球，又有细糠 羽毛球拍难以突破1000大关，消费能力不算特别高 羽毛球鞋消费价格较高，注重安全 综合上述信息，可知： 这是一位来自珠海的男同学，热衷羽毛球、价格敏感、常货比三家，偏好120–180元羽毛球、600元左右球鞋和千元以内球拍，通常在有优惠券时才下单，具有持续兴趣但消费能力有限、偏囤货型、精打细算型羽毛球用户画像。 12.3.1.2 视频平台 用户行为 登录次数 访问时段 使用时长 观看完成度 互动意愿（发弹幕/评论） 业务场景 内容类型偏好 视频作者偏好 视频格式（横屏/竖屏） 以本人的B站使用情况为例： 基础信息 男/学生 用户行为 每天登录 零碎时间都会刷 偶尔点赞，极少投币，从不评论发弹幕 业务场景 游戏/知识/影视/时政/生活 长视频/横屏 无任何充值行为 综合上述信息，可知： 这是一位男学生用户，属于高频使用、利用碎片时间观看内容的重度刷视频用户，偏好横屏长视频，兴趣覆盖游戏、知识、影视和时政类内容，但互动意愿弱、无付费行为，属于典型的“内容消费型、低参与度、非付费潜力”的被动观看型用户画像。 12.3.2 用户行为路径分析 该方法通过记录并分析用户在产品中的行为顺序，找到典型路径和关键转化/流失步骤，进而优化体验和增长。 该部分常用转化漏斗进行分析，特别适用于具有线性步骤的场景。 转化漏斗是单线程模型 如果有多条路径，则是路径分析。就像用美团点外卖一样，进入外卖界面，别人点“美食”，我点“拼好饭”，最终目的都是点外卖但实现路径不同 以视频平台为例： \\[ \\text{打开App} \\rightarrow \\text{浏览首页} \\rightarrow \\text{搜索视频} \\rightarrow \\text{点击视频} \\rightarrow \\text{观看} \\rightarrow \\text{退出视频} \\] 流量经过每个环节都会有所损耗，即存在转化率。漏斗分析则有助于识别完整过程中转化率较低的问题环节。 同时，在不同环节除了考虑转化率外，还应当考虑与之匹配的关键指标，例如在“浏览首页”时可以记录刷新次数指标，反映推荐内容的吸引力；在“观看视频”这一环节就可以考虑观看视频完成度这一指标，可以反映推荐匹配程度、开头吸引力不足、封面标题党等问题。 步骤 到达率 流失解释 打开APP 100% —— 浏览首页 95% 进入后无内容吸引 点击视频 60% 封面和标题匹配度低 播放完成度 &gt; 50% 30% 视频前广告干扰，加载体验差 点赞/收藏/关注 10% 缺乏激励，没有互动习惯 其余常见场景的路径如下所示 场景 漏斗示例 电商 访问 → 浏览 → 加购 → 下单 → 支付 视频/内容产品 曝光 → 点击 → 播放 → 完播 → 互动 社交产品 注册 → 完成资料 → 关注用户 → 发帖 → 留存 App增长 安装 → 激活 → 注册 → 次日留存 → 7日留存 广告转化 曝光 → 点击 → 到达页面 → 表单填写 → 成交 12.3.3 用户分层模型 用户分层根据业务逻辑将用户划分为不同价值和阶段，突出用户价值，以便提供差异化服务。 12.3.3.1 用户金字塔模型 用户金字塔模型是一种用于识别用户价值结构的分层框架，其核心思想是越往上层用户越少，但价值贡献越高；越往下层用户越多，但贡献越低。该模型常用于精细化运营、用户增长策略和用户价值管理。 用户类型 特点 占比 贡献 运营目标 运营策略 潜在用户 / 访客 浏览但未产生行为 最大 最低 进入体系 注册激励、首次内容推荐 新用户 / 激活用户 完成关键行为但粘性弱 较多 低 体验价值 新手引导、关键功能触发 轻度活跃用户 偶尔使用 中等 中等 提升使用频次 召回、个性化推荐 深度活跃用户 高频使用，形成习惯 少 高 形成习惯 订阅提醒、兴趣沉淀、多场景触达 核心价值用户 / KOC / 付费用户 贡献价值最大，可能传播 最少 最高 留存与传播 会员体系、权益提升、KOC激励、裂变奖励 KOC：关键意见消费者，影响力略低于KOL（小网红） KOL：关键意见领袖，具有强大影响力（大网红） 划分标准可以参考下面： 付费指标：GMV、LTV、付费次数 LTV：生命周期价值，指从用户生命周期内获得的收益总和 活跃指标：使用时长、使用频次、留存 互动指标：点赞、评论、分享 传播指标：KOC/KOL影响力 12.3.3.2 生命周期模型 生命周期模型是一种根据用户与产品关系变化，将用户划分为不同使用阶段的分析框架。核心理念是用户不是静止的，而是在不同时间节点经历从新访客到忠诚用户再到流失的动态变化过程。该模型可帮助企业识别增长阶段、定位问题并制定差异化运营策略。 阶段 行为表现 用户特征 运营目标 运营策略 新用户 首次访问，行为以浏览为主 价值感知弱 吸引体验兴趣 优化新人体验、奖励关键行为 活跃用户 使用频次增加 逐步形成使用习惯 提高使用频率和粘性 个性化推荐、付费转化刺激 核心用户 持续使用核心功能 价值贡献高 延长用户生命周期，提升用户价值 个性化服务、长期维护 沉默用户 使用频次降低 活跃度下降 降低流失风险 挖掘用户需求痛点，发送唤醒优惠 流失用户 长期未访问 价值不足或无需求 分析流失原因 低成本召回用户 不同阶段的关键指标： 生命周期阶段 指标示例 新用户 激活率、首次关键行为完成率、次日留存率 活跃用户 DAU、使用频次、D7/D30留存率 核心用户 留存周期、复购率/付费率、ARPU/ARPPU、LTV 沉默用户 回访周期、沉默比例、N日未登录分布、召回率 流失用户 流失率、反激活率 ARPU：每用户平均收入，总收入除以总用户数 ARPPU：每付费用户平均收入，总收入除以付费用户数 12.3.3.3 RFM模型 RFM模型是一种基于用户历史行为对用户价值进行评分和分层的经典模型。 R(Recency)，指最近一次消费时间，越近代表用户越活跃，复购意愿越强 F(Frequency)，指频次，越高代表用户习惯越强、粘性越高 M(Monetary)，指消费金额，越高代表用户价值越高 R反映的是活跃度，F反映习惯和忠诚度，M反映付费能力。 通过对这三个维度进行赋分，可以得到用户分层结构，例如每个维度有两个个等级评分： 分层类别 R F M 用户特征描述 运营策略举例 超级用户 高 高 高 最近消费、频次高、金额高，为平台贡献最大价值 VIP 服务、专属活动、会员福利、提前体验 高潜力增长用户 高 高 低 最近活跃且频次高，但消费金额不高 加购激励、优惠券、引导升级品类或客单价 高价值恢复用户 低 高 高 曾高消费高频，但近期沉默，是重点挽回客户 精准召回、专属优惠、情感价值回访 常规稳定用户 低 高 低 曾频繁使用但金额低，黏性一定但价值一般 推荐品类升级、优惠包、福利提升 新高消费用户 高 低 高 最近消费且金额高，但频次低，具提升潜力 推荐复购、售后关怀、推荐相关品类 新激活用户 高 低 低 刚完成首次行为或新激活，需要引导习惯形成 新手任务、上手教程、兴趣推荐引导 沉默低价值用户 低 低 低 低频低额且近期未访问，贡献价值有限 自动化运营触达、低成本营销策略 流失高价值用户 低 低 高 曾消费金额高但已沉默，是“挽回价值最高”的用户 定向强激励、私域客服触达、一对一关怀 "],["network.html", "13 网络型数据", " 13 网络型数据 这里的“网络型数据”指的是不同个体之间的某种联系编织为一张网络，例如省际经济联系网络、人口流动网络等。 "],["network_1.html", "13.1 构造联系", " 13.1 构造联系 对于想要分析的网络对象而言，“个体”往往是明确的，但“个体”之间的联系有时并不明朗。例如人口流动网络，第七次全国人口普查明确给出了“全国按现住地与五年前常住地分的人口”数据，其中“个体”是不同省市，“联系”是人口流动；而对于省际经济联系网络，虽然能明显看出“个体”是不同省市，但又该如何度量省际间的经济联系，这并不明确。 鉴于此，除了直接提取现有数据作为个体之间的联系，还可以对已有数据进行加工，将其转化为合适的联系。下面将罗列一些构造联系的方法。 13.1.1 引力模型 物理学上的引力模型为： \\[ F_{ij}=G\\frac{M_iM_j}{d_{ij}^2} \\tag{13.1} \\] 其中Fij表示个体之间的相互作用力，G为常数，Mi和Mj分别表示个体i和j的质量，dij为两者之间的距离。将式(13.1)稍加修改，即可转化为对两个个体之间某种联系的度量。 何勇[2]、张正峰[3]的文章都以引力模型为媒介来构造地区之间的联系。以何勇的文章为例，其文章将引力模型引入到文旅产业融合领域，得到下面的模型： \\[ F_{ij}=K_{ij}\\frac{\\sqrt[3]{D_i \\times P_i \\times M_i} \\times \\sqrt[3]{D_j \\times P_j \\times M_j}}{d_{ij}^2} \\tag{13.2} \\] \\[ K_{ij}=\\frac{P_i}{P_i+P_j} \\times \\frac{M_i}{M_i+M_j} \\tag{13.3} \\] 其中Fij为两个城市之间的文旅产业融合质量空间引力强度，D、P、M分别代表城市的文旅产业耦合度、文化产业增加值和国内旅游收入，dij代表城市之间的公路距离。Kij为修正系数，用于刻画两个城市之间Fij的非对称性。如果没有修正系数K，那么Fij将是对称的，即Fij=Fji。故该网络为加权有向网络。之后为了进行QAP分析而将原始矩阵的平均值作为阈值，把原始矩阵转化为0-1矩阵。 而在邱志萍[4]的文章中，虽然也是通过构建引力模型来构造省际间的商贸联系，但是该商贸联系是对称的，即为加权无向网络。为便于分析，以矩阵的平均值为阈值进行二值化处理，从而将加权无向网络转化为0-1无权无向网络。 在我的本科毕业论文中，我所构建的省际经济联系网络一开始是加权有向网络，为了简化网络，我将两个节点之间的边权重相加，得到加权无向网络，之后再以降序的标准选取所有边中累积比重在前80%的边，这样只对那些关键的联系进行讨论就可以了。 13.1.2 社会网络量表 张露露[5]利用Lubben社会网络量表(LSNS-6)来收集老年人的家庭及朋友两个网络，共包含6个条目。 该方法的实施就得仰仗抽样调查技术了。 References [2] 何勇. 石伟. 陈旭辉. 基于社会网络分析的文旅产业融合空间结构研究[J]. 东南大学学报（哲学社会科学版）, 2024, 26(3): 107-116. [3] 张正峰. 张栋. 基于社会网络分析的京津冀地区碳排放空间关联与碳平衡分区[J]. 中国环境科学, 2023, 43(4): 2057-2068. [4] 邱志萍. 刘举胜. 何建佳. 我国商贸流通网络的结构特征及驱动因素——基于引力模型的社会网络分析[J]. 中国流通经济, 2023, 37(2): 31-42. [5] 张露露. 刘安诺. 尤梅. 等. 社区老年人老化态度的潜在剖面分析及其与社会网络的关系[J]. 军事护理, 2024, 41(8): 39-42, 73. "],["network_2.html", "13.2 网络结构特征", " 13.2 网络结构特征 带“社会网络分析”关键词的社科类论文，大多都是拿网络中的一些指标进行讨论，在此罗列一些文章中较为常见的指标及具有强烈个人主观色彩的通俗解释。 指标 含义 网络密度 衡量网络的紧凑程度 点度中心性 值越大，朋友越多 介数中心性 值越大，类似交通枢纽、门户 接近度中心性 值越大，说明处于网络的中心位置 聚类系数 值越大，说明我的朋友之间大概率也是朋友 特征向量中心性 值越大，你有很多大佬朋友，你也更可能是大佬 平均路径长度 值越小，网络越紧凑 小世界指数 值越大，小世界特征越突出 互惠性 值越小，个体间地位越不平等，存在非对称性 网络结构特征可分为整体特征（如网络密度、平均路径长度及后面的凝聚子群分析，对应整体网络）与个体特征（如特征向量中心性、介数中心性，对应个体网络）两个角度进行分析。 "],["network_3.html", "13.3 拓展分析", " 13.3 拓展分析 除了在研究中讨论网络结构特征的一些指标，还有没有进一步的分析？ 13.3.1 凝聚子群 何勇[2]、邱志萍[4]、黄文胜[6]的文章中提到了凝聚子群分析。 和社区发现有什么区别？ 13.3.2 社区发现 张鑫[7]和端祥宇[8]的文章介绍了一些社区发现算法。社区发现指的是探索复杂网络中存在的聚簇结构，即社区。社区内部节点之间关系相对紧密、社区之间节点关系相对稀疏。 13.3.3 核心-边缘分析 邱志萍[4]的文章中提到了核心-边缘分析。核心-边缘分析是用于区分不同节点在网络中所扮演的角色，谁是核心节点，谁是边缘节点。 13.3.4 QAP分析 何勇[2]、邱志萍[4]的文章中提到了QAP(Quadratic Assignment Procedure)分析。QAP分析可进一步分为QAP相关分析和QAP回归分析。简单来说，平时都是两个变量之间的相关系数检验，以及多个变量之间的回归分析，当你把变量替换成矩阵之后，就是QAP相关分析及QAP回归分析了。关于QAP的介绍可参考知乎的这篇文章，其中提到的刘军[9]的文章更为详细。 我的理解是：QAP分析在每次计算时将矩阵视作长向量，在此基础上，先进行真实矩阵之间的相关分析或者回归分析，之后利用随机置换的手段（同时置换某一个矩阵的行与列，不破坏原始数据），重复进行相关分析或者回归分析，从而得到关于结果的分布，最终看看真实矩阵的结果落在该分布的哪一端，是否处在拒绝域上，从而作出判断。 13.3.5 计量模型 姜小鱼[10]在构建网络后，将网络结构特征的一些指标作为核心解释变量纳入到计量模型中，探讨其对全球价值链参与度及地位的影响。 王珊珊[11]运用TERGM模型（Temporal Exponential Random Graph Models，时间指数随机图模型）来探讨经济制裁对国际贸易格局的冲击。TERGM模型的主要特点是可以同时考虑网络中的内生因素（如网络中的结构效应）和外生因素（如网络外部的影响因素），并能够分析这些因素如何影响网络的动态演变。 13.3.6 综合评价 张亚明[12]运用熵权TOPSIS法对各个节点的中心性指标及连接情况进行综合评价，得到关于各节点在传播能力与传播效力之间的表现。 References [2] 何勇. 石伟. 陈旭辉. 基于社会网络分析的文旅产业融合空间结构研究[J]. 东南大学学报（哲学社会科学版）, 2024, 26(3): 107-116. [4] 邱志萍. 刘举胜. 何建佳. 我国商贸流通网络的结构特征及驱动因素——基于引力模型的社会网络分析[J]. 中国流通经济, 2023, 37(2): 31-42. [6] 黄文胜. 肖利斌. 郑向敏. 民营酒店员工社会网络类型和结构特性研究——基于中心性、凝聚子群的视角[J]. 企业经济, 2020(5): 87-94. [7] 张鑫. 刘秉权. 王晓龙. 复杂网络中社区发现方法的研究[J]. 计算机工程与应用, 2015, 51(24): 1-7. [8] 端祥宇. 袁冠. 孟凡荣. 动态社区发现方法研究综述[J]. 计算机科学与探索, 2021, 15(4): 612-630. [9] 刘军. QAP:测量\"关系\"之间关系的一种方法[J]. 社会, 2007, 27(4): 164-174. [10] 姜小鱼. 陈秧分. 农业对外投资布局对母国参与全球价值链的影响——基于社会网络分析视角[J]. 华中农业大学学报（社会科学版）, 2023(3): 44-53. [11] 王珊珊. 孙程九. 经济制裁是否破坏了增加值贸易网络——基于全球制裁数据的社会网络分析[J]. 国际贸易问题, 2023(2): 74-91. [12] 张亚明. 付尧飞. 宋雯婕. 等. 社交媒体时代全球智库国际传播力研究:基于87家智库Twitter账户的社交网络分析[J]. 智库理论与实践, 2024, 9(2): 48-59. "],["network_4.html", "13.4 其他启示", " 13.4 其他启示 可以在地图上展示网络[3]。 可以适当简化网络，如提取c核心网络（对节点度或边权重设置阈值）、提取自我中心网络（聚焦于某个节点及与之相关联的邻居）、划分社区并将社区视为新的节点… 可以对不同时点上同个研究对象的网络进行QAP相关性分析，探讨网络的变化特点[4]。 References [3] 张正峰. 张栋. 基于社会网络分析的京津冀地区碳排放空间关联与碳平衡分区[J]. 中国环境科学, 2023, 43(4): 2057-2068. [4] 邱志萍. 刘举胜. 何建佳. 我国商贸流通网络的结构特征及驱动因素——基于引力模型的社会网络分析[J]. 中国流通经济, 2023, 37(2): 31-42. "],["animation.html", "14 Manim动画", " 14 Manim动画 这一部分展示Manim动画成品。 "],["animation_1.html", "14.1 最小二乘与投影矩阵", " 14.1 最小二乘与投影矩阵 源码下载链接： 最小二乘与投影矩阵源码 视频链接：几何视角 &amp; B站：最小二乘与投影矩阵 "],["people.html", "15 七普数据可视化 ", " 15 七普数据可视化 "],["people_1.html", "15.1 数据来源及说明", " 15.1 数据来源及说明 data_all.xlsx：全国人口状况 “自然增长率”：取自年鉴“1-6各地区人口出生率、死亡率和自然增长率” “年龄结构”：取自七普上册“3-2各地区人口年龄构成(一)” “一般年龄结构”：取自年鉴“1-3人口年龄结构和抚养比” “人口流动”：取自七普下册“7-7全国按现住地和五年前常住地分的人口” “人口迁移前十省”：根据表“人口流动”计算得到，不参与代码 “新生儿构成”：取自七普下册“6-1各地区分性别、孩次的出生人口” “分时期分年龄分孩次生育率”：取自年鉴“2-40全国育龄妇女分年龄孩次的生育状况” data_db.xlsx：东北人口状况 “自然增长率”：取自年鉴“1-6各地区人口出生率、死亡率和自然增长率” “年龄结构与抚养比”：取自七普上册“3-2各地区人口年龄构成(一)” “人口流动”：取自七普下册“7-7全国按现住地和五年前常住地分的人口” “分年龄生育率”：取自七普下册“6-4各地区育龄妇女年龄别生育率” “分孩次生育率”：由于没有各地区平均育龄妇女人数的数据，故取七普上册“1-5各地区分年龄、性别的人口”中的女性育龄人口数进行替代，并取七普下册“6-1各地区分性别、孩次的出生人口”中孩次人数，计算得到生育率，虽然存在一定误差，但仍具有一定的参考价值 "],["people_2.html", "15.2 全国人口状况", " 15.2 全国人口状况 15.2.1 人口自然增长率 需要额外导入的包： library(gghighlight) # 元素高亮 gghighlight()函数 绘制全国及各省市的自然增长率轮廓图。 growth_df &lt;- openxlsx::read.xlsx(&#39;./dataset/df_all.xlsx&#39;, sheet=&#39;自然增长率&#39;) head(growth_df) ## province ngr_15 ngr_16 ngr_17 ngr_18 ngr_19 ngr_20 ## 1 全国 4.96 5.86 5.32 3.81 3.34 1.445681 ## 2 北 京 3.01 4.12 3.76 2.66 2.63 1.800000 ## 3 天 津 0.23 1.83 2.60 1.25 1.43 0.070000 ## 4 河 北 5.56 6.06 6.60 4.88 4.71 0.940000 ## 5 山 西 4.42 4.77 5.61 4.31 3.27 1.240000 ## 6 内蒙古 2.40 3.34 3.73 2.40 2.57 -0.100000 # 数据预处理，为“全国”添加高亮标签 growth_df &lt;- growth_df %&gt;% mutate(highlight=c(&#39;1&#39;,rep(&#39;0&#39;,times=31))) growth_df &lt;- growth_df %&gt;% pivot_longer( cols = c(2:7),names_to = &#39;year&#39;,values_to = &#39;value&#39; ) ggplot(growth_df)+ geom_line(aes(x=year,y=value,group=province), linewidth=1.2,alpha=0.8)+ gghighlight(highlight==&#39;1&#39;,use_direct_label = T, label_key = province,use_group_by = F)+ theme_bw()+ labs(y=&#39;人口自然增长率‰&#39;,x=&#39;年份&#39;)+ scale_x_discrete(labels = c(&#39;2015&#39;,&#39;2016&#39;,&#39;2017&#39;,&#39;2018&#39;,&#39;2019&#39;,&#39;2020&#39;)) 图 15.1: 全国及各省市的自然增长率轮廓图 15.2.2 年龄结构 age_df &lt;- openxlsx::read.xlsx(&#39;./dataset/df_all.xlsx&#39;, sheet=&#39;年龄结构&#39;) head(age_df) ## province young middle old ## 1 全国 17.97 68.50 13.52 ## 2 北 京 11.84 74.86 13.30 ## 3 天 津 13.47 71.77 14.75 ## 4 河 北 20.22 65.85 13.92 ## 5 山 西 16.35 70.74 12.90 ## 6 内蒙古 14.04 72.90 13.05 age_demo &lt;- age_df %&gt;% pivot_longer( cols = 2:4,names_to = &#39;age&#39;,values_to = &#39;value&#39; ) age_demo$age &lt;- factor(age_demo$age,levels = c(&#39;old&#39;,&#39;middle&#39;,&#39;young&#39;)) age_demo$province &lt;- factor(age_demo$province,levels = unique(as.vector(age_df$province))) age_demo %&gt;% ggplot(aes(x=fct_rev(province),y=value,fill=age))+ geom_bar(stat = &#39;identity&#39;)+ labs(y=&#39;比重%&#39;,x=&#39;&#39;,fill=&#39;&#39;)+ theme(axis.text.y = element_text(face=&#39;bold&#39;))+ geom_hline(yintercept = 93,linetype=&#39;dashed&#39;,color=&#39;blue&#39;)+ scale_y_continuous(breaks = c(0,10,20,30,40,50,60,70,80,90,93,100))+ scale_fill_discrete(labels=c(&#39;65岁及以上&#39;,&#39;15-64岁&#39;,&#39;0-14岁&#39;))+ coord_flip() 图 15.2: 全国及各省市年龄结构 age_series &lt;- openxlsx::read.xlsx(&#39;./dataset/df_all.xlsx&#39;, sheet=&#39;一般年龄结构&#39;) head(age_series) ## year young middle old gross_dep young_dep old_dep ## 1 2015 16.5 73.0 10.5 36.96755 22.63329 14.33425 ## 2 2016 16.7 72.5 10.8 37.90000 22.90000 15.00000 ## 3 2017 16.8 71.8 11.4 39.27577 23.39833 15.87744 ## 4 2018 16.9 71.2 11.9 40.44104 23.67523 16.76580 ## 5 2019 16.8 70.6 12.6 41.50000 23.80000 17.80000 ## 6 2020 17.9 68.6 13.5 45.90000 26.20000 19.70000 age_series &lt;- age_series %&gt;% pivot_longer( cols = c(2,4),names_to = &#39;age&#39;,values_to = &#39;value&#39; ) age_series %&gt;% ggplot()+ geom_line(aes(x=year,y=value,group=age,color=age))+ geom_point(aes(x=year,y=value,group=age,color=age))+ scale_y_continuous(breaks = seq(10,18,1))+ scale_color_manual(values = c(&#39;blue&#39;,&#39;red&#39;), limits=c(&#39;young&#39;,&#39;old&#39;), labels=c(&#39;0-14岁&#39;,&#39;65岁及以上&#39;))+ labs(x=&#39;年份&#39;,y=&#39;比重%&#39;,color=&#39;&#39;) 图 15.3: 一般年龄结构 15.2.3 人口流动 需要导入额外的包： library(pheatmap) # 热力图 flow_df &lt;- openxlsx::read.xlsx(&#39;./dataset/df_all.xlsx&#39;, sheet=&#39;人口流动&#39;) head(flow_df) ## province 北京 天津 河北 山西 内蒙古 辽宁 吉林 黑龙江 上海 江苏 浙江 安徽 ## 1 北京 0 6809 60188 17719 8200 13080 8214 16134 2241 6327 3181 8262 ## 2 天津 7417 0 21018 5354 3278 3792 3129 7121 383 1907 1000 3003 ## 3 河北 23741 5064 0 6023 5240 5921 4466 11763 499 2822 1670 3947 ## 4 山西 2959 1065 7134 0 2454 1256 777 1209 496 2024 936 1669 ## 5 内蒙古 2970 937 6552 6866 0 4847 3239 4956 292 1294 693 1239 ## 6 辽宁 4003 1317 5158 1897 7685 0 13076 21716 783 2042 1264 2528 ## 福建 江西 山东 河南 湖北 湖南 广东 广西 海南 重庆 四川 贵州 云南 西藏 陕西 ## 1 2494 3514 21008 28004 7460 4671 4164 1635 650 2682 7991 1867 2055 274 6301 ## 2 723 1008 8357 8077 1776 967 733 1030 271 729 2412 1389 925 179 1326 ## 3 1198 1524 7418 10851 3646 1668 1345 756 423 1567 4275 1299 1317 89 2785 ## 4 959 674 3231 7463 1899 798 753 347 323 1267 4151 836 721 64 3960 ## 5 534 490 4101 3539 1148 586 551 229 110 521 2172 434 415 25 5179 ## 6 982 667 4908 4647 1131 920 1396 986 268 675 2277 1652 828 125 946 ## 甘肃 青海 宁夏 新疆 总流入 总流出 净流出 ## 1 5666 541 1005 2306 254643 126748 -127895 ## 2 2912 382 384 1292 92274 40762 -51512 ## 3 1951 463 427 949 115107 193487 78380 ## 4 1132 350 315 424 51646 112519 60873 ## 5 4825 388 1991 399 61522 63224 1702 ## 6 1241 283 243 1319 86963 94028 7065 group &lt;- c( &#39;东部地区&#39;, &#39;东部地区&#39;,&#39;东部地区&#39;,&#39;中部地区&#39;,&#39;西部地区&#39;,&#39;东北地区&#39;, &#39;东北地区&#39;,&#39;东北地区&#39;,&#39;东部地区&#39;,&#39;东部地区&#39;,&#39;东部地区&#39;, &#39;中部地区&#39;,&#39;东部地区&#39;,&#39;中部地区&#39;,&#39;东部地区&#39;,&#39;中部地区&#39;, &#39;中部地区&#39;,&#39;中部地区&#39;,&#39;东部地区&#39;,&#39;西部地区&#39;,&#39;东部地区&#39;, &#39;西部地区&#39;,&#39;西部地区&#39;,&#39;西部地区&#39;,&#39;西部地区&#39;,&#39;西部地区&#39;, &#39;西部地区&#39;,&#39;西部地区&#39;,&#39;西部地区&#39;,&#39;西部地区&#39;,&#39;西部地区&#39; ) flow_demo &lt;- flow_df %&gt;% mutate(region=group) %&gt;% relocate(region,.before = province) flow_demo$region &lt;- factor(flow_demo$region,levels = c(&#39;东部地区&#39;,&#39;东北地区&#39;,&#39;中部地区&#39;,&#39;西部地区&#39;)) flow_demo &lt;- flow_demo %&gt;% arrange(region) flow_demo &lt;- flow_demo[,c(1,2,match(flow_demo$province,colnames(flow_demo)[3:33])+2)] flow_mat &lt;- as.matrix(flow_demo[,3:33] %&gt;% mutate(across(1:31,~(.-min(.))/(max(.)-min(.))))) rownames(flow_mat) &lt;- colnames(flow_mat) diag(flow_mat) &lt;- NA #会有一堆警告，没找到原因，猜测是编码问题，不影响正常使用 pheatmap(flow_mat,cluster_rows = F,cluster_cols = F, angle_col = 0,legend = F,fontsize_row = 8 ,fontsize_col = 8, gaps_row = c(10,13,19),gaps_col = c(10,13,19)) 图 15.4: 全国人口流动 "],["penalty.html", "16 变量选择与惩罚函数", " 16 变量选择与惩罚函数 这个章节主要介绍基于惩罚函数的变量选择方法，由于精力有限，只能对方法进行大致介绍，严格的推导与证明还是推荐查看原文献。 "],["penalty_1.html", "16.1 准备", " 16.1 准备 16.1.1 范数 向量\\(x=(x_1, \\cdots,x_n)\\)的\\(L_p\\)范数被定义为： \\[ || x ||_p=\\sqrt[p]{\\sum_i^{n} | x_i | ^p} \\tag{16.1} \\] 常见的有\\(L_1\\)范数与\\(L_2\\)范数（一般会以平方的形式出现），如下所示： \\[ || x ||_1 = \\sum_i^n | x_i | \\tag{16.2} \\] \\[ || x ||_2^2 = \\sum_i^n x_i^2 \\tag{16.3} \\] 以二元系数为例，\\(L_p\\)范数的示例如图16.1所示： 图 16.1: Lp范数示意图 16.1.2 目标函数 目标函数是我们的优化目标，其一般形式为： \\[ Q(\\beta)=L(\\beta | y,X)+P_{\\lambda}(| \\beta |) \\tag{16.4} \\] 其中\\(y=(y_1,\\cdots ,y_n)&#39;\\)是因变量，\\(X=(x_1,\\cdots,x_p)\\)是\\(n \\times p\\)维设计矩阵，\\(\\beta=(\\beta_1,\\dots,\\beta_p)&#39;\\)为系数向量，\\(L(\\cdot)\\)和\\(P_\\lambda(\\cdot)\\)分别表示具体的损失函数和惩罚函数。 可以看到，目标函数由两部分组成：损失函数\\(L(\\beta | y,X)\\)与惩罚函数\\(P_{\\lambda}(| \\beta |)\\)。其示意图如图16.2所示。 图 16.2: 目标函数分解 下面分别对\\(OLS\\)、\\(Lasso\\)、\\(Ridge\\)进行数据模拟。 假设真实函数为 \\[ y=3 \\times x_1+ 0 \\times x_2 \\tag{16.5} \\] 其中\\(y\\)仅与\\(x_1\\)有关。对此我们生成20组观测值， # 生成数据 set.seed(111) x1 &lt;- runif(20) x2 &lt;- runif(20) epsilon &lt;- rnorm(20) y &lt;- 2*x1+epsilon df &lt;- tibble(y=y, x1=x1, x2=x2) 然后计算\\(OLS\\)、\\(Lasso\\)、\\(Ridge\\)对应的估计值。 ols &lt;- lm(y~0+x1+x2, df) # 0表示不加截距项 library(glmnet) # glmnet是弹性网回归，alpha的值控制L1正则和L2正则之间的权重 # 当alpha=1即lasso回归；当alpha=2即ridge回归 lasso_cv &lt;- cv.glmnet(x=as.matrix(df[,2:3]), y=df$y, family=&#39;gaussian&#39;, intercept=F, alpha=1, nfolds=5) lasso &lt;- glmnet(x=as.matrix(df[,2:3]), y=df$y, family=&#39;gaussian&#39;, intercept=F, alpha=1, lambda=lasso_cv$lambda.min) ridge_cv &lt;- cv.glmnet(x=as.matrix(df[,2:3]), y=df$y, family=&#39;gaussian&#39;, intercept=F, alpha=0, nfolds = 5) ridge &lt;- glmnet(x=as.matrix(df[,2:3]), y=df$y, family=&#39;gaussian&#39;, intercept=F, alpha=0, lambda=ridge_cv$lambda.min) paste0(&#39;OLS_1:&#39;, round(ols$coefficients[1],3), &#39;；OLS_2:&#39;, round(ols$coefficients[2],3)) paste0(&#39;Lasso_1:&#39;, round(coef(lasso)[2],3), &#39;；Lasso_2:&#39;, round(coef(lasso)[3],3)) paste0(&#39;Ridge_1:&#39;, round(coef(ridge)[2],3), &#39;；Ridge_2:&#39;, round(coef(ridge)[3],3)) [1] &quot;OLS_1:2.554；OLS_2:-0.742&quot; [1] &quot;Lasso_1:1.405；Lasso_2:0&quot; [1] &quot;Ridge_1:0.959；Ridge_2:0.22&quot; 接下来看看不同系数是怎样影响目标函数的取值。 # 目标函数 objective_fun &lt;- function(beta1, beta2, df, type=&#39;ols&#39;, lambda){ xxx = df response = as.matrix(xxx$y) predictor = as.matrix(xxx[,2:3]) loss = sum((response - predictor %*% c(beta1, beta2))^2)/(2*nrow(df)) switch(type, &#39;ols&#39; = { penalty = 0 lambda = 0 }, &#39;lasso&#39; = { penalty = abs(beta1) + abs(beta2) }, &#39;ridge&#39; = { penalty = (beta1^2 + beta2^2)/2 } ) result = loss + lambda*penalty result } # 网格采样，设置系数组合 grid_parameter &lt;- expand.grid(seq(-3, 3, 0.05), seq(-3, 3, 0.05)) colnames(grid_parameter) &lt;- c(&#39;beta1&#39;, &#39;beta2&#39;) # 计算不同采样点对应的目标函数值 grid_parameter &lt;- grid_parameter %&gt;% rowwise() %&gt;% mutate(OLS = objective_fun(beta1, beta2, df), L1 = abs(beta1) + abs(beta2), Lasso = objective_fun(beta1, beta2, df, type=&#39;lasso&#39;, lambda=lasso$lambda), L2 = beta1^2 + beta2^2, Ridge = objective_fun(beta1, beta2, df, type=&#39;ridge&#39;, lambda=ridge$lambda)) head(grid_parameter) # A tibble: 6 × 7 # Rowwise: beta1 beta2 OLS L1 Lasso L2 Ridge &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 -3 -3 7.62 6 10.1 18 46.7 2 -2.95 -3 7.54 5.95 10.0 17.7 46.0 3 -2.9 -3 7.46 5.9 9.90 17.4 45.3 4 -2.85 -3 7.37 5.85 9.80 17.1 44.6 5 -2.8 -3 7.29 5.8 9.70 16.8 43.9 6 -2.75 -3 7.21 5.75 9.60 16.6 43.2 glmnet()函数[13]使用的目标函数为\\(\\frac{RSS}{2n}+\\lambda \\ast (\\frac{(1-\\alpha )}{2}|| \\beta ||_2^2+\\alpha || \\beta ||_1)\\)，为保持损失函数一致，故将ols的损失函数乘上\\(\\frac{1}{2n}\\)，最小二乘解保持不变。 下面可视化不同回归的目标函数值变化情况。 图 16.3: OLS+Lasso 图 16.4: contour of OLS+Lasso \\(L_1\\)正则项的存在使得目标函数有非常明显的沟槽。 图 16.5: OLS+Ridge 图 16.6: contour of OLS+Ridge glmnet()函数在不同的\\(\\lambda\\)下使用坐标下降算法[13]进行迭代，满足一定条件后就会停止迭代，因此得到的结果会与模拟数据的最小值点存在差异。 惩罚函数的存在使得系数向原点方向收缩，并改变了目标函数的形态，形态决定功能，由此产生目标解。 为什么损失函数是椭圆状？对于任意一个估计\\(\\tilde \\beta\\)，有 \\[ \\begin{aligned} Q&amp;=(Y-X\\tilde{\\beta})&#39;(Y-X\\tilde{\\beta}) \\\\ &amp;=(Y-\\hat{Y}+\\hat{Y}-X\\tilde{\\beta})&#39;(Y-\\hat{Y}+\\hat{Y}-X\\tilde{\\beta}) \\\\ &amp;=(e+X(\\hat{\\beta}-\\tilde{\\beta}))&#39;(e+X(\\hat{\\beta}-\\tilde{\\beta})) \\\\ &amp;=e&#39;e+e&#39;X(\\hat{\\beta}-\\tilde{\\beta})+(\\hat{\\beta}-\\tilde{\\beta})&#39;X&#39;e+(\\hat{\\beta}-\\tilde{\\beta})&#39;X&#39;X(\\hat{\\beta}-\\tilde{\\beta}) \\\\ &amp;=e&#39;e+(\\hat{\\beta}-\\tilde{\\beta})&#39;X&#39;X(\\hat{\\beta}-\\tilde{\\beta}) \\end{aligned} \\tag{16.6} \\] 其中\\(X&#39;e=0\\)，\\(\\hat \\beta\\)是最小二乘估计。对于\\((\\hat{\\beta}-\\tilde{\\beta})&#39;X&#39;X(\\hat{\\beta}-\\tilde{\\beta})\\)，有 \\[ \\begin{aligned} (\\hat{\\beta}-\\tilde{\\beta})&#39;X&#39;X(\\hat{\\beta}-\\tilde{\\beta})&amp;=(\\hat{\\beta}-\\tilde{\\beta})&#39;Q\\Lambda Q&#39;(\\hat{\\beta}-\\tilde{\\beta}) \\\\ &amp;=(Q&#39;(\\hat{\\beta}-\\tilde{\\beta}))&#39;\\Lambda(Q&#39;(\\hat{\\beta}-\\tilde{\\beta})) \\end{aligned} \\tag{16.7} \\] 可见\\((Q&#39;(\\hat{\\beta}-\\tilde{\\beta}))&#39;\\Lambda(Q&#39;(\\hat{\\beta}-\\tilde{\\beta}))\\)就是椭圆的表达式。 References [13] FRIEDMAN J. HASTIE T. TIBSHIRANI R. Regularization paths for generalized linear models via coordinate descent[J]. Journal of Statistical Software, 2010, 33(1): 1. "],["penalty_2.html", "16.2 单变量选择", " 16.2 单变量选择 在Fan &amp; Li[14]的论文中提到一个好的惩罚函数产生的估计应该满足如下三条性质： 无偏性 当真实系数足够大时，对它的估计应当是无偏的。 稀疏性 估计结果应当是分段的，当系数估计值较小时应当被设置为0。 连续性 估计结果应当是连续的。 文章[14]中提到在原点奇异即可满足稀疏性和连续性。 16.2.1 Lasso glmnet包的glmnet()函数 Tibshirani[15]提出的\\(Lasso\\)是用于变量选择的经典方法之一。其惩罚函数如下所示 \\[ P_\\lambda(\\beta)=\\lambda|| \\beta ||_1=\\lambda\\sum\\limits_{i=1}^p |\\beta_i| \\tag{16.8} \\] 式(16.8)表明\\(Lasso\\)就是在目标函数中添加了\\(L_1\\)正则项。\\(L_1\\)正则项的\\(contour\\)图如图16.1所示，函数图如图16.7所示，3D图如图16.3所示。 \\(Lasso\\)的解为 \\[ \\hat{\\beta_j}=sgn(z)(|z|-\\lambda)_{+} \\tag{16.9} \\] 其中\\(z\\)表示最小二乘估计的解，下同。解的图像如图16.8所示。由图可知，该解满足连续性和稀疏性，但不满足无偏性。 下面的优缺点来自各个方法的原文献及相关文献，下同。 优点 通过\\(L_1\\)正则化项，可以将一些系数压缩到零，从而实现变量选择 在高维数据中表现良好，尤其在信噪比大的情况下。 缺点 \\(Lasso\\)可能会引入较大的偏差，特别是在样本量不充足时。 \\(Lasso\\)不满足\\(Oracle\\)性质，即它不能在变量选择上与已知真实模型的情况进行比较。 需要选择合适的正则化参数 16.2.2 Hard &amp; Soft Threshold \\(Hard \\; Threshold\\)（硬阈值）与\\(Soft \\; Threshold\\)（软阈值）初见于文献[16]。解的形式为 \\[ \\begin{aligned} \\hat\\beta_{H}&amp;=zI(|z|&gt;\\lambda) \\\\ \\hat\\beta_{S}&amp;=sgn(z)(|z|-\\lambda)_{+} \\end{aligned} \\tag{16.10} \\] 可以发现，软阈值的解与\\(Lasso\\)的解一模一样。Tibshirani在其文章[15]中也指出了这种联系。 在Fan &amp; Li[14]的文章中给出了当惩罚函数为\\(P_\\lambda(|\\theta|)=\\lambda^2-(|\\theta|-\\lambda)^2I(|\\theta|-\\lambda)\\)时硬阈值的惩罚函数及解的图像，分别如图16.7、图16.8所示。 由图可知，硬阈值的解满足无偏性、稀疏性，不满足连续性。软阈值的解满足连续性、稀疏性，不满足无偏性。 16.2.3 SCAD ncvreg包的ncvreg()函数 Fan &amp; Li[14]提出的\\(SCAD\\)惩罚函数很好地弥补了硬阈值和软阈值的不足，同时满足无偏性、稀疏性和连续性。更进一步地，文中证明了\\(SCAD\\)法具有\\(Oracle\\)性质。 \\(Oracle\\)性质：(1)能正确选择模型，即对真实为零的系数，方法能将其估计值压缩到零，对真实非零的系数，方法能正确识别对应的变量；(2)真实非零系数的估计值是无偏或近似无偏的，且具有渐进正态性。 原文从导数的角度给出了\\(SCAD\\)函数的定义 \\[ P&#39;_\\lambda(\\theta)=\\lambda\\{I(\\theta\\leq\\lambda)+\\frac{(a\\lambda-\\theta)_+}{(a-1)\\lambda}I(\\theta \\gt \\lambda)\\}, \\; a\\gt2 \\; \\&amp; \\; \\theta \\gt 0 \\tag{16.11} \\] 并且也给出了解的表达式 \\[ \\hat{\\theta}= \\begin{cases} sgn(z)(|z|-\\lambda)_+, &amp; |z| \\leq 2\\lambda \\\\ \\{(a-1)z-sgn(z)a\\lambda\\}/(a-2), &amp; 2\\lambda \\lt |z| \\leq a\\lambda \\\\ z, &amp; |z| \\gt a\\lambda \\end{cases} \\tag{16.12} \\] 惩罚函数图像及解分别如图16.7、图16.8所示。 图 16.7: 硬阈值、软阈值和SCAD的惩罚函数 图 16.8: 硬阈值、软阈值和SCAD的解 优点 结合了硬阈值和\\(L_1\\)惩罚的优点，提供了连续且具有阈值特性的解。 通过局部二次近似，\\(SCAD\\)惩罚能够减少大系数的惩罚，同时保持变量选择的特性。 \\(SCAD\\)在高信噪比情况下表现良好，能够准确地识别出重要的变量。 缺点 需要估计额外的参数，这可能增加计算复杂度。 \\(SCAD\\)的计算可能比\\(Lasso\\)和\\(Adaptive \\; Lasso\\)更复杂，尤其是在处理大量变量时。 16.2.4 Adaptive Lasso glmnet包的glmnet()函数 or ARGOS包的alasso()函数 Hui Zou[17]改进了\\(Lasso\\)回归，在其文章中提出了\\(Adaptive \\; Lasso\\)，能够根据不同系数分配不同的权重，解决了\\(Lasso\\)变量选择不一致的问题，具有\\(Oracle\\)性质。 \\(Adaptive \\; Lasso\\)的惩罚函数为 \\[ P_{\\lambda, w}(\\beta)=\\lambda\\sum_{i=1}^p w_i|\\beta_i| \\tag{16.13} \\] 其中\\(w_i\\)为权重，可通过数据驱动得到。以\\(3|\\beta_1|+|\\beta_2|\\)为例绘制惩罚函数图。 图 16.9: Adaptive Lasso的惩罚函数 \\(Adaptive \\; Lasso\\)的解如图16.10所示，显然满足无偏性、稀疏性和连续性 图 16.10: Adaptive Lasso的解 优点 通过使用数据依赖的权重对不同的系数进行惩罚，改进了\\(Lasso\\)方法。 具有\\(Oracle\\)性质。 在预测准确性和变量选择上，\\(Adaptive \\; Lasso\\)能够平衡\\(Lasso\\)和\\(SCAD\\)的优点。 缺点 需要选择合适的正则化参数，这可能需要依赖于数据驱动的方法。 T老师建议，当\\(p\\)很大时，\\(SCAD\\)优于\\(Adaptive \\; Lasso\\)。当\\(p&gt;&gt;n\\)时，选用\\(Sure \\; Independence \\; Screening\\)法。 References [14] FAN J. LI R. Variable selection via nonconcave penalized likelihood and its oracle properties[J]. Journal of the American Statistical Association, 2001, 96(456): 1348-1360. [15] TIBSHIRANI R. Regression shrinkage and selection via the lasso[J]. Journal of the Royal Statistical Society Series B: Statistical Methodology, 1996, 58(1): 267-288. [16] DONOHO D L. JOHNSTONE I M. Ideal spatial adaptation by wavelet shrinkage[J/OL]. Biometrika, 1994, 81: 425-455. https://api.semanticscholar.org/CorpusID:239520. [17] ZOU. HUI. The Adaptive Lasso and Its Oracle Properties[J]. Publications of the American Statistical Association, 2006, 101(476): 1418-1429. "],["penalty_3.html", "16.3 群组变量选择", " 16.3 群组变量选择 本节是对《高维数据下群组变量选择的惩罚方法综述》[18]的讨论。 解释变量的分组结构可以分为两类：自然分组和人为分组。 自然分组由先验信息划分，例如描述分类变量的虚拟变量组（季节、学历等）。 人为分组则是在某种目的下将高度相关的变量归为一组，例如在公司财务评价中会把销售毛利率、净资产收益率、总资产报酬率等指标归为“盈利能力”。常见的分组依据为相关系数。 无论是自然分组还是人为分组，都是解释变量的集合，即群组变量，由此引申出群组变量选择方法。 在群组变量情形下，将自变量分为\\(J\\)个组别，即\\(X=(X_1,\\cdots,X_J)\\)依旧是\\(n \\times p\\)维设计矩阵，对应的系数向量为\\(\\beta=(\\beta^{(1)&#39;},\\dots,\\beta^{(J)&#39;})&#39;\\)，其中\\(X_j=(x_{j1},\\cdots,x_{jp_j})\\)，\\(\\beta^{(j)}=(\\beta_1^{(j)},\\cdots,\\beta_{p_j}^{(j)})\\)。目标函数形式同式(16.4)。 16.3.1 思维导图 16.3.2 处理高度相关数据的组变量选择方法 在处理高度相关数据时，惩罚函数是具有单个变量选择功能的函数和处理高度相关数据的函数的线性组合，其一般形式为 \\[ P_{\\lambda_1,\\lambda_2}(| \\beta |)=\\sum_{j=1}^pf_{\\lambda_1}(| \\beta_j |)+\\lambda_2|| \\beta ||_2^2 \\tag{16.14} \\] 不难发现，式(16.14)的惩罚函数具有共同的组成部分，即\\(L_2\\)正则项\\(|| \\beta ||_2^2\\)。文献[19]指出，如果\\(X&#39;X\\)不是近似单位阵的话，那么最小二乘估计将会变得异常敏感。而为\\(X&#39;X\\)加上\\(kI\\)则能有效控制系数估计值的膨胀及不稳定性。 为什么共线性会导致系数膨胀？ 根据文献[19]，记\\(L_1^2=(\\hat{\\beta}-\\beta)&#39;(\\hat{\\beta}-\\beta)\\)，可得\\(E(L_1^2)=\\sigma^2\\sum\\limits_{i=1}^p (1/\\lambda_i)\\)，\\(Var(L_1^2)=2\\sigma^4\\sum\\limits_{i=1}^p (1/\\lambda_i)^2\\)。当共线性严重时，会导致\\(X&#39;X\\)的最小特征值十分接近0，从而导致较大的\\(E(L_1^2)\\)和\\(Var(L_1^2)\\)，即系数估计值膨胀及不稳定。 岭回归的解为 \\[ \\hat{\\beta}_{ridge}=(X&#39;X+\\lambda I)^{-1}X&#39;Y \\tag{16.15} \\] 岭回归通过引入可控的偏差从而有效地减少了系数估计的方差。 也就是增大了\\(X&#39;X\\)特征值 优点 通过引入L2正则化项，可以解决特征数量多于样本数量时的过拟合问题。 能够处理特征间高度相关的情况。 缺点 虽然可以缩小系数，但不会将系数压缩到零，因此不会进行特征选择。 需要选择合适的正则化参数。 16.3.2.1 Enet glmnet包的glmnet()函数 \\(Enet\\)首先见刊于Hui Zou &amp; Trevor Hastie[20]的文章。 Hui Zou也是\\(Adaptive \\; Lasso\\)的创造者。 \\(Enet\\)的惩罚函数为 \\[ P_{\\lambda}(| \\beta |)=\\lambda_1\\sum_{j=1}^p| \\beta_j |+\\lambda_2\\sum_{j=1}^p \\beta_j^2 \\tag{16.16} \\] 首先观察\\(\\frac{1}{2}| \\beta |+\\frac{1}{2}\\beta^2\\)的图像，图片摘自原文。 图 16.11: Enet惩罚函数 \\(\\alpha=\\frac{\\lambda_2}{\\lambda_1+\\lambda_2}\\) 正如注解中说的，\\(Enet\\)惩罚函数在顶点处是奇异的，在边处是凸的，并且这种凸性与\\(\\alpha\\)挂钩。 原文同样也给出了\\(OLS\\)、\\(Lasso\\)、\\(Ridge\\)、\\(naive \\;elastic \\; net\\)的解，如下图所示。 图 16.12: 弹性网的解 \\(Enet\\)的解在一开始与\\(Lasso\\)的解保持一致，均为0，但在达到阈值之后就同\\(Ridge\\)一样逐渐增大惩罚力度。 the naive elastic net can be viewed as a two-stage procedure: a ridge-type direct shrinkage followed by a lasso-type thresholding. 这里提到的naive elastic net的解是对原数据进行数据变换后的解，在数据变换后可以作为\\(Lasso\\)回归处理。后文又提到了elastic net的解，该解是在前者的基础上乘上\\((1+\\lambda_2)\\)，使得该解在保留前者优良性质的基础上还能取消对系数的压缩。 \\(Enet\\)解的具体表达式如下所示 为了后面引用方便，这个解的表达式摘自\\(Mnet\\)的文献[21] \\[ \\begin{aligned} \\hat{\\theta}_{nEnet} &amp;= sgn(z)\\frac{(| z |-\\lambda_1)}{1+\\lambda_2} \\\\ \\tilde{\\theta}_{Enet} &amp;= (1+\\lambda_2)\\hat{\\theta}_{nEnet} \\end{aligned} \\tag{16.17} \\] 原文也提到了对分组效应的看法：“定性地说，如果一组高度相关的变量的回归系数趋于相等（如果是负相关的，则达到符号的变化），那么回归方法就会表现出分组效应。特别是，在某些变量完全相同的极端情况下，回归方法应该给相同的变量分配相同的系数”。 并且，原文通过\\(Lemma \\; 2\\)表明严格凸惩罚函数（例如\\(L_2\\)正则项）在极端情况（变量间高度相关）下能够识别出分组效应，并且同组变量的回归系数估计值将会趋同，反观\\(L_1\\)正则项甚至没有唯一解。\\(Theorem \\; 1\\)则给出了两个高度相关的变量的差异上界，这种差异在相关系数为1下几乎为0。由于\\(Enet\\)中包含了\\(L_2\\)正则项，因此能够处理高度相关的变量，即具有群组变量选择的功能。 原文提到\\(Lasso\\)的三个缺点，前两个详见\\(Least Angle Regression\\)原文[22]，第三个详见\\(Lasso\\)原文[15]。 优点 综合了岭回归和\\(Lasso\\)的优点，既能很好地处理高度相关的数据，又能选择变量。 当\\(p&gt;&gt;n\\)时，在不可表条件以及其他一些复杂条件下，弹性网具有\\(Oracle\\)性质。 缺点 往往选择过多的变量组。 一般情形下不具有\\(Oracle\\)性质。 16.3.2.2 Mnet ncvreg包的ncvreg()函数 Jian Huang等人[23]提出的\\(Mnet\\)法结合了\\(MCP\\)函数[21]和\\(L_2\\)正则项。 \\(Mnet\\)的惩罚函数为 \\[ P_{\\lambda , a}(| \\beta |)=\\sum_{j=1}^p f_{\\lambda_1,a}^{MCP}(| \\beta_j |)+\\frac{1}{2}\\lambda_2\\sum_{j=1}^p \\beta_j^2 \\tag{16.18} \\] 其中 \\[ f_{\\lambda,a}^{MCP}(\\theta)= \\begin{cases} \\lambda\\theta-\\frac{\\theta^2}{2a}, &amp; \\theta \\leq a \\lambda \\\\ \\frac{a\\lambda^2}{2}, &amp; \\theta \\gt a \\lambda \\end{cases} \\tag{16.19} \\] \\[ f_{\\lambda,a}^{&#39;MCP}(\\theta)= \\begin{cases} \\lambda-\\frac{\\theta}{a}, &amp; \\theta \\leq a \\lambda \\\\ 0, &amp; \\theta \\gt a \\lambda \\end{cases} \\tag{16.20} \\] 原文中提到\\(MCP\\)惩罚函数会使得结果是无偏的，稀疏的和连续的。 式(16.18)和式(16.16)相比，将\\(L_1\\)正则项换为MCP函数。 \\(f_{\\lambda_1,a}^{MCP}(| \\beta |)+\\frac{\\lambda_2}{2}\\beta^2\\)的函数图像如图16.13所示。 图 16.13: Mnet的惩罚函数 原文还给出了\\(Mnet\\)的解与\\(Enet\\)的解之间的联系，结合式(16.17)，有 \\[ \\begin{aligned} \\hat{\\theta}_{Mnet}&amp;= \\begin{cases} sgn(z)\\frac{a(| z |-\\lambda_1)_{+}}{a(1+\\lambda_2)-1}, &amp; | z | \\leq a \\lambda_1(1+\\lambda_2) \\\\ \\frac{z}{1+\\lambda_2}, &amp; | z | \\gt a \\lambda_1(1+\\lambda_2) \\end{cases} \\\\ \\tilde{\\theta}_{sMnet}&amp;= \\begin{cases} \\frac{a(1+\\lambda_2)}{a(1+\\lambda_2)-1}\\hat{\\theta}_{Enet}, &amp; | z | \\leq a \\lambda_1(1+\\lambda_2) \\\\ z, &amp; | z | \\gt a \\lambda_1(1+\\lambda_2) \\end{cases} \\end{aligned} \\tag{16.21} \\] \\(Mnet\\)的解通过尺度变化（和\\(Enet\\)一样都是乘上\\(1+\\lambda_2\\)），不仅能取消对估计值的压缩，还能得到无偏估计。 而在分组效应方面，和\\(Enet\\)一样，\\(Mnet\\)原文也给出了两个高度相关的变量之间差异的上界，这意味高度相关的自变量能够被\\(Mnet\\)识别出来。 优点 在处理高度相关问题时比弹性网更具优势。 在\\(p&gt;n\\)或\\(p&lt;n\\)时，在某些合理条件下，\\(Mnet\\)能正确选择具有非零系数的解释变量且用岭回归来估计相应的参数，因此具有\\(Oracle\\)性质。 16.3.2.3 SCAD_l2 ncvreg包的ncvreg()函数 Zeng和Xie[24]提出的\\(SCAD\\_l_2\\)法综合了\\(SCAD\\)函数和\\(L_2\\)正则项。 《高维数据下群组变量选择的惩罚方法综述》引用的Group variable selection for data with dependent structures[25]并没有\\(SCAD\\_L_2\\)，而是介绍\\(gLars\\)和\\(gRidge\\)算法。 \\(SCAD\\_l_2\\)的惩罚函数为 \\[ P_{\\lambda, a}(| \\beta |)=\\sum_{j=1}^p f_{\\lambda_1,a}^{SCAD}(\\beta_j)+\\lambda_2\\sum_{j=1}^p \\beta_j^2 \\tag{16.22} \\] 其中 \\[ f_{\\lambda,a}^{SCAD}(\\theta)= \\begin{cases} \\lambda | \\theta |, &amp; 0 \\leq | \\theta | \\lt \\lambda \\\\ -\\frac{\\theta^2-2a\\lambda| \\theta |+\\lambda^2}{2(a-1)}, &amp;\\lambda \\leq | \\theta | \\lt a\\lambda \\\\ \\frac{(a+1)\\lambda^2}{2}, &amp;a\\lambda \\leq | \\theta | \\end{cases} \\tag{16.23} \\] 原文中给出了\\(SCAD\\_L_2\\)的解和惩罚函数，分别如图16.14、图16.15所示。 图 16.14: SCAD_L2的解 图 16.15: SCAD_L2的惩罚函数 解的具体表达式如下所示 \\[ \\hat{\\theta}_{naive}= \\begin{cases} \\frac{sgn(z)(| z | -\\lambda_1)_{+}}{1+2\\lambda_2}, &amp;| z | \\leq 2\\lambda_1(1+\\lambda_2) \\\\ \\frac{(a-1)z-a\\lambda_1sgn(z)}{-1+(a-1)(1+2\\lambda_2)}, &amp;2\\lambda_1(1+\\lambda_2) \\lt | z | \\leq a\\lambda_1(1+2\\lambda_2) \\\\ \\frac{z}{1+2\\lambda_2}, &amp; a\\lambda_1(1+2\\lambda_2) \\lt | z | \\end{cases} \\tag{16.24} \\] 当\\(| z |\\)较大时，\\(\\hat{\\theta}_{SCAD\\_L_2} = (1+2\\lambda_2)\\hat{\\theta}_{naive}\\)也是无偏的，与式(16.21)不谋而合。 而在分组效应方面，相同作者的另一篇文献[25]定义了“群组变量”的概念：(1)和因变量或者残差高度相关；(2)组内变量之间高度相关。和\\(Enet\\)的文献一样，原文证明了\\(SCAD\\_L_2\\)的惩罚函数是严格凸的，并且同样也给出了差异上界，故该方法也具有识别群组变量的功能。 优点 在模拟分析中，与\\(Lasso\\)、\\(SCAD\\)和\\(Enet\\)相比，该方法不仅能降低预测误差、保留模型的稀疏性质，而且能揭露变量更多的分组信息。 估计值具有无偏性、连续性、稀疏性和群组效应。 16.3.2.4 小结 这三种方法都是依靠\\(L_2\\)正则项来实现群组变量的识别，加之另一惩罚函数来实现单个变量选择。 三者都是通过证明惩罚函数是严格凸的或者变量之间的差异上界随着相关程度增加而趋于0来表明自己的方法能够识别群组变量。 除了\\(Enet\\)，\\(Mnet\\)和\\(SCAD\\_L_2\\)在系数估计值较大时都具有无偏性。 这个性质是在设计矩阵为列正交阵的条件下证明的。正如文献[23]中提到的，对于一般情形，为了无偏性而做的尺度变换会增加估计的均方误差，又因为尺度变换不影响方法在筛选变量过程中呈现出来的性质，故一般不考虑尺度变换。 16.3.3 仅能选择组变量的方法 前一类方法将具有高度相关的数据视为同一个群组，但变量的组结构并不明确。下面介绍的方法则是在已知组结构的情形下对整租变量同时选择或者删除。这类方法的惩罚函数形如 \\[ P_{\\lambda}(| \\beta |)=\\lambda\\sum_{j=1}^JP_{outer}(\\sum_{k=1}^{p_j}P_{inner}(| \\beta_k^{(j)}|)) \\tag{16.25} \\] 这类方法的组间惩罚函数具有单个变量选择功能，通过将组内对系数的惩罚和视作一个整体从而实现对组别的选择。而组内惩罚函数不具有单个变量选择功能，故该类方法只能选择重要组而不能选择组内的重要变量。 16.3.3.1 Group Lasso gglasso包的gglasso()函数 or grpreg包的grpreg函数 Yuan和Lin[26]在其文章中除了给出了\\(Group \\; Lasso\\)，还讨论了\\(Group \\; Lar\\)、\\(Group \\; non-negative \\; garrotte\\)，这里仅讨论\\(Group \\; Lasso\\)。 \\(Group \\; Lasso\\)的惩罚函数为 \\[ P_{\\lambda,K}(| \\beta |)=\\lambda \\sum_{j=1}^J || \\beta^{(j)} ||_{K_j} \\tag{16.26} \\] 其中\\(||\\beta^{(j)}||_{K_j}=(\\beta^{(j)&#39;}K_j\\beta^{(j)})^{\\frac{1}{2}}\\)为由\\(K_j\\)决定的椭圆范数，\\(K_j\\)为\\(p_j\\)阶正定对称矩阵。椭圆范数有 \\[ \\begin{aligned} ||\\beta^{(j)}||_{K_j}&amp;=(\\beta^{(j)&#39;}K_j\\beta^{(j)})^{\\frac{1}{2}} \\\\ &amp;= (\\beta^{(j)&#39;}Q\\Lambda Q&#39;\\beta^{(j)})^{\\frac{1}{2}} \\\\ &amp;= [(\\Lambda^{\\frac{1}{2}}Q&#39;\\beta^{(j)})&#39;(\\Lambda^{\\frac{1}{2}}Q&#39;\\beta^{(j)})]^{\\frac{1}{2}} \\end{aligned} \\tag{16.27} \\] 可见椭圆范数就是线性变换后的\\(L_2\\)范数。 原文给出了\\(K_j\\)的建议值，即\\(K_j=p_jI_j\\)，故式(16.26)转化为 \\[ P_{\\lambda,K}(| \\beta |)=\\lambda \\sum_{j=1}^J \\sqrt{p_j}|| \\beta^{(j)} ||_2 \\tag{16.28} \\] 式(16.28)的组内惩罚即为带权重的组内系数的\\(L_2\\)范数。不妨记组内惩罚为\\(a_j=\\sqrt{p_j}|| \\beta^{(j)} ||_2\\)，当然有\\(a_j \\geq 0\\)。回忆\\(L_1\\)范数的定义，\\(||x||_1=\\sum\\limits_{i=1}^p |x_i|\\)，式(16.28)中的对应部分为\\(\\sum\\limits_{j=1}^J a_j\\)，也就是说，若将组内惩罚值视作新的分量，那么组间惩罚的形式就是\\(L_1\\)范数。 原文对\\(L_1\\)、\\(Group \\; Lasso\\)、\\(L_2\\)的惩罚函数进行了可视化，如图16.16所示。 图 16.16: Group Lasso的惩罚函数 可以发现式(16.28)介于\\(L_1\\)正则项与\\(L_2\\)正则项之间。其中子图\\((e)\\)反映了\\(||\\beta_1||+|\\beta2|=1\\)。 优点 对于确定的\\(p\\)，在不可表条件的变形条件下，\\(Group \\; Lasso\\)在随机设计模型中具有组选择一致性。 缺点 \\(Group \\; Lasso\\)不具备\\(Oracle\\)性质。 参数估计值偏差过大，也往往会选择过多的组。 16.3.3.2 CAP \\(CAP\\)也是在已知群组结构的情形下进行群组变量选择的方法。Zhao等人[27]提到了可以利用聚类技术先去进行分组，然后再进行群组变量选择。此外，原文还提到了重叠与非重叠群组的概念，而\\(CAP\\)对这两种情形均适用。 Specifically, we introduce the Composite Absolute Penalties (CAP) family, which allows given grouping and hierarchical relationships between the predictors to be expressed. Grouped selection occurs for nonoverlapping groups. Hierarchical variable selection is reached by defining groups with particular overlapping patterns. \\(CAP\\)惩罚函数的一般形式为 \\[ P_{\\lambda, \\gamma}(|\\beta|)=\\lambda||(||\\beta^{(1)}||_{\\gamma_1},\\cdots,||\\beta^{(J)}||_{\\gamma_J})&#39;||_{\\gamma_0}^{\\gamma_0} \\tag{16.29} \\] 不难发现，组间惩罚函数为\\(||\\cdot||_{\\gamma_0}^{\\gamma_0}\\)，组内惩罚函数为\\(||\\cdot||_{\\gamma_j}\\)。相较于式(16.28)，式(16.29)在组内惩罚函数的形式方面给予了更多的灵活度。这使得\\(CAP\\)不仅能处理非重叠群组，还能处理重叠群组。 非重叠群组就是指各个组分配到的变量没有重复，该情形较为容易理解，这里就不再赘述。 对于重叠群组，这里仅考虑有\\(X_1\\)、\\(X_2\\)的简单情形。若我们想让\\(X_1\\)在\\(X_2\\)之前选入模型，我们可以构造这样的群组关系，令\\(G_1=\\{X_1, X_2\\}\\)、\\(G_2=\\{X_2\\}\\)、\\(\\gamma_0=1\\)、\\(\\gamma_1,\\gamma_2&gt;1\\)，对此有 \\[ P_{\\lambda,\\gamma}(|\\beta|)=\\lambda(||(\\beta_1,\\beta_2)||_{\\gamma_1}+||\\beta_2||_{\\gamma_2}) \\tag{16.30} \\] 其中\\(||\\beta_2||_{\\gamma_2}=|\\beta_2|\\)。绘制\\(||(\\beta_1,\\beta_2)||_{\\gamma_1}+|\\beta_2|\\)的等值线图，如下所示： 图 16.17: 重叠群组的惩罚函数 当\\(\\gamma_1 &gt; 1\\)时，\\(\\beta_1\\)轴上尖尖的特征愈发明显，而\\(\\beta_2\\)轴上愈发平坦。因此，\\(\\beta_1\\)轴更容易与损失函数相交，此时\\(\\beta_2=0\\)。可见，若想让一个变量先于另一个变量被选入到模型中，应当将这两个变量放在同一个组别中，并且另一个变量单独处于一个组。 优点 允许组组之间存在重叠变量。 16.3.4 双层变量选择方法 双层变量选择方法既能选择重要组别，又能在组内选择重要变量。 在这一节中，双层变量选择法又分为两类方法，第一类称为“复合函数惩罚法”，该类方法通过复合函数的形式来选择重要对象。因此组内惩罚函数与组间惩罚函数都应该是具有单个变量选择功能的惩罚函数。惩罚函数的形式同式(16.25)。第二类称为“稀疏组惩罚法”，该类方法凭借具有单个变量选择功能的惩罚函数和具有仅能选择组变量功能的惩罚函数的线性组合来选择对象，又称为“可加惩罚”，其一般形式为 \\[ P_{\\lambda_1,\\lambda_2}(|\\beta|)=\\lambda_1P_{indiv}(|\\beta|)+\\lambda_2P_{grp}(|\\beta|) \\tag{16.31} \\] 16.3.4.1 Group Bridge grpreg包的grpreg函数 Huang等人[28]提出的\\(Group \\; Bridge\\)属于“复合函数惩罚法”，其惩罚函数形式为 \\[ P_\\lambda(|\\beta|)=\\sum\\limits_{j=1}^J\\lambda c_j||\\beta^{(j)}||_1^\\gamma \\tag{16.32} \\] 其中\\(0&lt;\\gamma&lt;1\\)，\\(c_j\\)是一个调整参数，可取\\(c_j \\propto |A_j|^{1-\\gamma}\\)，\\(|A_j|\\)表示第\\(j\\)组的变量数。注意组内系数的正则项为\\(||\\beta^{(j)}||_1^\\gamma\\)，即先求得\\(\\beta^{(j)}\\)的\\(L_1\\)范数，再求其\\(\\gamma\\)次幂，组间求和的形式就是\\(L_\\gamma\\)范数。结合图16.1可知，当\\(0&lt;\\gamma&lt;1\\)时，\\(L_\\gamma\\)范数在结点处是奇异的，故也能产生稀疏解。因此\\(Group \\; Bridge\\)在组内和组间都能进行变量选择。 该方法的惩罚函数图像如图16.18所示。 文献[28]中提到\\(Group \\; Bridge\\)也允许重叠群组的存在。 优点 当\\(p \\rightarrow \\infty, \\, n \\rightarrow \\infty\\)但\\(p&lt;n\\)时，在某些正则条件下，\\(Group \\; Bridge(0&lt;\\gamma&lt;1)\\)具有群组\\(Oracle\\)性质。 缺点 不具备组内相合性。 16.3.4.2 Group MCP grpreg包的grpreg函数 \\(Group \\; Bridge\\)的惩罚函数结合了\\(L_1\\)正则项与\\(L_\\gamma\\)正则项(\\(0&lt;\\gamma&lt;1\\))，因此在某些点是奇异的，即不可微。故Breheny和Huang[29]提出的\\(Group \\; MCP\\)采用\\(MCP\\)函数来作为组间及组内惩罚函数。在\\(MCP\\)的原始文献[21]中已经证明了该惩罚函数具有无偏性、连续性和稀疏性。 \\(Group \\; MCP\\)的惩罚函数为 \\[ P_{\\lambda, a, b}(|\\beta|)=\\sum\\limits_{j=1}^Jf_{\\lambda,b}^{MCP}(\\sum\\limits_{k=1}^{p_j}f_{\\lambda, a}^{MCP}(|\\beta_k^{(j)}|)) \\tag{16.33} \\] 由式(16.19)可知，当\\(\\beta&gt;a\\lambda\\)时，\\(f_{\\lambda,a}^{MCP}(\\beta)\\)取得最大值\\(\\frac{a\\lambda^2}{2}\\)。当组内的各个系数均大于\\(a\\lambda\\)时，组内惩罚达到最大值，即\\(p_j\\frac{a\\lambda^2}{2}\\)。之后将组内惩罚值传入到组间惩罚函数，此时需比较\\(b\\lambda\\)与\\(p_j\\frac{a\\lambda^2}{2}\\)的大小，显然当\\(p_j\\frac{a\\lambda}{2}&gt;b\\)时，组间惩罚也取到了最大值。 此外，与\\(Group \\; Lasso\\)和\\(Group \\; Bridge\\)相比，原文指出了\\(Group MCP\\)无论是在个体变量选择上还是组别选择上都是capped，如图16.18所示。 不知道怎么翻译 the group MCP penalty is capped at both the individual covariate and group levels, while the group lasso and group bridge penalties are not. 图 16.18: Group MCP的惩罚函数 该图表明\\(Group \\; MCP\\)方法能够有效避免过度惩罚较大的系数，同时保持解的稀疏性。 之后，原文写了一段耐人寻味的文字。 Group bridge allows the presence of a single large predictor to continually lower the entry threshold of the other variables in its group. This property, whereby a single strong predictor drags others into the model, prevents group bridge from achieving consistency for the selection of individual variables. Group MCP, on the other hand, limits the amount of signal that a single predictor can contribute towards the reduction of the penalty applied to the other members of the group. \\(Group \\; Bridge\\)的惩罚函数对系数的惩罚是一直存在的，并且系数越大，惩罚越大。在群组变量的背景下，当一个系数真的很大时，该系数就会极大地增加该组的组内惩罚函数值，尽管其他系数对组内惩罚函数值的贡献十分有限。这样从外人（组间惩罚函数）的视角来看就会觉得这一组变量挺重要的，实则滥竽充数，颇有“一人得道，鸡犬升天”的意味。 而\\(Group \\; MCP\\)的惩罚函数对系数的惩罚是有限的，这就确保了当组内较大的系数摆脱了非零的嫌疑后，他就可以撒手不管，组内惩罚函数值的大小还得靠其他系数来一起做贡献，“大家好才是真的好”。 与文献[18]的论断相反，有待商榷。 优点 具有组内和组间的相合性。 16.3.4.3 Sparse Group Lasso sparsegl包的sparsegl()函数 Simon等人[30]提出的\\(Sparse Group Lasso\\)的惩罚函数如下所示 \\[ P_\\lambda(|\\beta|)=\\lambda_1||\\beta||_1+\\lambda_2\\sum\\limits_{j=1}^J ||\\beta^{(j)}||_2 \\tag{16.34} \\] 不难发现，式(16.34)就是\\(L_1\\)正则项加上\\(Group \\; Lasso\\)中除去\\(\\sqrt{p_j}\\)的惩罚项，式(16.28)。该方法依靠\\(L_1\\)正则项进行单个变量选择，依靠\\(Group \\; Lasso\\)的部分进行群组变量选择，从而实现既能选择单个重要变量，又能选择重要组别。当然，也正如文章中说的，第一项和第二项被具有相应功能的惩罚项替代也是可以的。 References [15] TIBSHIRANI R. Regression shrinkage and selection via the lasso[J]. Journal of the Royal Statistical Society Series B: Statistical Methodology, 1996, 58(1): 267-288. [18] 王小燕. 谢邦昌. 马双鸽. 等. 高维数据下群组变量选择的惩罚方法综述[J]. 数理统计与管理, 2015, 34(6): 978-988. [19] HOERL A E. KENNARD R W. Ridge regression: Biased estimation for nonorthogonal problems[J]. Technometrics, 1970, 12(1): 55-67. [20] ZOU H. HASTIE T. Regularization and variable selection via the elastic net[J]. Journal of the Royal Statistical Society Series B: Statistical Methodology, 2005, 67(2): 301-320. [21] ZHANG C H. Penalized linear unbiased selection[J]. Department of Statistics and Bioinformatics, Rutgers University, 2007, 3(2010): 894-942. [22] EFRON B. HASTIE T. TIBSHIRANI J R. Least Angle Regression[J]. Annals of Statistics, 2004, 32(2): 407-451. [23] HUANG J. BREHENY P. LEE S. 等. The Mnet method for variable selection[J]. Statistica Sinica, 2016: 903-923. [24] ZENG L. XIE J. Group variable selection via SCAD-L2[J/OL]. Statistics, 2014, 48: 49-66. https://api.semanticscholar.org/CorpusID:14065404. [25] ZENG L. XIE J. Group variable selection for data with dependent structures[J]. Journal of Statistical Computation and Simulation, 2012, 82(1): 95-106. [26] YUAN M. LIN Y. Model selection and estimation in regression with grouped variables[J]. Journal of the Royal Statistical Society Series B: Statistical Methodology, 2006, 68(1): 49-67. [27] ZHAO P. ROCHA G. YU B. The composite absolute penalties family for grouped and hierarchical variable selection[J]. 2009. [28] HUANG J. MA S. XIE H. 等. A group bridge approach for variable selection[J]. Biometrika, 2009, 96(2): 339-355. [29] BREHENY P. HUANG J. Penalized methods for bi-level variable selection[J]. Statistics and Its Interface, 2009, 2(3): 369. [30] SIMON N. FRIEDMAN J. HASTIE T. 等. A Sparse-Group Lasso[J]. Journal of Computational and Graphical Statistics: A Joint Publication of American Statistical Association, Institute of Mathematical Statistics, Interface Foundation of North America, 2013, 22(2): 231-245. "],["penalty_4.html", "16.4 算法", " 16.4 算法 （群组）坐标下降法： https://www.bilibili.com/video/BV1Vg411k7oU/?spm_id_from=333.337.search-card.all.click https://zhuanlan.zhihu.com/p/52300427 梯度下降法： https://www.bilibili.com/video/BV1a94y1S7PP/?spm_id_from=333.337.search-card.all.click&amp;vd_source=1ff1a8ac5564814fec4d27cae552f90e "],["penalty_5.html", "16.5 总结", " 16.5 总结 回顾目标函数这一节，惩罚函数的加入使得目标函数的最小值解有所变动，因此要重新找到这个最小值解。故系数向量\\(\\beta\\)不得不进行一定程度的收缩，从而在损失函数和惩罚函数的动态平衡中找到最小值解。 那么我们为什么要进行惩罚？ 对于\\(L_2\\)正则项而言，惩罚是为了牺牲无偏性而换取更小的方差，使得模型更加稳定。对于\\(L_1\\)正则项而言，惩罚在一定程度上压缩了各个估计值，但更重要的是为了得到稀疏解。稀疏性是我们在高维场合中优先追求的目标，而惩罚函数是我们获取稀疏解的不二之选。 对于一系列变量，稀疏解的意义就在于帮助我们识别出那些非零系数的变量，也就是重要变量。如果一个自变量对因变量具有不可忽视的作用，那么即便在受到惩罚后也会保持相对较大的数值。相反，如果一个自变量对因变量的影响是微不足道的，那么在惩罚函数的作用下他就应该为零。可以说，惩罚函数的值反映了自变量在模型中的话语权。 从几何视角来看，惩罚函数在某种标准下度量了系数向量离原点的距离，若某一分量在这个寻求动态平衡的过程中变为了零，那么就说明该分量对应的自变量就是不重要的。 不同的惩罚函数则是决定了我们是以何种路径、方式去找到稀疏解。 惩罚函数的存在使得系数向原点方向收缩，并改变了目标函数的形态，形态决定功能，由此产生目标解。 "],["integrative.html", "17 整合分析", " 17 整合分析 本章是对《大数据的整合分析方法》[31]的讨论。 References [31] 马双鸽. 王小燕. 方匡南. 大数据的整合分析方法[J]. 统计研究, 2015, 32(11): 3-11. "],["integrative_1.html", "17.1 引言", " 17.1 引言 大数据背景下，有关同一问题的数据来源多种多样，这就决定了子样本之间存在异质性和同质性（矛盾的“对立统一”）。由于都是对同一问题的描述，因此子样本之间必然存在同质性（矛盾的“普遍性”）。又由于数据来源各不相同，在数据采集的过程中必然受到时间、地点、调查方式等诸多因素的影响，从而使得子样本具有独特的数据特征，即异质性（矛盾的“特殊性”）。对此，既不能将所有子样本合并为一个数据集来统一建模而无视差异，也不能对各个子样本各自建模而忽略相关性，整合分析应运而生。 同样，整合分析也是解决“大p小n”问题的有效方式。“大p小n”问题表现为大数据的来源差异性、高维性和稀疏性等特点。数据采集能力的提升拓宽了数据的来源渠道，即来源差异性；在这个过程中，数据维度也在不断增加，不可避免地会纳入与研究目的无关的变量，即高维性特征；信息的边际价值并未随数据量的增加而增加，即稀疏性。 鉴于此，本篇文章采用基于惩罚方法的整合分析对多个数据集进行分析。根据前一章的内容，我们已经了解到惩罚函数在单个数据集上具有单个变量或群组变量筛选的功能，将其思想推广至整合分析领域，便是本篇文章的主要内容。 "],["integrative_2.html", "17.2 模型基本形式", " 17.2 模型基本形式 假设有\\(M\\)个数据集，\\(p\\)个解释变量。第\\(m\\)个数据集的样本量为\\(n^{(m)}\\)，因变量\\(y^{(m)}\\)为\\(n^{(m)} \\times 1\\)向量，解释变量\\(X^{(m)}\\)为\\(n^{m} \\times p\\)矩阵，假设数据已经被标准化。 对第\\(m\\)个数据集建立如下模型： \\[ y^{(m)}=X^{(m)}\\beta^{(m)}+\\epsilon^{(m)} \\tag{17.1} \\] 其中，\\(\\beta^{(m)}=(\\beta_1^{(m)}, \\cdots,\\beta_p^{(m)})&#39;\\)为回归系数，\\(\\epsilon^{(m)}\\)满足\\(E(\\epsilon^{(m)})=0\\)、\\(Var(\\epsilon^{(m)})=\\sigma^2_{(m)}\\)。 由此拓展到所有数据集中，则 \\[ \\beta=\\textrm{argmin}\\{L(X,y;\\beta)+P(\\beta;\\lambda)\\} \\tag{17.2} \\] 其中\\(y=(y^{(1)&#39;},\\cdots,y^{(M)&#39;})&#39;\\)为\\(\\sum\\limits_{m=1}^M n^{(m)} \\times 1\\)的因变量，\\(X=diag(X^{(1)},\\cdots,X^{(m)})\\)为\\(\\sum\\limits_{m=1}^M n^{(m)} \\times Mp\\)的设计矩阵，\\(\\beta=(\\beta^{(1)&#39;},\\cdots,\\beta^{(M)&#39;})&#39;\\)为\\(Mp \\times 1\\)的未知参数向量。\\(L(X,y;\\beta)=\\sum\\limits_{i=1}^M L(X^{(m)},y^{(m)};\\beta^{(m)})\\)是所有数据集上的损失之和，\\(L(\\cdot)\\)可取平方损失或负向对数似然损失。\\(P(\\lambda;\\beta)\\)是惩罚函数，\\(\\lambda\\)用于调控惩罚力度，\\(\\lambda\\)越大，惩罚力度越大，\\(\\beta\\)被压缩得越严重，估计为零的回归系数也就越多。 \\[ \\begin{pmatrix} y^{(1)}\\\\ \\vdots \\\\ y^{(m)} \\end{pmatrix}= \\begin{pmatrix} X^{(1)}\\beta^{(1)}+\\epsilon^{(1)}\\\\ \\vdots \\\\ X^{(m)}\\beta^{(m)}+\\epsilon^{(m)} \\end{pmatrix}= \\begin{pmatrix} X^{(1)} &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; X^{(m)} \\end{pmatrix} \\begin{pmatrix} \\beta^{(1)} \\\\ \\vdots \\\\ \\beta^{(m)} \\end{pmatrix}+ \\begin{pmatrix} \\epsilon^{(1)} \\\\ \\vdots \\\\ \\epsilon^{(m)} \\end{pmatrix} \\] 记\\(X_j\\)在所有数据集中的回归系数为\\(\\beta_j=(\\beta_j^{(1)},\\cdots,\\beta_j^{(m)})&#39;\\)。在整合分析中，由于\\(\\beta_j\\)归属于同一个自变量\\(X_j\\)，故\\(\\beta_j\\)内部之间具有一定程度的关联性或相似性，即可将其视为群组。 图 17.1: 数据结构 "],["integrative_3.html", "17.3 同构数据的整合分析", " 17.3 同构数据的整合分析 在同构数据中，常见于调查问卷相同、实验设计相同等数据收集方式一致的情形中，自变量在\\(M\\)个数据集中的显著性是一致的，即若\\(X_j\\)在数据集\\(m\\)中显著，则它在所有数据集中都显著。鉴于此，可以将\\(\\beta_j\\)视作一个群组并采取组变量的选择方法进行筛选。 L2 Group Bridge \\(L_2 \\; Group \\; Bridge\\)为组内\\(L_2\\)惩罚，组间\\(Bridge\\)惩罚，其惩罚函数为 \\[ P(\\beta;\\lambda,\\gamma)=\\lambda\\sum_{j=1}^p ||\\beta_j||^\\gamma=\\lambda\\sum_{j=1}^p((\\sum_{i=1}^M (\\beta_j^{(i)})^2)^{1/2})^\\gamma \\tag{17.3} \\] 若以\\(Group \\; Lasso\\)估计作为初始值进行迭代估计，可以证明\\(L_2 \\; Group \\; Bridge\\)满足选择一致性。 L2 Group MCP \\(L_2 \\; Group \\; MCP\\)组内为\\(L_2\\)惩罚，组间为\\(MCP\\)惩罚，其惩罚函数为 \\[ P(\\beta;\\lambda,a)=\\sum_{j=1}^p P_{MCP}(||\\beta_j||;\\lambda,a) \\tag{17.4} \\] 关于\\(MCP\\)函数的介绍可以参见Mnet。 Group Lasso \\(Group \\; Lasso\\)组内和组间都是\\(L_1\\)惩罚，其惩罚函数形式为 \\[ P(\\beta;\\lambda)=\\lambda\\sum_{j=1}^p ||\\beta_j|| \\tag{17.5} \\] "],["integrative_4.html", "17.4 异构数据的整合分析", " 17.4 异构数据的整合分析 在异构数据中，不同数据集中同一个自变量的显著性不一定相同，因此涉及到双层变量选择。 L1 Group MCP \\(L_1 \\; Group \\; MCP\\)组内\\(L_1\\)惩罚，组间为\\(MCP\\)惩罚，其惩罚函数为 \\[ P(\\beta;\\lambda,a)=\\sum_{j=1}^p P_{MCP}(||\\beta_j||_1;\\lambda,a) \\tag{17.6} \\] 经证明，\\(L_1 \\; Group \\; MCP\\)仅满足组间选择一致性。 Group MCP \\(Group \\; MCP\\)又称\\(Composite \\; MCP\\)，组内、组间都是\\(MCP\\)惩罚，其惩罚函数为 \\[ P(\\beta;\\lambda,a,b)=\\sum_{j=1}^p P_{MCP}(\\sum_{m=1}^M P_{MCP}(|\\beta_j^{(m)}|;\\lambda,a);\\lambda,b) \\tag{17.7} \\] 经证明，\\(Group \\; MCP\\)在组内和组间均满足选择一致性。 L1 Group Bridge \\(L_1 \\; Group \\; Bridge\\)组内为\\(L_1\\)惩罚，组间为\\(Bridge\\)惩罚，其惩罚函数为 \\[ P(\\beta;\\lambda)=\\lambda\\sum_{j=1}^p p_j||\\beta_j||_i^\\gamma \\tag{17.8} \\] Sparse Group Lasso &amp; Adaptive Sparse Group Lasso \\(Sparse \\; Group \\; Lasso\\)与\\(Adaptive \\; Sparse \\; Group \\; Lasso\\)都是基于\\(Lasso\\)的惩罚方法，两者的惩罚函数分别为 \\[ P_{SGL}(\\beta;\\lambda_1,\\lambda_2)=\\lambda_1\\sum_{j=1}^p ||\\beta_j||+\\lambda_2||\\beta||_1 \\\\ P_{adSGL}(\\beta;\\lambda_1,\\lambda_2)=\\lambda_1\\sum_{j=1}^p w_j||\\beta_j||_2+\\lambda_2\\xi&#39;|\\beta| \\tag{17.9} \\] 由于#SGL#是\\(Lasso\\)和\\(Group \\; Lasso\\)的线性组合，两者在理论上都不满足\\(Oracle\\)性质，预期\\(SGL\\)也不满足\\(Oracle\\)性质。而\\(adSGL\\)通过引入组权重\\(w\\)和单个系数权重\\(\\xi\\)，改进选择一致性和估计一致性。 "],["integrative_5.html", "17.5 具有网络结构关系的整合分析", " 17.5 具有网络结构关系的整合分析 在同一个数据集中，不同解释变量之间可能存在相互作用关系（也意味着是系数间的相互作用关系），从而形成了一张网络结构图，此种关系称为“数据集内部结构”。针对网络结构的惩罚函数如下所示 \\[ P(\\beta;\\lambda)=\\lambda\\sum_{1 \\leq j,k \\leq p}a_{jk}(\\frac{||\\beta_j||_2}{\\sqrt{M_j}}-\\frac{||\\beta_k||_2}{\\sqrt{M_k}})^2 \\tag{17.10} \\] 其中\\(a_{jk}\\)由网络图得到，描述了两个节点之间是否相连以及链接的强度。若\\(X_j\\)和\\(X_k\\)越相似，则其惩罚越重(\\(a_{jk}\\)越大)，从而拉近\\(\\beta_j\\)和\\(\\beta_k\\)之间的距离，它们的估计值越相近。 同一解释变量在不同数据集中的系数存在某种相似性，称之为“跨数据集结构”。对此有\\(Contrast\\)惩罚，惩罚函数如下所示 \\[ P_C(\\beta)=\\lambda\\sum_{j=1}^p \\sum_{k \\neq l} a_j^{(kl)}(\\beta_j^{(k)}-\\beta_j^{(l)})^2 \\tag{17.11} \\] 其中\\(a_j^{(kl)}=I(sgn(\\beta_j^{(k)})=sgn(\\beta_j^{(l)}))\\)。简而言之，就是若\\(\\beta_j^{(k)}\\)和\\(\\beta_j^{(l)}\\)的符号相同，则\\(a_j^{(kl)}\\)取1，那么在最小化的目标下会拉近\\(\\beta_j^{(k)}\\)和\\(\\beta_j^{(l)}\\)之间的距离，反之为0。 "],["code.html", "18 算法复现", " 18 算法复现 该部分内容记录本人对一些论文算法的复现结果。 "],["code_1.html", "18.1 惩罚Cox比例风险模型", " 18.1 惩罚Cox比例风险模型 该节是对Regularization paths for Cox’s proportional hazards model via coordinate descent[1]论文的复现。 该论文在Cox比例风险模型的基础上添加了弹性网惩罚函数，并利用坐标下降法进行求解，可通过glmnet包实现。 算法复现详见第此处，注意复现结果与官方函数略有出入。 References [1] SIMON N. FRIEDMAN J H. HASTIE T. 等. Regularization paths for Cox’s proportional hazards model via coordinate descent[J]. Journal of Statistical Software, 2011, 39: 1-13. "],["code_2.html", "18.2 异质截距项的线性模型", " 18.2 异质截距项的线性模型 该节是对A Concave Pairwise Fusion Approach to Subgroup Analysis[32]论文的复现。 \\[ y_i = \\mu_i+\\mathbf{x}_i^T\\beta+\\varepsilon_i \\tag{18.1} \\] 该论文在普通线性模型的基础上，考虑了截距项的异质性，即样本内部可能存在亚组，同一亚组共享相同的截距项，不同的亚组之间的截距项不同。该论文利用增广拉格朗日法构造目标函数（引入融合惩罚项\\(\\mu_i-\\mu_j=\\eta_{ij}\\)），并通过ADMM算法进行求解。 18.2.1 自定义算法 算法逻辑： 传入参数 y：向量，响应变量 x：矩阵，预测变量 scale：是否对x进行标准化，默认为TRUE penalty：设置惩罚函数类型(L1、MCP、SCAD) \\(\\theta\\)：ADMM算法的惩罚系数 \\(\\lambda\\)：惩罚函数中的惩罚系数 \\(\\gamma\\)：惩罚函数中的正则化因子 tol：收敛精度，默认为0.00001 max_iter：最大迭代次数，默认为1000 其余符号说明 除了传入参数外，在运算过程中还有其它符号，其含义如下所示。 \\(\\upsilon\\)：拉格朗日乘子 \\(Q\\)：\\(X(X^TX)^{-1}X^T\\) \\(Delta\\)：\\(\\{(e_i-e_j), i&lt;j\\}^T\\)，其中\\(e_i\\)表示第\\(i\\)个分量为1，其余分量为0的n维向量 \\(\\delta\\_\\mu\\)：\\(\\mu_i-\\mu_j\\) \\(\\delta\\)：\\(\\mu_i-\\mu_j+\\theta^{-1}\\upsilon_{ij}\\) \\(\\alpha\\)：亚组的截距项 初始值 根据ols估计获取\\(\\beta\\)、\\(\\mu\\)、\\(\\eta\\)、\\(\\upsilon\\)的初始值。 迭代 每次循环按照\\(\\mu, \\beta, \\delta, \\eta, \\upsilon\\)的顺序进行迭代。 \\[ \\begin{aligned} \\mu^{(m+1)}&amp;=(\\theta \\Delta^T\\Delta+I_n-Q)^{-1}((I_n-Q)y+\\theta \\Delta^T(\\eta^{(m)}-\\theta^{-1}\\upsilon^{(m)})) \\\\ \\beta^{(m+1)}&amp;=(X^TX)^{-1}X^T(y-\\mu^{(m+1)}) \\\\ \\delta_{ij}^{(m+1)}&amp;=\\mu_i^{(m+1)}-\\mu_j^{(m+1)}+\\theta^{-1}\\upsilon_{ij}^{(m)} \\\\ \\eta_{ij}^{(m+1)}&amp;=\\textrm{Penalty} \\\\ \\upsilon_{ij}^{(m+1)} &amp;= \\upsilon_{ij}^{(m)}+\\theta (\\mu_i^{(m+1)}-\\mu_j^{(m+1)}-\\eta_{ij}^{(m+1)}) \\end{aligned} \\] 注意\\(\\delta_{ij}\\)与\\(\\eta_{ij}\\)是针对每一个分量而言的，得将每一个分量代入惩罚函数才能得到对应的结果。 停止 当迭代次数达到最大迭代次数或残差\\(r^{(m+1)}=\\Delta\\mu^{(m+1)}-\\eta^{(m+1)}\\)的模长足够小时，则停止迭代。 输出 输出每个观测对象的截距项\\(\\hat \\mu_i\\)，回归系数\\(\\hat \\beta\\)，截距项对应的亚组标签，每个亚组的截距项均值\\(\\hat \\alpha_k\\)。 注意，论文涉及到\\(i&lt;j\\)的排序，自定义算法采取先控制i再迭代j的步骤进行排序，如(1,2)、(1,3)、(1,4)…该顺序影响了后面矩阵的排列方式与lower.tri 由于设备限制，为节省运行时间，并未实现原文中的“\\(\\lambda\\)路径图”及“对\\(\\lambda\\)调参”的功能 自定义算法如下所示 library(tidyverse) library(igraph) library(R6) SubgroupIntercept &lt;- R6Class( classname &lt;- &#39;SubgroupIntercept&#39;, public &lt;- list( # 传入参数 y = NULL, # 响应变量 x = NULL, # 预测变量 n = NULL, # 样本容量 scale = NULL, # 是否标准化 penalty = NULL, # 惩罚函数类型：L1、MCP、SCAD theta = NULL, # 式(4)二次项的惩罚系数 lambda = NULL, # 惩罚函数中的惩罚系数 gamma = NULL, # 惩罚函数中的正则化因子 tol = NULL, # 收敛精度 max_iter = NULL, # 最大迭代次数 # 初始化 initialize = function(y, x, scale = T, penalty, theta, lambda, gamma, tol = 0.00001, max_iter = 1000){ self$y &lt;- y self$x &lt;- x self$n &lt;- length(y) self$scale &lt;- scale self$penalty &lt;- penalty self$theta &lt;- theta self$lambda &lt;- lambda self$gamma &lt;- gamma self$tol &lt;- tol self$max_iter &lt;- max_iter }, # 主函数——运行 run = function(){ start_time &lt;- Sys.time() # 检验输入是否合理 private$validate() # 标准化协变量 if(self$scale == T) self$x &lt;- apply(self$x, 2, scale) # 获取初始值 initial_value &lt;- private$initial_value() private$beta &lt;- initial_value[[1]] private$mu &lt;- initial_value[[2]] private$eta &lt;- initial_value[[3]] private$upsilon &lt;- initial_value[[4]] # 计算Q矩阵 Q &lt;- private$gen_Q() # 计算Delta Delta &lt;- private$gen_Delta() # 开始迭代 for (i in 1:self$max_iter) { print(paste0(&#39;正在进行第[&#39;, i, &#39;]次迭代&#39;)) private$mu &lt;- private$iter_mu(Q, Delta, private$eta, private$upsilon) private$beta &lt;- private$iter_beta(private$mu) private$delta_mu &lt;- private$iter_delta_mu(private$mu) private$delta &lt;- private$iter_delta(private$delta_mu, private$upsilon) private$eta &lt;- private$iter_eta(private$delta, self$lambda) private$upsilon &lt;- private$iter_upsilon(private$upsilon, private$delta_mu, private$eta) # 终止条件 r &lt;- Delta[[1]] %*% private$mu - private$eta if(sqrt(sum(r^2)) &lt;= self$tol){ print(&#39;达到精度要求&#39;) break } } # 获取亚组分类结果 label &lt;- private$cluster_eta(private$eta) # 根据亚组计算对应的alpha subgroup_alpha &lt;- tibble(mu = private$mu, label = label) subgroup_alpha &lt;- subgroup_alpha %&gt;% group_by(label) %&gt;% summarise(alpha = mean(mu), size = n()) result &lt;- list(mu = private$mu, beta = private$beta, label = label, alpha = subgroup_alpha) end_time &lt;- Sys.time() cost_time &lt;- end_time - start_time print(paste(&#39;花费时间：&#39;, round(cost_time, 3), &quot;秒&quot;)) return(result) } ), private &lt;- list( # 迭代参数 mu = NULL, # 截距项 beta = NULL, # 回归系数 delta_mu = NULL, # mu_i-mu_j delta = NULL, # delta_ij=mu_i-mu_j+1/theta*upsilon eta = NULL, # 由惩罚函数计算 upsilon =NULL, # 拉格朗日乘子 # 验证输入是否正确 validate = function(){ if(!is.vector(self$y)) stop(&#39;Y要求为向量&#39;) if(!is.matrix(self$x)) stop(&#39;X要求为矩阵&#39;) if(!is.logical(self$scale)) stop(&#39;scale要求为布尔值&#39;) if(!self$penalty %in% c(&#39;L1&#39;, &#39;MCP&#39;,&#39;SCAD&#39;)) stop(&#39;请选择合适的惩罚函数&#39;) if(self$theta &lt;= 0) stop(&#39;请选择合适的theta值&#39;) if(self$lambda &lt;= 0) stop(&#39;请选择合适的lambda值&#39;) if(self$gamma &lt;= 0) stop(&#39;请选择合适的gamma值&#39;) if(self$tol &lt;= 0) stop(&#39;请选择合适的精度要求&#39;) if(self$max_iter &lt;= 0) stop(&#39;请选择合适的最大迭代次数&#39;) }, # 获取初始值 initial_value = function(){ df &lt;- cbind(self$y, self$x) %&gt;% as.data.frame() colnames(df) &lt;- c(&#39;y&#39;, paste0(&#39;x_&#39;, 1:dim(x)[2])) ols &lt;- lm(y~., data = df) beta_0 &lt;- coef(ols)[-1] mu_0 &lt;- (self$y - self$x %*% beta_0) %&gt;% as.vector() eta_0 &lt;- -outer(mu_0, mu_0, &#39;-&#39;) # lower.tri按列提取下三角，故添加负号 eta_0 &lt;- eta_0[lower.tri(eta_0)] upsilon_0 &lt;- rep(0, times = self$n * (self$n - 1)/2) result &lt;- list(beta_0 = beta_0, mu_0 = mu_0, eta_0 = eta_0, upsilon_0 = upsilon_0) return(result) }, # 计算Q矩阵 gen_Q = function(){ Q &lt;- self$x %*% solve(t(self$x) %*% self$x) %*% t(self$x) return(Q) }, # 计算Delta矩阵 gen_Delta = function(){ gen_mat &lt;- function(n){ mat_1 &lt;- if(n == self$n-1){ NULL }else{ matrix(0, nrow = self$n-1-n, ncol = n) } mat_2 &lt;- matrix(1, nrow = 1, ncol = n, byrow = T) mat_3 &lt;- diag(-1, nrow=n) mat &lt;- rbind(mat_1, mat_2, mat_3) return(mat) } Delta &lt;- as.list(c((self$n-1):1)) %&gt;% map(~gen_mat(.)) Delta &lt;- do.call(cbind, Delta) %&gt;% t() Delta_Delta &lt;- diag(self$n, nrow = self$n) - matrix(1, nrow = self$n, ncol = self$n) result &lt;- list(Delta = Delta, Delta_Delta = Delta_Delta) return(result) }, # mu迭代式 iter_mu = function(Q, Delta, eta_current, upsilon_current){ term_1 &lt;- solve(self$theta * Delta[[2]] + diag(1, nrow = self$n) - Q) term_2 &lt;- (diag(1, nrow = self$n) - Q) %*% self$y + self$theta * t(Delta[[1]]) %*% (eta_current - 1/self$theta * upsilon_current) mu_next &lt;- (term_1 %*% term_2) %&gt;% as.vector() return(mu_next) }, # beta迭代式 iter_beta = function(mu_next){ beta_next &lt;- solve(t(self$x) %*% self$x) %*% t(self$x) %*% (self$y - mu_next) return(beta_next) }, # eta迭代式 iter_eta = function(delta_current, lambda){ ST &lt;- function(t, lambda) sign(t) * max(abs(t)-lambda,0) delta_list &lt;- as.list(delta_current) delta_to_eta &lt;- function(delta_ij){ switch(self$penalty, &#39;L1&#39; = { eta_ij &lt;- ST(delta_ij, self$lambda/self$theta) }, &#39;MCP&#39; = { if(abs(delta_ij) &lt;= (self$gamma*self$lambda)){ eta_ij &lt;- ST(delta_ij, self$lambda/self$theta)/(1-1/(self$gamma*self$theta)) }else{ eta_ij &lt;- delta_ij } }, &#39;SCAD&#39; = { if(abs(delta_ij) &lt;= (self$lambda+self$lambda/self$theta)){ eta_ij &lt;- ST(delta_ij, self$lambda/self$theta) }else if(abs(delta_ij) &gt; (self$lambda+self$lambda/self$theta) &amp; abs(delta_ij) &lt;= (self$gamma*self$lambda)){ eta_ij &lt;- ST(delta_ij, self$gamma*self$lambda/((self$gamma-1)*self$theta))/(1-1/((self$gamma-1)*self$theta)) }else{ eta_ij &lt;- delta_ij } } ) return(eta_ij) } eta_next &lt;- delta_list %&gt;% map_vec(~delta_to_eta(.)) return(eta_next) }, # mu_i-mu_j迭代式 iter_delta_mu = function(mu_next){ delta_mu &lt;- -outer(mu_next, mu_next, &#39;-&#39;) delta_mu &lt;- delta_mu[lower.tri(delta_mu)] return(delta_mu) }, # delta迭代式 iter_delta = function(delta_mu, upsilon_current){ delta_next &lt;- delta_mu + 1/self$theta * upsilon_current return(delta_next) }, # upsilon迭代式 iter_upsilon = function(upsilon_current, delta_mu, eta_next){ upsilon_next &lt;- upsilon_current + self$theta * (delta_mu - eta_next) return(upsilon_next) }, # eta归类 # 注意到根据eta为0来分组刚好符合图论中“连通分量”的概念 cluster_eta = function(eta){ mat &lt;- matrix(NA, nrow = self$n, ncol = self$n) mat[lower.tri(mat)] &lt;- eta # 转化为矩阵方便提取索引 link &lt;- which(mat == 0, arr.ind = T) node &lt;- data.frame(name = 1:self$n) graph &lt;- graph_from_data_frame(link, directed = F, vertices = node) label &lt;- components(graph)$membership return(label) } ) ) 18.2.2 数据模拟 下面随机生成100条观测，设置3个预测变量，均服从标准正态分布，且两两之间的相关系数为0.3，然后随机生成\\(\\beta\\)值与\\(\\varepsilon\\)，设置截距项以相等的概率随机分为1或-1，根据上述变量生成响应变量。 library(MASS) set.seed(123) sigma &lt;- matrix(0.3, ncol = 3, nrow = 3) + diag(0.7, nrow = 3) x &lt;- mvrnorm(n=100, mu=rep(0,3), Sigma = sigma) epsilon &lt;- rnorm(100) mu &lt;- sample(c(1,-1), 100, replace = T, prob = rep(0.5, 2)) beta &lt;- runif(3) y &lt;- (mu + x %*% beta + epsilon) %&gt;% as.vector() 构建模型，并运行。 model &lt;- SubgroupIntercept$new(y, x, penalty = &#39;MCP&#39;, theta = 1, lambda = 0.5, gamma = 3, max_iter = 10000) result &lt;- model$run() ## [1] &quot;正在进行第[1]次迭代&quot; ## [1] &quot;正在进行第[2]次迭代&quot; ## [1] &quot;正在进行第[3]次迭代&quot; ## [1] &quot;正在进行第[4]次迭代&quot; ## [1] &quot;正在进行第[5]次迭代&quot; ## [1] &quot;正在进行第[6]次迭代&quot; ## [1] &quot;正在进行第[7]次迭代&quot; ## [1] &quot;正在进行第[8]次迭代&quot; ## [1] &quot;正在进行第[9]次迭代&quot; ## [1] &quot;正在进行第[10]次迭代&quot; ## [1] &quot;正在进行第[11]次迭代&quot; ## [1] &quot;正在进行第[12]次迭代&quot; ## [1] &quot;正在进行第[13]次迭代&quot; ## [1] &quot;正在进行第[14]次迭代&quot; ## [1] &quot;正在进行第[15]次迭代&quot; ## [1] &quot;正在进行第[16]次迭代&quot; ## [1] &quot;正在进行第[17]次迭代&quot; ## [1] &quot;正在进行第[18]次迭代&quot; ## [1] &quot;正在进行第[19]次迭代&quot; ## [1] &quot;正在进行第[20]次迭代&quot; ## [1] &quot;正在进行第[21]次迭代&quot; ## [1] &quot;正在进行第[22]次迭代&quot; ## [1] &quot;正在进行第[23]次迭代&quot; ## [1] &quot;正在进行第[24]次迭代&quot; ## [1] &quot;正在进行第[25]次迭代&quot; ## [1] &quot;正在进行第[26]次迭代&quot; ## [1] &quot;正在进行第[27]次迭代&quot; ## [1] &quot;正在进行第[28]次迭代&quot; ## [1] &quot;正在进行第[29]次迭代&quot; ## [1] &quot;正在进行第[30]次迭代&quot; ## [1] &quot;正在进行第[31]次迭代&quot; ## [1] &quot;正在进行第[32]次迭代&quot; ## [1] &quot;正在进行第[33]次迭代&quot; ## [1] &quot;正在进行第[34]次迭代&quot; ## [1] &quot;正在进行第[35]次迭代&quot; ## [1] &quot;正在进行第[36]次迭代&quot; ## [1] &quot;正在进行第[37]次迭代&quot; ## [1] &quot;正在进行第[38]次迭代&quot; ## [1] &quot;正在进行第[39]次迭代&quot; ## [1] &quot;正在进行第[40]次迭代&quot; ## [1] &quot;正在进行第[41]次迭代&quot; ## [1] &quot;正在进行第[42]次迭代&quot; ## [1] &quot;正在进行第[43]次迭代&quot; ## [1] &quot;正在进行第[44]次迭代&quot; ## [1] &quot;正在进行第[45]次迭代&quot; ## [1] &quot;正在进行第[46]次迭代&quot; ## [1] &quot;正在进行第[47]次迭代&quot; ## [1] &quot;正在进行第[48]次迭代&quot; ## [1] &quot;正在进行第[49]次迭代&quot; ## [1] &quot;正在进行第[50]次迭代&quot; ## [1] &quot;正在进行第[51]次迭代&quot; ## [1] &quot;正在进行第[52]次迭代&quot; ## [1] &quot;正在进行第[53]次迭代&quot; ## [1] &quot;正在进行第[54]次迭代&quot; ## [1] &quot;正在进行第[55]次迭代&quot; ## [1] &quot;正在进行第[56]次迭代&quot; ## [1] &quot;正在进行第[57]次迭代&quot; ## [1] &quot;正在进行第[58]次迭代&quot; ## [1] &quot;正在进行第[59]次迭代&quot; ## [1] &quot;正在进行第[60]次迭代&quot; ## [1] &quot;正在进行第[61]次迭代&quot; ## [1] &quot;正在进行第[62]次迭代&quot; ## [1] &quot;正在进行第[63]次迭代&quot; ## [1] &quot;正在进行第[64]次迭代&quot; ## [1] &quot;正在进行第[65]次迭代&quot; ## [1] &quot;正在进行第[66]次迭代&quot; ## [1] &quot;正在进行第[67]次迭代&quot; ## [1] &quot;正在进行第[68]次迭代&quot; ## [1] &quot;正在进行第[69]次迭代&quot; ## [1] &quot;正在进行第[70]次迭代&quot; ## [1] &quot;正在进行第[71]次迭代&quot; ## [1] &quot;正在进行第[72]次迭代&quot; ## [1] &quot;正在进行第[73]次迭代&quot; ## [1] &quot;正在进行第[74]次迭代&quot; ## [1] &quot;正在进行第[75]次迭代&quot; ## [1] &quot;正在进行第[76]次迭代&quot; ## [1] &quot;正在进行第[77]次迭代&quot; ## [1] &quot;正在进行第[78]次迭代&quot; ## [1] &quot;正在进行第[79]次迭代&quot; ## [1] &quot;正在进行第[80]次迭代&quot; ## [1] &quot;正在进行第[81]次迭代&quot; ## [1] &quot;正在进行第[82]次迭代&quot; ## [1] &quot;正在进行第[83]次迭代&quot; ## [1] &quot;正在进行第[84]次迭代&quot; ## [1] &quot;正在进行第[85]次迭代&quot; ## [1] &quot;正在进行第[86]次迭代&quot; ## [1] &quot;正在进行第[87]次迭代&quot; ## [1] &quot;正在进行第[88]次迭代&quot; ## [1] &quot;正在进行第[89]次迭代&quot; ## [1] &quot;正在进行第[90]次迭代&quot; ## [1] &quot;正在进行第[91]次迭代&quot; ## [1] &quot;正在进行第[92]次迭代&quot; ## [1] &quot;正在进行第[93]次迭代&quot; ## [1] &quot;正在进行第[94]次迭代&quot; ## [1] &quot;正在进行第[95]次迭代&quot; ## [1] &quot;正在进行第[96]次迭代&quot; ## [1] &quot;正在进行第[97]次迭代&quot; ## [1] &quot;正在进行第[98]次迭代&quot; ## [1] &quot;正在进行第[99]次迭代&quot; ## [1] &quot;正在进行第[100]次迭代&quot; ## [1] &quot;正在进行第[101]次迭代&quot; ## [1] &quot;正在进行第[102]次迭代&quot; ## [1] &quot;正在进行第[103]次迭代&quot; ## [1] &quot;正在进行第[104]次迭代&quot; ## [1] &quot;正在进行第[105]次迭代&quot; ## [1] &quot;正在进行第[106]次迭代&quot; ## [1] &quot;正在进行第[107]次迭代&quot; ## [1] &quot;正在进行第[108]次迭代&quot; ## [1] &quot;正在进行第[109]次迭代&quot; ## [1] &quot;正在进行第[110]次迭代&quot; ## [1] &quot;正在进行第[111]次迭代&quot; ## [1] &quot;正在进行第[112]次迭代&quot; ## [1] &quot;正在进行第[113]次迭代&quot; ## [1] &quot;正在进行第[114]次迭代&quot; ## [1] &quot;正在进行第[115]次迭代&quot; ## [1] &quot;正在进行第[116]次迭代&quot; ## [1] &quot;正在进行第[117]次迭代&quot; ## [1] &quot;正在进行第[118]次迭代&quot; ## [1] &quot;正在进行第[119]次迭代&quot; ## [1] &quot;正在进行第[120]次迭代&quot; ## [1] &quot;正在进行第[121]次迭代&quot; ## [1] &quot;正在进行第[122]次迭代&quot; ## [1] &quot;正在进行第[123]次迭代&quot; ## [1] &quot;正在进行第[124]次迭代&quot; ## [1] &quot;正在进行第[125]次迭代&quot; ## [1] &quot;正在进行第[126]次迭代&quot; ## [1] &quot;正在进行第[127]次迭代&quot; ## [1] &quot;正在进行第[128]次迭代&quot; ## [1] &quot;正在进行第[129]次迭代&quot; ## [1] &quot;正在进行第[130]次迭代&quot; ## [1] &quot;正在进行第[131]次迭代&quot; ## [1] &quot;正在进行第[132]次迭代&quot; ## [1] &quot;正在进行第[133]次迭代&quot; ## [1] &quot;正在进行第[134]次迭代&quot; ## [1] &quot;正在进行第[135]次迭代&quot; ## [1] &quot;正在进行第[136]次迭代&quot; ## [1] &quot;正在进行第[137]次迭代&quot; ## [1] &quot;正在进行第[138]次迭代&quot; ## [1] &quot;正在进行第[139]次迭代&quot; ## [1] &quot;正在进行第[140]次迭代&quot; ## [1] &quot;正在进行第[141]次迭代&quot; ## [1] &quot;正在进行第[142]次迭代&quot; ## [1] &quot;正在进行第[143]次迭代&quot; ## [1] &quot;正在进行第[144]次迭代&quot; ## [1] &quot;正在进行第[145]次迭代&quot; ## [1] &quot;正在进行第[146]次迭代&quot; ## [1] &quot;正在进行第[147]次迭代&quot; ## [1] &quot;正在进行第[148]次迭代&quot; ## [1] &quot;正在进行第[149]次迭代&quot; ## [1] &quot;正在进行第[150]次迭代&quot; ## [1] &quot;正在进行第[151]次迭代&quot; ## [1] &quot;正在进行第[152]次迭代&quot; ## [1] &quot;正在进行第[153]次迭代&quot; ## [1] &quot;正在进行第[154]次迭代&quot; ## [1] &quot;正在进行第[155]次迭代&quot; ## [1] &quot;正在进行第[156]次迭代&quot; ## [1] &quot;正在进行第[157]次迭代&quot; ## [1] &quot;正在进行第[158]次迭代&quot; ## [1] &quot;正在进行第[159]次迭代&quot; ## [1] &quot;正在进行第[160]次迭代&quot; ## [1] &quot;正在进行第[161]次迭代&quot; ## [1] &quot;正在进行第[162]次迭代&quot; ## [1] &quot;正在进行第[163]次迭代&quot; ## [1] &quot;正在进行第[164]次迭代&quot; ## [1] &quot;正在进行第[165]次迭代&quot; ## [1] &quot;正在进行第[166]次迭代&quot; ## [1] &quot;正在进行第[167]次迭代&quot; ## [1] &quot;正在进行第[168]次迭代&quot; ## [1] &quot;正在进行第[169]次迭代&quot; ## [1] &quot;正在进行第[170]次迭代&quot; ## [1] &quot;正在进行第[171]次迭代&quot; ## [1] &quot;正在进行第[172]次迭代&quot; ## [1] &quot;正在进行第[173]次迭代&quot; ## [1] &quot;正在进行第[174]次迭代&quot; ## [1] &quot;正在进行第[175]次迭代&quot; ## [1] &quot;正在进行第[176]次迭代&quot; ## [1] &quot;正在进行第[177]次迭代&quot; ## [1] &quot;正在进行第[178]次迭代&quot; ## [1] &quot;正在进行第[179]次迭代&quot; ## [1] &quot;正在进行第[180]次迭代&quot; ## [1] &quot;正在进行第[181]次迭代&quot; ## [1] &quot;正在进行第[182]次迭代&quot; ## [1] &quot;正在进行第[183]次迭代&quot; ## [1] &quot;正在进行第[184]次迭代&quot; ## [1] &quot;正在进行第[185]次迭代&quot; ## [1] &quot;正在进行第[186]次迭代&quot; ## [1] &quot;正在进行第[187]次迭代&quot; ## [1] &quot;正在进行第[188]次迭代&quot; ## [1] &quot;正在进行第[189]次迭代&quot; ## [1] &quot;正在进行第[190]次迭代&quot; ## [1] &quot;正在进行第[191]次迭代&quot; ## [1] &quot;正在进行第[192]次迭代&quot; ## [1] &quot;正在进行第[193]次迭代&quot; ## [1] &quot;正在进行第[194]次迭代&quot; ## [1] &quot;正在进行第[195]次迭代&quot; ## [1] &quot;正在进行第[196]次迭代&quot; ## [1] &quot;正在进行第[197]次迭代&quot; ## [1] &quot;正在进行第[198]次迭代&quot; ## [1] &quot;正在进行第[199]次迭代&quot; ## [1] &quot;正在进行第[200]次迭代&quot; ## [1] &quot;正在进行第[201]次迭代&quot; ## [1] &quot;正在进行第[202]次迭代&quot; ## [1] &quot;正在进行第[203]次迭代&quot; ## [1] &quot;正在进行第[204]次迭代&quot; ## [1] &quot;正在进行第[205]次迭代&quot; ## [1] &quot;正在进行第[206]次迭代&quot; ## [1] &quot;正在进行第[207]次迭代&quot; ## [1] &quot;正在进行第[208]次迭代&quot; ## [1] &quot;正在进行第[209]次迭代&quot; ## [1] &quot;正在进行第[210]次迭代&quot; ## [1] &quot;正在进行第[211]次迭代&quot; ## [1] &quot;正在进行第[212]次迭代&quot; ## [1] &quot;正在进行第[213]次迭代&quot; ## [1] &quot;正在进行第[214]次迭代&quot; ## [1] &quot;正在进行第[215]次迭代&quot; ## [1] &quot;正在进行第[216]次迭代&quot; ## [1] &quot;正在进行第[217]次迭代&quot; ## [1] &quot;正在进行第[218]次迭代&quot; ## [1] &quot;正在进行第[219]次迭代&quot; ## [1] &quot;正在进行第[220]次迭代&quot; ## [1] &quot;正在进行第[221]次迭代&quot; ## [1] &quot;正在进行第[222]次迭代&quot; ## [1] &quot;正在进行第[223]次迭代&quot; ## [1] &quot;正在进行第[224]次迭代&quot; ## [1] &quot;正在进行第[225]次迭代&quot; ## [1] &quot;正在进行第[226]次迭代&quot; ## [1] &quot;正在进行第[227]次迭代&quot; ## [1] &quot;正在进行第[228]次迭代&quot; ## [1] &quot;正在进行第[229]次迭代&quot; ## [1] &quot;正在进行第[230]次迭代&quot; ## [1] &quot;正在进行第[231]次迭代&quot; ## [1] &quot;正在进行第[232]次迭代&quot; ## [1] &quot;正在进行第[233]次迭代&quot; ## [1] &quot;正在进行第[234]次迭代&quot; ## [1] &quot;正在进行第[235]次迭代&quot; ## [1] &quot;正在进行第[236]次迭代&quot; ## [1] &quot;正在进行第[237]次迭代&quot; ## [1] &quot;正在进行第[238]次迭代&quot; ## [1] &quot;正在进行第[239]次迭代&quot; ## [1] &quot;正在进行第[240]次迭代&quot; ## [1] &quot;正在进行第[241]次迭代&quot; ## [1] &quot;正在进行第[242]次迭代&quot; ## [1] &quot;正在进行第[243]次迭代&quot; ## [1] &quot;正在进行第[244]次迭代&quot; ## [1] &quot;正在进行第[245]次迭代&quot; ## [1] &quot;正在进行第[246]次迭代&quot; ## [1] &quot;正在进行第[247]次迭代&quot; ## [1] &quot;正在进行第[248]次迭代&quot; ## [1] &quot;正在进行第[249]次迭代&quot; ## [1] &quot;正在进行第[250]次迭代&quot; ## [1] &quot;正在进行第[251]次迭代&quot; ## [1] &quot;正在进行第[252]次迭代&quot; ## [1] &quot;正在进行第[253]次迭代&quot; ## [1] &quot;正在进行第[254]次迭代&quot; ## [1] &quot;正在进行第[255]次迭代&quot; ## [1] &quot;正在进行第[256]次迭代&quot; ## [1] &quot;正在进行第[257]次迭代&quot; ## [1] &quot;正在进行第[258]次迭代&quot; ## [1] &quot;正在进行第[259]次迭代&quot; ## [1] &quot;正在进行第[260]次迭代&quot; ## [1] &quot;正在进行第[261]次迭代&quot; ## [1] &quot;正在进行第[262]次迭代&quot; ## [1] &quot;正在进行第[263]次迭代&quot; ## [1] &quot;正在进行第[264]次迭代&quot; ## [1] &quot;正在进行第[265]次迭代&quot; ## [1] &quot;正在进行第[266]次迭代&quot; ## [1] &quot;正在进行第[267]次迭代&quot; ## [1] &quot;正在进行第[268]次迭代&quot; ## [1] &quot;正在进行第[269]次迭代&quot; ## [1] &quot;正在进行第[270]次迭代&quot; ## [1] &quot;正在进行第[271]次迭代&quot; ## [1] &quot;正在进行第[272]次迭代&quot; ## [1] &quot;正在进行第[273]次迭代&quot; ## [1] &quot;正在进行第[274]次迭代&quot; ## [1] &quot;正在进行第[275]次迭代&quot; ## [1] &quot;正在进行第[276]次迭代&quot; ## [1] &quot;正在进行第[277]次迭代&quot; ## [1] &quot;正在进行第[278]次迭代&quot; ## [1] &quot;正在进行第[279]次迭代&quot; ## [1] &quot;正在进行第[280]次迭代&quot; ## [1] &quot;正在进行第[281]次迭代&quot; ## [1] &quot;正在进行第[282]次迭代&quot; ## [1] &quot;正在进行第[283]次迭代&quot; ## [1] &quot;正在进行第[284]次迭代&quot; ## [1] &quot;正在进行第[285]次迭代&quot; ## [1] &quot;正在进行第[286]次迭代&quot; ## [1] &quot;正在进行第[287]次迭代&quot; ## [1] &quot;正在进行第[288]次迭代&quot; ## [1] &quot;正在进行第[289]次迭代&quot; ## [1] &quot;正在进行第[290]次迭代&quot; ## [1] &quot;正在进行第[291]次迭代&quot; ## [1] &quot;正在进行第[292]次迭代&quot; ## [1] &quot;正在进行第[293]次迭代&quot; ## [1] &quot;正在进行第[294]次迭代&quot; ## [1] &quot;正在进行第[295]次迭代&quot; ## [1] &quot;正在进行第[296]次迭代&quot; ## [1] &quot;正在进行第[297]次迭代&quot; ## [1] &quot;正在进行第[298]次迭代&quot; ## [1] &quot;正在进行第[299]次迭代&quot; ## [1] &quot;正在进行第[300]次迭代&quot; ## [1] &quot;正在进行第[301]次迭代&quot; ## [1] &quot;正在进行第[302]次迭代&quot; ## [1] &quot;正在进行第[303]次迭代&quot; ## [1] &quot;正在进行第[304]次迭代&quot; ## [1] &quot;正在进行第[305]次迭代&quot; ## [1] &quot;正在进行第[306]次迭代&quot; ## [1] &quot;正在进行第[307]次迭代&quot; ## [1] &quot;正在进行第[308]次迭代&quot; ## [1] &quot;正在进行第[309]次迭代&quot; ## [1] &quot;正在进行第[310]次迭代&quot; ## [1] &quot;正在进行第[311]次迭代&quot; ## [1] &quot;正在进行第[312]次迭代&quot; ## [1] &quot;正在进行第[313]次迭代&quot; ## [1] &quot;正在进行第[314]次迭代&quot; ## [1] &quot;正在进行第[315]次迭代&quot; ## [1] &quot;正在进行第[316]次迭代&quot; ## [1] &quot;正在进行第[317]次迭代&quot; ## [1] &quot;正在进行第[318]次迭代&quot; ## [1] &quot;正在进行第[319]次迭代&quot; ## [1] &quot;正在进行第[320]次迭代&quot; ## [1] &quot;正在进行第[321]次迭代&quot; ## [1] &quot;正在进行第[322]次迭代&quot; ## [1] &quot;正在进行第[323]次迭代&quot; ## [1] &quot;正在进行第[324]次迭代&quot; ## [1] &quot;正在进行第[325]次迭代&quot; ## [1] &quot;正在进行第[326]次迭代&quot; ## [1] &quot;正在进行第[327]次迭代&quot; ## [1] &quot;正在进行第[328]次迭代&quot; ## [1] &quot;正在进行第[329]次迭代&quot; ## [1] &quot;正在进行第[330]次迭代&quot; ## [1] &quot;正在进行第[331]次迭代&quot; ## [1] &quot;正在进行第[332]次迭代&quot; ## [1] &quot;正在进行第[333]次迭代&quot; ## [1] &quot;正在进行第[334]次迭代&quot; ## [1] &quot;正在进行第[335]次迭代&quot; ## [1] &quot;正在进行第[336]次迭代&quot; ## [1] &quot;正在进行第[337]次迭代&quot; ## [1] &quot;正在进行第[338]次迭代&quot; ## [1] &quot;正在进行第[339]次迭代&quot; ## [1] &quot;正在进行第[340]次迭代&quot; ## [1] &quot;正在进行第[341]次迭代&quot; ## [1] &quot;正在进行第[342]次迭代&quot; ## [1] &quot;正在进行第[343]次迭代&quot; ## [1] &quot;正在进行第[344]次迭代&quot; ## [1] &quot;正在进行第[345]次迭代&quot; ## [1] &quot;正在进行第[346]次迭代&quot; ## [1] &quot;正在进行第[347]次迭代&quot; ## [1] &quot;正在进行第[348]次迭代&quot; ## [1] &quot;正在进行第[349]次迭代&quot; ## [1] &quot;正在进行第[350]次迭代&quot; ## [1] &quot;正在进行第[351]次迭代&quot; ## [1] &quot;正在进行第[352]次迭代&quot; ## [1] &quot;正在进行第[353]次迭代&quot; ## [1] &quot;正在进行第[354]次迭代&quot; ## [1] &quot;正在进行第[355]次迭代&quot; ## [1] &quot;正在进行第[356]次迭代&quot; ## [1] &quot;正在进行第[357]次迭代&quot; ## [1] &quot;正在进行第[358]次迭代&quot; ## [1] &quot;正在进行第[359]次迭代&quot; ## [1] &quot;正在进行第[360]次迭代&quot; ## [1] &quot;正在进行第[361]次迭代&quot; ## [1] &quot;正在进行第[362]次迭代&quot; ## [1] &quot;正在进行第[363]次迭代&quot; ## [1] &quot;正在进行第[364]次迭代&quot; ## [1] &quot;正在进行第[365]次迭代&quot; ## [1] &quot;正在进行第[366]次迭代&quot; ## [1] &quot;正在进行第[367]次迭代&quot; ## [1] &quot;正在进行第[368]次迭代&quot; ## [1] &quot;正在进行第[369]次迭代&quot; ## [1] &quot;正在进行第[370]次迭代&quot; ## [1] &quot;正在进行第[371]次迭代&quot; ## [1] &quot;正在进行第[372]次迭代&quot; ## [1] &quot;正在进行第[373]次迭代&quot; ## [1] &quot;正在进行第[374]次迭代&quot; ## [1] &quot;正在进行第[375]次迭代&quot; ## [1] &quot;正在进行第[376]次迭代&quot; ## [1] &quot;正在进行第[377]次迭代&quot; ## [1] &quot;正在进行第[378]次迭代&quot; ## [1] &quot;正在进行第[379]次迭代&quot; ## [1] &quot;正在进行第[380]次迭代&quot; ## [1] &quot;正在进行第[381]次迭代&quot; ## [1] &quot;正在进行第[382]次迭代&quot; ## [1] &quot;正在进行第[383]次迭代&quot; ## [1] &quot;正在进行第[384]次迭代&quot; ## [1] &quot;正在进行第[385]次迭代&quot; ## [1] &quot;正在进行第[386]次迭代&quot; ## [1] &quot;正在进行第[387]次迭代&quot; ## [1] &quot;正在进行第[388]次迭代&quot; ## [1] &quot;正在进行第[389]次迭代&quot; ## [1] &quot;正在进行第[390]次迭代&quot; ## [1] &quot;正在进行第[391]次迭代&quot; ## [1] &quot;正在进行第[392]次迭代&quot; ## [1] &quot;正在进行第[393]次迭代&quot; ## [1] &quot;正在进行第[394]次迭代&quot; ## [1] &quot;正在进行第[395]次迭代&quot; ## [1] &quot;正在进行第[396]次迭代&quot; ## [1] &quot;正在进行第[397]次迭代&quot; ## [1] &quot;正在进行第[398]次迭代&quot; ## [1] &quot;正在进行第[399]次迭代&quot; ## [1] &quot;正在进行第[400]次迭代&quot; ## [1] &quot;正在进行第[401]次迭代&quot; ## [1] &quot;正在进行第[402]次迭代&quot; ## [1] &quot;正在进行第[403]次迭代&quot; ## [1] &quot;正在进行第[404]次迭代&quot; ## [1] &quot;正在进行第[405]次迭代&quot; ## [1] &quot;正在进行第[406]次迭代&quot; ## [1] &quot;正在进行第[407]次迭代&quot; ## [1] &quot;正在进行第[408]次迭代&quot; ## [1] &quot;正在进行第[409]次迭代&quot; ## [1] &quot;正在进行第[410]次迭代&quot; ## [1] &quot;正在进行第[411]次迭代&quot; ## [1] &quot;正在进行第[412]次迭代&quot; ## [1] &quot;正在进行第[413]次迭代&quot; ## [1] &quot;正在进行第[414]次迭代&quot; ## [1] &quot;正在进行第[415]次迭代&quot; ## [1] &quot;正在进行第[416]次迭代&quot; ## [1] &quot;正在进行第[417]次迭代&quot; ## [1] &quot;正在进行第[418]次迭代&quot; ## [1] &quot;正在进行第[419]次迭代&quot; ## [1] &quot;正在进行第[420]次迭代&quot; ## [1] &quot;正在进行第[421]次迭代&quot; ## [1] &quot;正在进行第[422]次迭代&quot; ## [1] &quot;正在进行第[423]次迭代&quot; ## [1] &quot;正在进行第[424]次迭代&quot; ## [1] &quot;正在进行第[425]次迭代&quot; ## [1] &quot;正在进行第[426]次迭代&quot; ## [1] &quot;正在进行第[427]次迭代&quot; ## [1] &quot;正在进行第[428]次迭代&quot; ## [1] &quot;正在进行第[429]次迭代&quot; ## [1] &quot;正在进行第[430]次迭代&quot; ## [1] &quot;正在进行第[431]次迭代&quot; ## [1] &quot;正在进行第[432]次迭代&quot; ## [1] &quot;正在进行第[433]次迭代&quot; ## [1] &quot;正在进行第[434]次迭代&quot; ## [1] &quot;正在进行第[435]次迭代&quot; ## [1] &quot;正在进行第[436]次迭代&quot; ## [1] &quot;正在进行第[437]次迭代&quot; ## [1] &quot;正在进行第[438]次迭代&quot; ## [1] &quot;正在进行第[439]次迭代&quot; ## [1] &quot;正在进行第[440]次迭代&quot; ## [1] &quot;正在进行第[441]次迭代&quot; ## [1] &quot;正在进行第[442]次迭代&quot; ## [1] &quot;正在进行第[443]次迭代&quot; ## [1] &quot;正在进行第[444]次迭代&quot; ## [1] &quot;正在进行第[445]次迭代&quot; ## [1] &quot;正在进行第[446]次迭代&quot; ## [1] &quot;正在进行第[447]次迭代&quot; ## [1] &quot;正在进行第[448]次迭代&quot; ## [1] &quot;正在进行第[449]次迭代&quot; ## [1] &quot;正在进行第[450]次迭代&quot; ## [1] &quot;正在进行第[451]次迭代&quot; ## [1] &quot;正在进行第[452]次迭代&quot; ## [1] &quot;正在进行第[453]次迭代&quot; ## [1] &quot;正在进行第[454]次迭代&quot; ## [1] &quot;正在进行第[455]次迭代&quot; ## [1] &quot;正在进行第[456]次迭代&quot; ## [1] &quot;正在进行第[457]次迭代&quot; ## [1] &quot;正在进行第[458]次迭代&quot; ## [1] &quot;正在进行第[459]次迭代&quot; ## [1] &quot;正在进行第[460]次迭代&quot; ## [1] &quot;正在进行第[461]次迭代&quot; ## [1] &quot;正在进行第[462]次迭代&quot; ## [1] &quot;正在进行第[463]次迭代&quot; ## [1] &quot;正在进行第[464]次迭代&quot; ## [1] &quot;正在进行第[465]次迭代&quot; ## [1] &quot;正在进行第[466]次迭代&quot; ## [1] &quot;正在进行第[467]次迭代&quot; ## [1] &quot;正在进行第[468]次迭代&quot; ## [1] &quot;正在进行第[469]次迭代&quot; ## [1] &quot;达到精度要求&quot; ## [1] &quot;花费时间： 12.076 秒&quot; 截距项的分组结果如下所示 result$alpha ## # A tibble: 4 × 3 ## label alpha size ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 -1.39 44 ## 2 2 0.995 51 ## 3 3 -3.33 3 ## 4 4 2.96 2 References [32] MA S. HUANG J. A Concave Pairwise Fusion Approach to Subgroup Analysis[J/OL]. Journal of the American Statistical Association, 2017, 112(517): 410-423. DOI:10.1080/01621459.2016.1148039. "],["code_3.html", "18.3 （异质）线性+非线性的Cox比例风险模型", " 18.3 （异质）线性+非线性的Cox比例风险模型 该节是对Subgroup detection in the heterogeneous partially linear additive Cox model[33]论文的复现。 \\[ \\lambda(t|X_i,Z_i)=\\lambda_0(t)\\exp\\{X_i^T\\beta_i+\\sum_{j=1}^q f_j(Z_{ij})\\} \\tag{18.2} \\] 该论文在Cox比例风险模型的基础上，不单单引入回归系数的异质性，还引入了\\(f(\\cdot)\\)来捕捉非线性效应。其中\\(f(\\cdot)\\)利用B-spline去近似，同时也引入融合惩罚项\\(u_{ik}=\\beta_i-\\beta_k\\)与\\(Y&#39;\\)，通过majorized ADMM算法进行求解。 18.3.1 自定义算法 算法逻辑： 传入参数 T：矩阵，必须包含列名“time”和“status”，分别表示观测时间和最终状态 X：具有线性效应、异质性的协变量矩阵 Z：具有非线性效应的协变量矩阵 penalty：惩罚函数类型，SCAD或MCP K：K-means的聚类个数 \\(\\lambda\\)：惩罚函数中的惩罚系数 a: 融合惩罚中的正则化因子，默认SCAD是3.7，MCP是2.5 \\(\\theta\\)：majorized ADMM算法的惩罚系数 df：splines::bs()的参数，控制基函数个数，默认为6，详见splines::bs()，下同 degree：splines::bs()的参数，设置基函数的次数，默认为3 tol：收敛精度，默认为0.001 max_iter：最大迭代次数，默认为10000 其余符号说明 除了传入参数外，在运算过程中还有其它符号，其含义如下所示。 \\(\\beta\\)：\\((\\beta_1^T,\\cdots,\\beta_n^T)^T\\)，长度为\\(np\\)的向量 \\(\\gamma\\)：\\((\\gamma_1^T,\\cdots,\\gamma_q^T)^T\\)，长度为\\(dq\\)的向量，其中\\(\\gamma_j=(\\gamma_{j1},\\cdots,\\gamma_{jd})^T\\) \\(u\\)：\\((u_{ik}^T,i&lt;k)^T\\)，长度为\\(\\frac{n(n-1)}{2}p\\)的向量，其中\\(u_{ik}=\\beta_i-\\beta_k\\) \\(Y\\)：\\((Y_1,\\cdots,Y_n)^T\\)，长度为\\(n\\)的向量，\\(Y_i=X_i^T\\beta_i+B_i(Z_i)^T\\gamma\\) \\(Y&#39;\\)的含义类似，只是与\\(Y\\)的更新规则不同 \\(w\\)：\\((w_1,\\cdots,w_n)^T\\)，长度为\\(n\\)的拉格朗日乘子向量 \\(\\nu\\)：\\((\\nu_{ik}^T, i&lt;k)^T\\)，长度为\\(\\frac{n(n-1)}{2}p\\)的拉格朗日乘子向量 \\(B\\)：\\((B_1(Z_1),\\cdots,B_n(Z_n))^T\\)，\\(n\\times (dq)\\)维矩阵，其中每个\\(B_i(Z_i)\\)有\\(q\\)个分量，每个分量又能由\\(d\\)个基函数表示 就是按列拼接而成的基函数矩阵，并且经过列方向上的中心化处理 \\(X\\)：\\(\\textrm{diag}\\{X_1^T,\\cdots,X_n^T\\}\\)，应该是\\(n \\times (pn)\\)维矩阵 这里的\\(\\textrm{diag}\\)是针对\\(X_i^T\\)而言的，若把\\(X_i^T\\)展开就不是真正意义上的对角阵，而是类似阶梯状的矩阵 \\(D\\)：\\(\\{(e_i-e_j), i&lt;j\\}^T\\)，\\(\\frac{n(n-1)}{2} \\times n\\)矩阵，\\(e_i\\)是第\\(i\\)个分量为1，其余分量为0并且长度为\\(n\\)的向量 \\(A\\)：\\(D \\otimes I_p\\)，\\(\\frac{n(n-1)}{2}p \\times np\\)维矩阵 \\(Q\\)：\\(I_n-B(B^TB)^{-1}B^T\\) \\(\\tilde g_j\\)：\\(\\sum_{i=1}^n \\delta_iI_{j \\in R_i}\\)，即第\\(j\\)个对象出现在所有风险集中的次数 \\(\\nabla_ig(Y&#39;^{(m+1)})\\)：\\(-\\delta_i+\\sum_{k=1}^n \\delta_k[\\exp (Y_i&#39;^{(m+1)})\\cdot I_{i \\in R_k}]/[\\sum_{l \\in R_k} \\exp (Y_l&#39;^{(m+1)})]\\) \\(c_{ik}\\)：\\(\\beta_i-\\beta_k+\\nu_{ik}/\\theta\\) 初始值 对矩阵\\(X\\)进行K-means聚类，一般聚成2-5类即可。对每一类分别拟合基础的Cox比例风险模型，将回归系数作为对应观测的初始值。其余参数的初始值分别设置为\\(u^{(0)}=A\\beta^{(0)}, w^{(0)}=0,\\nu^{(0)}=0,Y^{(0)}=Y&#39;^{(0)}=X\\beta^{(0)}+B\\gamma^{(0)}\\)。 原文仅提到根据\\(X\\)先进行聚类，再拟合Cox模型，但由于假设\\(Z\\)是同质的，如果也先聚类再拟合，得到的各组回归系数并不相同甚至维度也不满足\\(\\gamma\\)的长度，因此将协变量\\(Z\\)转化成\\(B\\)后单独拟合Cox模型，将该次回归系数作为\\(\\gamma\\)的初始值 迭代 迭代顺序为\\(\\gamma,\\beta;Y&#39;,Y,u;w,\\nu\\) \\[ \\begin{aligned} \\gamma^{(m+1)} &amp;= (B^TB)^{-1}B^T(Y^{(m)}-X\\beta^{(m)}+\\frac{w^{(m)}}{\\theta}) \\\\ \\beta^{(m+1)} &amp;= (X^TQX+A^TA)^{-1}[X^TQ(\\frac{w^{(m)}}{\\theta}+Y^{(m)})+A^T(u^{(m)}-\\frac{\\nu ^{(m)}}{\\theta})] \\\\ Y_i&#39;^{(m+1)} &amp;= X_i^T\\beta_i^{(m+1)}+B_i(Z_i)^T\\gamma^{(m+1)} \\\\ Y_i^{(m+1)} &amp;= (\\tilde g_i + \\theta)^{-1} [-\\nabla_ig(Y&#39;^{(m+1)})+\\tilde g_i Y_i&#39;^{(m+1)} - w_i^{(m)}+\\theta (X_i^T\\beta_i^{(m+1)}+B_i(Z_i)^T\\gamma^{(m+1)})] \\\\ u^{(m+1)} &amp;= \\textrm{Penalty} \\\\ w_i^{(m+1)} &amp;= w_i^{(m)}+\\theta (Y_i^{(m+1)}-X_i^T\\beta^{(m+1)}-B_i(Z_i)^T\\gamma^{(m+1)}) \\\\ \\nu_{ik}^{(m+1)} &amp;= \\nu_{ik}^{(m)}+\\theta (\\beta_i^{(m+1)}-\\beta_k^{(m+1)}-u_{ik}^{(m+1)}) \\end{aligned} \\] 停止 设置停止条件为达到最大迭代次数或者残差\\(r\\)满足一定的精度要求即可。 \\[ r^{(m+1)} = ||A\\beta^{(m+1)}-u^{(m+1)}||+||Y^{(m+1)}-X\\beta^{(m+1)}-B\\gamma^{(m+1)}|| \\] 输出 beta：列表，X的异质性回归系数 gamma：向量，基函数的回归系数 label：向量，样本亚组标签 alpha：数据框，beta亚组 library(tidyverse) library(survival) library(Matrix) library(splines) library(R6) SubgroupBeta &lt;- R6Class( classname &lt;- &#39;SubgroupBeta&#39;, public &lt;- list( # 传入参数 T = NULL, # 观测时间与最终状态 X = NULL, # 具有线性效应、异质性的协变量矩阵 Z = NULL, # 具有非线性效应的协变量矩阵 n = NULL, # 样本容量 p = NULL, # X维度p q = NULL, # Z维度q penalty = NULL, # 惩罚函数，SCAD或MCP，默认为MCP K = NULL, # K-means聚类的个数，默认为2 lambda = NULL, # 惩罚函数的惩罚系数 a = NULL, # 融合惩罚中的正则化因子，默认SCAD是3.7，MCP是2.5 theta = NULL, # majorized ADMM算法的惩罚系数 df = NULL, # 基函数个数 degree = NULL, # 基函数的次数 tol = NULL, # 收敛精度，默认为0.001 max_iter = NULL, # 最大迭代次数，默认为10000 # 初始化 initialize = function(T, X, Z, penalty = &#39;MCP&#39;, K = 2, lambda, a = NULL, theta, df = 6, degree = 3, tol = 0.001, max_iter = 10000){ self$T &lt;- T self$X &lt;- X self$Z &lt;- Z self$n &lt;- dim(T)[1] # 观测数 self$p &lt;- dim(X)[2] # 维度p self$q &lt;- dim(Z)[2] # 维度q self$penalty &lt;- penalty self$K &lt;- K self$lambda &lt;- lambda self$a &lt;- a self$theta &lt;- theta self$df &lt;- df self$degree &lt;- degree self$tol &lt;- tol self$max_iter &lt;- max_iter }, # 主函数——运行 run = function(trace = TRUE){ start_time &lt;- proc.time() # 检验输入是否合理 private$validate() # 获取初始值 initial_value &lt;- private$initial_value() private$beta &lt;- initial_value[[1]] B &lt;- initial_value[[2]] private$gamma &lt;- initial_value[[3]] X_ls &lt;- initial_value[[4]] private$Y &lt;- initial_value[[5]] private$Y2 &lt;- initial_value[[5]] A &lt;- private$gen_A() private$u &lt;- A %*% unlist(private$beta) private$w &lt;- rep(0, self$n) private$nu &lt;- rep(0, self$n * (self$n-1) * self$p / 2) # 计算矩阵Q Q &lt;- diag(1, ncol = self$n, nrow = self$n)- B %*% solve(t(B) %*% B) %*% t(B) # 计算向量g g &lt;- private$gen_g() # 计算(B&#39;B)^{-1}B&#39;，用于gamma更新 B_ols &lt;- solve(t(B) %*% B) %*% t(B) # 计算(X&#39;QX+A&#39;A)^{-1}与X&#39;Q，用于beta更新 X_diag &lt;- map(X_ls, ~matrix(., nrow = 1, byrow = T)) X_diag &lt;- do.call(bdiag, X_diag) %&gt;% as.matrix() # 转化为对角形式的X XQX_AA &lt;- solve(t(X_diag) %*% Q %*% X_diag + t(A) %*% A) XQ &lt;- t(X_diag) %*% Q # 参数迭代 for (i in 1:self$max_iter) { if(trace == TRUE) print(paste0(&#39;正在进行第[&#39;, i, &#39;]次迭代&#39;)) private$gamma &lt;- private$iter_gamma(X_ls, B_ols, private$Y, private$beta, private$w) private$beta &lt;- private$iter_beta(XQX_AA, XQ, A, private$w, private$Y, private$u, private$nu) private$Y2 &lt;- private$iter_Y2(X_ls, B, g, private$beta, private$gamma) private$Y &lt;- private$iter_Y(g, private$Y2, private$w) private$u &lt;- private$iter_u(self$penalty, private$beta, private$nu, A) private$w &lt;- private$iter_w(private$w, private$Y, private$Y2) private$nu &lt;- private$iter_nu(private$nu, private$beta, private$u, A) # 终止条件 term_1 &lt;- A %*% unlist(private$beta) - private$u term_2 &lt;- private$Y - X_diag %*% unlist(private$beta) - B %*% private$gamma if((norm(term_1, type = &#39;2&#39;) + norm(term_2, type = &#39;2&#39;)) &lt;= self$tol){ if(trace == TRUE) print(&#39;达到精度要求&#39;) break } } # beta亚组 private$beta &lt;- lapply(private$beta, round, digits = 2) str_beta &lt;- sapply(private$beta, paste, collapse = &#39;,&#39;) label &lt;- as.numeric(factor(str_beta, levels = unique(str_beta))) subgroup_beta &lt;- tibble(label = label, beta = private$beta) %&gt;% group_by(label) %&gt;% summarise(size = n(), beta = unique(beta)) %&gt;% arrange(-size) K_hat &lt;- unique(label) %&gt;% length() result &lt;- list(beta = private$beta, gamma = private$gamma, label = label, K_hat = K_hat, alpha = subgroup_beta) end_time &lt;- proc.time() cost_time &lt;- end_time - start_time print(cost_time) return(result) }, # 主函数——调优 tune_lambda = function(seq_lambda, trace = TRUE){ start_time &lt;- proc.time() # 检验输入是否合理 private$validate() # 获取初始值 initial_value &lt;- private$initial_value() private$beta &lt;- initial_value[[1]] B &lt;- initial_value[[2]] private$gamma &lt;- initial_value[[3]] X_ls &lt;- initial_value[[4]] private$Y &lt;- initial_value[[5]] private$Y2 &lt;- initial_value[[5]] A &lt;- private$gen_A() private$u &lt;- A %*% unlist(private$beta) private$w &lt;- rep(0, self$n) private$nu &lt;- rep(0, self$n * (self$n-1) * self$p / 2) # 计算矩阵Q Q &lt;- diag(1, ncol = self$n, nrow = self$n)- B %*% solve(t(B) %*% B) %*% t(B) # 计算向量g g &lt;- private$gen_g() # 计算(B&#39;B)^{-1}B&#39;，用于gamma更新 B_ols &lt;- solve(t(B) %*% B) %*% t(B) # 计算(X&#39;QX+A&#39;A)^{-1}与X&#39;Q，用于beta更新 X_diag &lt;- map(X_ls, ~matrix(., nrow = 1, byrow = T)) X_diag &lt;- do.call(bdiag, X_diag) %&gt;% as.matrix() # 转化为对角形式的X XQX_AA &lt;- solve(t(X_diag) %*% Q %*% X_diag + t(A) %*% A) XQ &lt;- t(X_diag) %*% Q bic_vec &lt;- rep(0, length(seq_lambda)) result_ls &lt;- vector(&#39;list&#39;, length = length(seq_lambda)) for (i in 1:length(seq_lambda)) { self$lambda &lt;- seq_lambda[i] # Warm Start，仅保留上轮的beta和gamma，其余恢复为初始值设定 private$Y &lt;- private$Y2 # Y2 = X_beta + B_gamma，因此保留，故Y的初始值与Y2相同 private$u &lt;- A %*% unlist(private$beta) private$w &lt;- rep(0, self$n) private$nu &lt;- rep(0, self$n * (self$n-1) * self$p / 2) for (j in 1:self$max_iter) { if(trace == TRUE) print(paste0(&#39;lambda = &#39;, self$lambda, &#39;; 第[&#39;, j, &#39;]次迭代&#39;)) private$gamma &lt;- private$iter_gamma(X_ls, B_ols, private$Y, private$beta, private$w) private$beta &lt;- private$iter_beta(XQX_AA, XQ, A, private$w, private$Y, private$u, private$nu) private$Y2 &lt;- private$iter_Y2(X_ls, B, g, private$beta, private$gamma) private$Y &lt;- private$iter_Y(g, private$Y2, private$w) private$u &lt;- private$iter_u(self$penalty, private$beta, private$nu, A) private$w &lt;- private$iter_w(private$w, private$Y, private$Y2) private$nu &lt;- private$iter_nu(private$nu, private$beta, private$u, A) # 终止条件 term_1 &lt;- A %*% unlist(private$beta) - private$u term_2 &lt;- private$Y - X_diag %*% unlist(private$beta) - B %*% private$gamma if((norm(term_1, type = &#39;2&#39;) + norm(term_2, type = &#39;2&#39;)) &lt;= self$tol){ if(trace == TRUE) cat(&#39;==========\\n&#39;) if(trace == TRUE) print(paste0(&#39;lambda = &#39;, self$lambda, &#39; 达到精度要求&#39;)) break } } # beta亚组 private$beta &lt;- lapply(private$beta, round, digits = 2) str_beta &lt;- sapply(private$beta, paste, collapse = &#39;,&#39;) label &lt;- as.numeric(factor(str_beta, levels = unique(str_beta))) subgroup_beta &lt;- tibble(label = label, beta = private$beta) %&gt;% group_by(label) %&gt;% summarise(size = n(), beta = unique(beta)) %&gt;% arrange(-size) K_hat &lt;- unique(label) %&gt;% length() bic_vec[i] &lt;- private$Bic(private$Y2, K_hat) if(trace == TRUE) print(paste0(&#39;BIC: &#39;, round(bic_vec[i],3))) if(trace == TRUE) cat(&#39;==========\\n&#39;) result &lt;- list(beta = private$beta, gamma = private$gamma, label = label, K_hat = K_hat, alpha = subgroup_beta, best_lambda = self$lambda, BIC = bic_vec[i]) result_ls[[i]] &lt;- result } best_index &lt;- which.min(bic_vec) best_result &lt;- result_ls[[best_index]] end_time &lt;- proc.time() print(end_time - start_time) return(list(best_result = best_result, BIC = bic_vec)) } ), private &lt;- list( # 迭代参数 gamma = NULL, # B样条的回归系数 beta = NULL, # X的回归系数 Y2 = NULL, # Y&#39; Y = NULL, # Y u = NULL, # 融合惩罚项 w = NULL, # 拉格朗日乘子 nu = NULL, # 拉格朗日乘子 c = NULL, # beta_i-beta_k+nu_ik/theta bic = NULL, # BIC # 验证输入是否正确 validate = function(){ if(!(dim(self$T)[2] == 2 &amp; all(colnames(self$T) %in% c(&#39;time&#39;, &#39;status&#39;)))) stop(&#39;T要求为包含time和status的两列矩阵&#39;) if(!is.matrix(self$X)) stop(&#39;X要求为矩阵&#39;) if(!is.matrix(self$Z)) stop(&#39;Z要求为矩阵&#39;) if(!self$penalty %in% c(&#39;MCP&#39;,&#39;SCAD&#39;)) stop(&#39;请选择合适的惩罚函数，SCAD或MCP&#39;) if(!(self$K &gt; 0 &amp; self$K == as.integer(self$K))) stop(&#39;确保K是正整数&#39;) if(self$lambda &lt;= 0) stop(&#39;请选择合适的lambda值&#39;) if(self$a &lt;= 0) stop(&#39;请选择合适的theta值&#39;) if(self$theta &lt;= 0) stop(&#39;请选择合适的theta值&#39;) if(!(self$df &gt; 0 &amp; self$df == as.integer(self$df))) stop(&#39;请选择合适的df值&#39;) if(!(self$degree &gt; 0 &amp; self$degree == as.integer(self$degree))) stop(&#39;请选择合适的degree值&#39;) if(self$tol &lt;= 0) stop(&#39;请选择合适的精度要求&#39;) if(self$max_iter &lt;= 0) stop(&#39;请选择合适的最大迭代次数&#39;) }, # 获取初始值 initial_value = function(){ # beta初始值 result &lt;- kmeans(self$X, centers = self$K) df &lt;- cbind(self$T, self$X) %&gt;% as.data.frame() df$label &lt;- result$cluster # 添加类别标签 df &lt;- df %&gt;% group_nest(label) %&gt;% # 分组回归，批量建模 mutate(model = map(data, ~coxph(Surv(time, status)~., data=.x)), coef = map(model, ~.x[[&#39;coefficients&#39;]])) coef &lt;- df$coef %&gt;% as.list() # 提取每个类别的回归系数向量 beta_0 &lt;- coef[result$cluster] # 为了后续处理方便，beta暂时以列表形式存储 # gamma初始值与B B &lt;- asplit(self$Z, 2) # 按列分割Z矩阵，分别由bs拟合 B &lt;- B %&gt;% map(~bs(., df = self$df, degree = self$degree)) B &lt;- do.call(cbind, B) %&gt;% scale(center = TRUE, scale = FALSE) # 列方向的中心化处理 colnames(B) &lt;- paste0(&#39;b_&#39;, 1:ncol(B)) df &lt;- cbind(self$T, B) %&gt;% as.data.frame() model &lt;- coxph(Surv(time, status)~., data = df) gamma_0 &lt;- model[[&#39;coefficients&#39;]] # Y与Y&#39;的初始值 X_ls &lt;- asplit(self$X, 1) Y_0 &lt;- map2_vec(X_ls, beta_0, ~.x %*% .y) + B %*% gamma_0 return(list(beta_0 = beta_0, B = B, gamma_0 = gamma_0, X_ls = X_ls, Y_0 = Y_0)) }, # 计算矩阵A gen_A = function(){ gen_mat &lt;- function(n){ mat_1 &lt;- if(n == self$n-1){ NULL }else{ matrix(0, nrow = self$n-1-n, ncol = n) } mat_2 &lt;- matrix(1, nrow = 1, ncol = n, byrow = T) mat_3 &lt;- diag(-1, nrow=n, ncol = n) mat &lt;- rbind(mat_1, mat_2, mat_3) return(mat) } D &lt;- as.list(c((self$n-1):1)) %&gt;% map(~gen_mat(.)) D &lt;- do.call(cbind, D) %&gt;% t() A &lt;- D %x% diag(1, ncol = self$p, nrow = self$p) return(A) }, # 计算g gen_g = function(){ status &lt;- self$T[,&#39;status&#39;] %&gt;% as.vector() time &lt;- self$T[,&#39;time&#39;] %&gt;% as.vector() g &lt;- as.list(time) %&gt;% map(~ifelse(. &gt;= time, 1, 0)) %&gt;% map_vec(~status %*% .) return(g) }, # 计算nabla_g gen_nabla_g = function(Y2){ status &lt;- self$T[,&#39;status&#39;] %&gt;% as.vector() time &lt;- self$T[,&#39;time&#39;] %&gt;% as.vector() exp_Y2 &lt;- as.list(exp(Y2)) # 向量sum_{l \\in R_k} exp(Y2) sum_l_Rk &lt;- as.list(time) %&gt;% map(~which(. &lt;= time)) %&gt;% map_vec(~sum(exp(Y2[.]))) # 计算nabla_g nabla_g &lt;- as.list(time) %&gt;% map(~ifelse(. &gt;= time, 1, 0)) %&gt;% # 计算I_{i \\in R_k} map(~. %*% diag(status) %*% sum_l_Rk^(-1)) %&gt;% # 计算delta与I与{l \\in R_k}的复合项 map2_vec(exp_Y2, ~.x * .y) - status return(nabla_g) }, # 计算c，输出列表 gen_c = function(beta, nu, A){ delta_beta &lt;- A %*% unlist(beta) c &lt;- delta_beta + nu / self$theta c &lt;- matrix(c, ncol = self$p, byrow = T) %&gt;% asplit(1) return(c) }, # gamma迭代式 iter_gamma = function(X_ls, B_ols, Y_current, beta_current, w_current){ # 这里的beta是列表形式 X_beta &lt;- map2_vec(X_ls, beta_current, ~.x %*% .y) gamma_next &lt;- B_ols %*% (Y_current - X_beta + w_current / self$theta) return(gamma_next) }, # beta迭代式，输出列表 iter_beta = function(XQX_AA, XQ, A, w_current, Y_current, u_current, nu_current){ beta_next &lt;- XQX_AA %*% (XQ %*% (w_current / self$theta + Y_current) + t(A) %*% (u_current - nu_current / self$theta)) beta_next &lt;- matrix(beta_next, ncol = self$p, byrow = T) %&gt;% asplit(1) # 输出列表形式的beta return(beta_next) }, # Y2迭代式 iter_Y2 = function(X_ls, B, g, beta_next, gamma_next){ term_1 &lt;- map2_vec(X_ls, beta_next, ~.x %*% .y) # X与beta term_2 &lt;- asplit(B,1) %&gt;% map_vec(~. %*% gamma_next) # B与gamma Y2_next &lt;- term_1 + term_2 return(Y2_next) }, # Y迭代式 iter_Y = function(g, Y2_next, w_current){ nabla_g_next &lt;- private$gen_nabla_g(Y2_next) Y_next &lt;- (g + self$theta)^(-1) * (-nabla_g_next+ g * Y2_next - w_current + self$theta * Y2_next) return(Y_next) }, # u迭代式 iter_u = function(penalty, beta_next, nu_current, A){ S &lt;- function(c, lambda){ result &lt;- max((1 - lambda / norm(c, type = &#39;2&#39;)), 0) * c return(result) } c &lt;- private$gen_c(beta_next, nu_current, A) switch(penalty, &#39;SCAD&#39; = { if(is.null(self$a)) self$a &lt;- 3.7 u_next &lt;- map(c, function(c_ik){ norm_c_ik &lt;- norm(c_ik, type = &#39;2&#39;) if(norm_c_ik &lt;= self$lambda + self$lambda / self$theta){ u_ik &lt;- S(c_ik, self$lambda / self$theta) }else if(self$lambda + self$lambda / self$theta &lt; norm_c_ik &amp; norm_c_ik &lt;= self$a * self$lambda){ u_ik &lt;- S(c_ik, self$a * self$lambda / ((self$a - 1) * self$theta)) / (1-1/((self$a - 1) * self$theta)) }else { u_ik &lt;- c_ik } return(u_ik) }) %&gt;% unlist() }, &#39;MCP&#39; = { if(is.null(self$a)) self$a &lt;- 2.5 u_next &lt;- map(c, function(c_ik){ norm_c_ik &lt;- norm(c_ik, type = &#39;2&#39;) if(norm_c_ik &lt;= self$a * self$lambda){ u_ik &lt;- S(c_ik, self$lambda / self$theta) / (1-1/(self$a * self$theta)) }else { u_ik &lt;- c_ik } return(u_ik) }) %&gt;% unlist() } ) return(u_next) }, # w迭代式 iter_w = function(w_current, Y_next, Y2_next){ w_next &lt;- w_current + self$theta * (Y_next - Y2_next) return(w_next) }, # nu迭代式 iter_nu = function(nu_current, beta_next, u_next, A){ delta_beta &lt;- A %*% unlist(beta_next) nu_next &lt;- nu_current + self$theta * (delta_beta - u_next) return(nu_next) }, # BIC准则 Bic = function(Y2, K){ # X_beta+B_gamma就是Y2 status &lt;- self$T[,&#39;status&#39;] %&gt;% as.vector() time &lt;- self$T[,&#39;time&#39;] %&gt;% as.vector() log_sum_l_Ri &lt;- as.list(time) %&gt;% map(~which(. &lt;= time)) %&gt;% map_vec(~log(sum(exp(Y2[.])))) term_1 &lt;- -sum(Y2[status] - log_sum_l_Ri[status]) term_2 &lt;- log(self$n * K + self$q) * log(self$n) * (K * self$p + self$q) / self$n bic &lt;- term_1 + term_2 return(bic) } ) ) 下面给出python版本的代码。 import numpy as np import pandas as pd import time from sklearn.cluster import KMeans from sklearn.preprocessing import scale from scipy import sparse from scipy.linalg import block_diag from lifelines import CoxPHFitter from patsy import bs class SubgroupBeta: def __init__(self, time_status, X, Z, lambda_value, a = None, penalty = &#39;MCP&#39;, K = 2, theta = 1, df = 6, degree = 3, tol = 1e-4, max_iter = 10000): self.time_status = time_status self.X = X self.Z = Z self.n = X.shape[0] self.p = X.shape[1] self.q = Z.shape[1] self.lambda_value = lambda_value self.a = a self.penalty = penalty self.K = K self.theta = theta self.df = df self.degree = degree self.tol = tol self.max_iter = max_iter self.gamma = None self.beta = None self.Y2 = None self.Y = None self.u = None self.w = None self.nu = None self.c = None self.bic = None # 拟合cox模型 def fit_cox_model(self, data): cph = CoxPHFitter() cph.fit(data, duration_col = &#39;time&#39;, event_col = &#39;status&#39;) coef = np.array(cph.params_).reshape(-1,1) return coef # 生成B样条的基函数矩阵 def gen_B(self, z, df = 6, degree = 3, intercept = False): B = bs(z, df = df, degree = degree, include_intercept = intercept) return B def initial_value(self): # kmeans kmeans = KMeans(self.K, random_state = 1) kmeans.fit(self.X) labels = kmeans.labels_ # beta初始值 col_names = [&#39;time&#39;, &#39;status&#39;] + [f&#39;X_{i+1}&#39; for i in range(self.p)] df = pd.DataFrame(np.hstack((self.time_status, self.X)), columns = col_names) df[&#39;label&#39;] = labels beta_cox = df.groupby(&#39;label&#39;, group_keys=False).apply(self.fit_cox_model, include_groups=False) beta_0 = np.hstack(beta_cox.tolist()) # 每列都是beta的系数 beta_0 = beta_0[:, labels] # gamma初始值 B = [] for col_Z in range(self.q): z = self.Z[:, col_Z] B_col_Z = self.gen_B(z) B.append(B_col_Z) B = np.hstack(B) B = scale(B, axis = 0, with_mean = True, with_std = False) col_names = [&#39;time&#39;, &#39;status&#39;] + [f&#39;b_{i+1}&#39; for i in range(B.shape[1])] df = pd.DataFrame(np.hstack((self.time_status, B)), columns = col_names) gamma_0 = self.fit_cox_model(df) # Y与Y2的初始值 Y_0 = np.einsum(&#39;ij,ji-&gt;i&#39;, self.X, beta_0).reshape(-1,1) + B @ gamma_0 return beta_0, B, gamma_0, Y_0 # 生成矩阵A def gen_A(self): # 生成稀疏Delta矩阵 rows = [] for i in range(self.n-1): row = sparse.lil_matrix((self.n-1-i, self.n)) for j in range(i+1, self.n): row[j-i-1, i] = 1 row[j-i-1, j] = -1 rows.append(row) Delta = sparse.vstack(rows) A = sparse.kron(Delta, np.eye(self.p)) return A # 计算g def gen_g(self): time_obs = self.time_status[:, 0] # R是风险集，每行都是第i个元素的风险集 R = (time_obs[:, np.newaxis] &gt;= time_obs).astype(int) g = R @ self.time_status[:, 1].reshape(-1,1) return g # 计算nabla_g def gen_nabla_g(self, Y2): time_obs = self.time_status[:, 0].flatten() status = self.time_status[:, 1].reshape(-1,1) exp_Y2 = np.exp(Y2) R = (time_obs[:, np.newaxis] &lt;= time_obs).astype(int) sum_l_Rk = 1 / (R @ exp_Y2) R_rev = (time_obs[:, np.newaxis] &gt;= time_obs).astype(int) nabla_g = -status + exp_Y2 * (R_rev @ (status * sum_l_Rk)) return nabla_g # 计算c def gen_c(self, beta, nu, A): delta_beta = A @ beta.flatten(order = &#39;F&#39;).reshape(-1,1) c = delta_beta + nu / self.theta # 每行都是beta_i - beta_k c = c.reshape(-1, self.p) return c, delta_beta # gamma迭代式 def iter_gamma(self, B_ols, Y_current, beta_current, w_current): X_beta = np.einsum(&#39;ij,ji-&gt;i&#39;, self.X, beta_current).reshape(-1, 1) gamma_next = B_ols @ (Y_current - X_beta + w_current / self.theta) return gamma_next # beta迭代式 def iter_beta(self, XQX_AA, XQ, A, w_current, Y_current, u_current, nu_current): beta_next = XQX_AA @ (XQ @ (w_current / self.theta + Y_current) + A.T @ (u_current - nu_current / self.theta)) beta_next = beta_next.reshape(self.p, -1, order = &#39;F&#39;) return beta_next # Y2迭代式 def iter_Y2(self, B, g, beta_next, gamma_next): term_1 = np.einsum(&#39;ij,ji-&gt;i&#39;, self.X, beta_next).reshape(-1, 1) term_2 = B @ gamma_next Y2_next = term_1 + term_2 return Y2_next # Y迭代式 def iter_Y(self, g, Y2_next, w_current): nabla_g_next = self.gen_nabla_g(Y2_next) Y_next = 1/(g + self.theta) * (-nabla_g_next + g * Y2_next - w_current + self.theta * Y2_next) return Y_next # u迭代式 def penalty_fun(self): def S(c_ik, lambda_val): result = np.max([(1 - lambda_val / np.linalg.norm(c_ik, ord = 2)), 0]) * c_ik return result def SCAD(c): if self.a is None: self.a = 3.7 lamba_lambda_theta = self.lambda_value + self.lambda_value / self.theta a_lambda = self.a * self.lambda_value norm_c_vec = np.linalg.norm(c, ord = 2, axis = 1) cond_1 = norm_c_vec &lt;= lamba_lambda_theta cond_2 = (lamba_lambda_theta &lt; norm_c_vec) &amp; (norm_c_vec &lt;= a_lambda) cond_3 = norm_c_vec &gt; a_lambda u_next = np.copy(c) if np.any(cond_1): u_next[cond_1, :] = np.apply_along_axis(S, axis = 1, arr = c[cond_1, :], lambda_val = self.lambda_value/self.theta) if np.any(cond_2): u_next[cond_2, :] = np.apply_along_axis(S, axis = 1, arr = c[cond_2, :], lambda_val = self.a * self.lambda_value/((self.a - 1) * self.theta)) / (1 - 1 / ((self.a-1) * self.theta)) if np.any(cond_3): u_next[cond_3, :] = c[cond_3, :] return u_next def MCP(c): if self.a is None: self.a = 2.5 a_lambda = self.a * self.lambda_value norm_c_vec = np.linalg.norm(c, ord = 2, axis = 1) cond_1 = norm_c_vec &lt;= a_lambda cond_2 = norm_c_vec &gt; a_lambda u_next = np.copy(c) if np.any(cond_1): u_next[cond_1, :] = np.apply_along_axis(S, axis = 1, arr = c[cond_1, :], lambda_val = self.lambda_value / self.theta) / (1 - 1 / (self.a * self.theta)) if np.any(cond_2): u_next[cond_2, :] = c[cond_2, :] return u_next if self.penalty == &#39;SCAD&#39;: return SCAD elif self.penalty == &#39;MCP&#39;: return MCP else: raise ValueError(&quot;Invalid penalty function type&quot;) def iter_u(self, beta_next, nu_current, A, penalty_fun): c, delta_beta = self.gen_c(beta_next, nu_current, A) u_next = penalty_fun(c).flatten().reshape(-1,1) return u_next, delta_beta # w迭代式 def iter_w(self, w_current, Y_next, Y2_next): w_next = w_current + self.theta * (Y_next - Y2_next) return w_next # nu迭代式 def iter_nu(self, nu_current, delta_beta, u_next): u_next = u_next.flatten().reshape(-1,1) nu_next = nu_current + self.theta * (delta_beta - u_next) return nu_next # 主函数——运行 def run(self, trace = True): start_time = time.time() # 获取初始值 self.beta, B, self.gamma, self.Y = self.initial_value() self.Y2 = self.Y A = self.gen_A() self.u = A @ self.beta.flatten(order = &#39;F&#39;).reshape(-1,1) self.w = np.zeros((self.n, 1)) self.nu = np.zeros((int(self.n * (self.n - 1) * self.p / 2), 1)) # 计算Q矩阵 Q = np.eye(self.n) - B @ np.linalg.inv(B.T @ B) @ B.T # 计算g g = self.gen_g() # 计算(B&#39;B)^(-1)B&#39; B_ols = np.linalg.inv(B.T @ B) @ B.T # 计算(X&#39;QX+A&#39;A)^{-1}与X&#39;Q X_diag = np.split(self.X, self.n, axis = 0) X_diag = block_diag(*X_diag) XQX_AA = np.linalg.inv(X_diag.T @ Q @ X_diag + A.T @ A) XQ = X_diag.T @ Q # 生成惩罚函数 penalty_fun = self.penalty_fun() for i in range(self.max_iter): if trace: print(f&#39;第[{i+1}]次迭代&#39;) self.gamma = self.iter_gamma(B_ols, self.Y, self.beta, self.w) self.beta = self.iter_beta(XQX_AA, XQ, A, self.w, self.Y, self.u, self.nu) self.Y2 = self.iter_Y2(B, g, self.beta, self.gamma) self.Y = self.iter_Y(g, self.Y2, self.w) self.u, delta_beta = self.iter_u(self.beta, self.nu, A, penalty_fun) self.w = self.iter_w(self.w, self.Y, self.Y2) self.nu = self.iter_nu(self.nu, delta_beta, self.u) # 终止条件 term_1 = delta_beta - self.u term_2 = self.Y - self.Y2 norm_r = np.linalg.norm(term_1, ord=2) + np.linalg.norm(term_2, ord = 2) if norm_r &lt;= self.tol: if trace == True: print(&#39;达到精度要求&#39;) break # beta亚组 self.beta = np.round(self.beta, 3) subgroup_beta = np.unique(self.beta, axis = 1, return_counts = True) size = subgroup_beta[1] subgroup_beta = subgroup_beta[0].T self.K = subgroup_beta.shape[0] df = pd.DataFrame([{&#39;beta&#39;: tuple(row.tolist())} for row in subgroup_beta]) df[&#39;size&#39;] = size df = df.sort_values(by = &#39;size&#39;, ascending = False).reset_index(drop = True) df[&#39;label&#39;] = range(self.K) df_label = pd.DataFrame([{&#39;beta&#39;: tuple(row.tolist())} for row in self.beta.T]) df_label = df_label.merge(df, on = &#39;beta&#39;, how = &#39;left&#39;) label = df_label[&#39;label&#39;].tolist() result = { &#39;beta&#39; : self.beta.T, &#39;gamma&#39; : self.gamma, &#39;alpha&#39; : df, &#39;K&#39; : self.K, &#39;label&#39; : label } print(f&#39;耗时：{time.time() - start_time:.2f}s&#39;) return result # 主函数——调优 def tune_lambda(self, seq_lambda, seed = None, trace = True): start_time = time.time() # 获取初始值 beta_0, B, gamma_0, Y_0 = self.initial_value() A = self.gen_A() u_0 = A @ beta_0.flatten(order = &#39;F&#39;).reshape(-1,1) w_0 = np.zeros((self.n, 1)) nu_0 = np.zeros((int(self.n * (self.n - 1) * self.p / 2), 1)) # 计算Q矩阵 Q = np.eye(self.n) - B @ np.linalg.inv(B.T @ B) @ B.T # 计算g g = self.gen_g() # 计算(B&#39;B)^(-1)B&#39; B_ols = np.linalg.inv(B.T @ B) @ B.T # 计算(X&#39;QX+A&#39;A)^{-1}与X&#39;Q X_diag = np.split(self.X, self.n, axis = 0) X_diag = block_diag(*X_diag) XQX_AA = np.linalg.inv(X_diag.T @ Q @ X_diag + A.T @ A) XQ = X_diag.T @ Q # 生成惩罚函数 penalty_fun = self.penalty_fun() # 存储结果 bic_ls = [] bic_log_ls = [] bic_c_ls = [] bic_logk_ls = [] result_ls = [] K_ls = [] for i in range(len(seq_lambda)): self.lambda_value = seq_lambda[i] # 初始化 self.beta = beta_0 self.gamma = gamma_0 self.Y = Y_0 self.Y2 = Y_0 self.u = u_0 self.w = w_0 self.nu = nu_0 for j in range(self.max_iter): if trace: print(f&#39;seed_[{seed+1}]: lambda={seq_lambda[i]}--[{j+1}]&#39;) self.gamma = self.iter_gamma(B_ols, self.Y, self.beta, self.w) self.beta = self.iter_beta(XQX_AA, XQ, A, self.w, self.Y, self.u, self.nu) self.Y2 = self.iter_Y2(B, g, self.beta, self.gamma) self.Y = self.iter_Y(g, self.Y2, self.w) self.u, delta_beta = self.iter_u(self.beta, self.nu, A, penalty_fun) self.w = self.iter_w(self.w, self.Y, self.Y2) self.nu = self.iter_nu(self.nu, delta_beta, self.u) # 终止条件 term_1 = delta_beta - self.u term_2 = self.Y - self.Y2 norm_r = np.linalg.norm(term_1, ord=2) + np.linalg.norm(term_2, ord = 2) if norm_r &lt;= self.tol: if trace == True: print(&#39;达到精度要求&#39;) break # beta亚组 self.beta = np.round(self.beta, 3) subgroup_beta = np.unique(self.beta, axis = 1, return_counts = True) size = subgroup_beta[1] subgroup_beta = subgroup_beta[0].T self.K = subgroup_beta.shape[0] df = pd.DataFrame([{&#39;beta&#39;: tuple(row.tolist())} for row in subgroup_beta]) df[&#39;size&#39;] = size df = df.sort_values(by = &#39;size&#39;, ascending = False).reset_index(drop = True) df[&#39;label&#39;] = range(self.K) df_label = pd.DataFrame([{&#39;beta&#39;: tuple(row.tolist())} for row in self.beta.T]) df_label = df_label.merge(df, on = &#39;beta&#39;, how = &#39;left&#39;) label = df_label[&#39;label&#39;].tolist() # 计算bic time_obs = self.time_status[:, 0] status = self.time_status[:, 1].reshape(1,-1) R = (time_obs[:, np.newaxis] &lt;= time_obs).astype(int) term_1 = - status @ (self.Y2 - np.log(R @ np.exp(self.Y2))) term_2 = np.log(self.n * self.K + self.q) * np.log(self.n) * (self.K * self.p + self.q) / self.n bic_value = term_1 + term_2 bic_value = round(bic_value.item(), 3) bic_ls.append(bic_value) term_2 = np.log(np.log(self.n * self.K + self.q)) * np.log(self.n) * (self.K * self.p + self.q) / self.n bic_log_value = term_1 + term_2 bic_log_value = round(bic_log_value.item(), 3) bic_log_ls.append(bic_log_value) term_2 = 0.5 * np.log(self.n * self.K + self.q) * np.log(self.n) * (self.K * self.p + self.q) / self.n bic_c_value = term_1 + term_2 bic_c_value = round(bic_c_value.item(), 3) bic_c_ls.append(bic_c_value) term_2 = np.log(self.K) * np.log(self.n * self.K + self.q) * np.log(self.n) * (self.K * self.p + self.q) / self.n bic_logk_value = term_1 + term_2 bic_logk_value = round(bic_logk_value.item(), 3) bic_logk_ls.append(bic_logk_value) K_ls.append(self.K) result = { &#39;beta&#39; : self.beta.T, &#39;gamma&#39; : self.gamma, &#39;alpha&#39; : df, &#39;K&#39; : self.K, &#39;label&#39; : label, &#39;bic&#39; : bic_value } result_ls.append(result) best_index = bic_ls.index(min(bic_ls)) best_result = result_ls[best_index] print(f&#39;总耗时{time.time() - start_time:.2f}s&#39;) tune_result = {&#39;bic&#39; : bic_ls, &#39;result&#39; : result_ls, &#39;best_result&#39; : best_result, &#39;bic_log&#39; : bic_log_ls, &#39;bic_c&#39; : bic_c_ls, &#39;K_ls&#39; : K_ls, &#39;bic_logk&#39; : bic_logk_ls} return tune_result if __name__ == &quot;__main__&quot;: total_start_time = time.time() def gen_data(seed, n=100): np.random.seed(seed) X = np.random.normal(loc = 0, scale = 1, size = (n, 2)) Z_1 = np.random.uniform(low = 0, high = 1, size = (n, 1)) f_Z1 = np.sin(np.pi * (Z_1-0.5)) Z_2 = np.random.uniform(low = 0, high = 1, size = (n, 1)) f_Z2 = np.cos(np.pi * (Z_2 - 0.5)) - 2/np.pi Z = np.hstack((Z_1, Z_2)) kmeans = KMeans(2, random_state = 1) kmeans.fit(X) labels = kmeans.labels_ X_1 = X[labels == 0, :] X_2 = X[labels == 1, :] X = np.vstack((X_1, X_2)) beta = 3 * np.ones((int(n/2),2)) beta = np.vstack((-beta, beta)) log_U = np.log(np.random.uniform(0, 1, size = (n,1))) X_beta = np.einsum(&#39;ij,ji-&gt;i&#39;, X, beta.T).reshape(-1,1) time_obs = -np.exp(-X_beta - f_Z1 - f_Z2) * log_U status = np.random.choice([0,1], size = (n,1), p = [0.2, 0.8]) time_status = np.hstack((time_obs, status)) return time_status, X, Z np.random.seed(564) seed_ls = np.random.randint(low = 1, high = 1000, size= 10).tolist() seq_lambda = np.arange(0.04, 0.075, 0.005) bic_mat = np.zeros((len(seed_ls), len(seq_lambda))) K_mat = np.zeros((len(seed_ls), len(seq_lambda))) result_ls = [] for i in range(len(seed_ls)): seed = seed_ls[i] time_status, X, Z = gen_data(n=100,seed = seed) model = SubgroupBeta(time_status, X, Z, lambda_value = 0.06, tol = 1e-3) result_tune = model.tune_lambda(seq_lambda, seed = i) bic_mat[i] = result_tune[&#39;bic&#39;] K_mat[i] = result_tune[&#39;K_ls&#39;] result_ls.append(result_tune[&#39;result&#39;]) seed_ls = list(map(str, seed_ls)) seq_lambda = list(map(str, seq_lambda.tolist())) bic_mat = pd.DataFrame(bic_mat, index = seed_ls, columns = seq_lambda) K_mat = pd.DataFrame(K_mat, index = seed_ls, columns = seq_lambda) total_end_time = time.time() print(f&#39;所有种子总计耗时：{total_end_time - total_start_time}&#39;) 18.3.2 数据模拟 # case_3 # 有顺序调整 set.seed(123) x_1 &lt;- rnorm(100) x_2 &lt;- rnorm(100) X &lt;- cbind(x_1, x_2) X_cluster &lt;- kmeans(X, centers = 2) X_1 &lt;- X[which(X_cluster$cluster == 1),] X_2 &lt;- X[which(X_cluster$cluster == 2),] X &lt;- rbind(X_1,X_2) X_diag &lt;- map(asplit(X,1), ~matrix(., nrow = 1, byrow = T)) X_diag &lt;- do.call(bdiag, X_diag) %&gt;% as.matrix() z_1 &lt;- runif(100) z_2 &lt;- runif(100) f_z1 &lt;- sin((z_1 - 0.5) * pi) f_z2 &lt;- cos((z_2 - 0.5) * pi) - 2 / pi beta &lt;- c(rep(c(3,3), times = 50), rep(c(-3,-3), times = 50)) time &lt;- as.vector(-exp(-X_diag %*% beta - f_z1 - f_z2)) * log(runif(100)) status &lt;- sample(c(1,0), size = 100, replace = TRUE, prob = c(0.8, 0.2)) T &lt;- cbind(time, status) Z &lt;- cbind(z_1, z_2) case3 &lt;- SubgroupBeta$new(T = T, X = X, Z = Z, penalty = &#39;MCP&#39;, K = 2, lambda = 0.1, a = 2.5, theta = 1, df = 6, degree = 3) result3 &lt;- case3$run(trace = FALSE) ## 用户 系统 流逝 ## 119.50 5.81 127.66 result3$alpha ## label size beta ## 1 1 56 2.26, 2.19 ## 2 2 44 -2.56, -2.54 result3$alpha$beta ## [[1]] ## [1] 2.26 2.19 ## ## [[2]] ## [1] -2.56 -2.54 References [33] CAI T. HU T. Subgroup detection in the heterogeneous partially linear additive Cox model[J/OL]. Journal of Nonparametric Statistics, 2024, 0(0): 1-26. DOI:10.1080/10485252.2024.2303103. "],["ball.html", "19 羽毛球修炼手册", " 19 羽毛球修炼手册 谨此纪念将要进行一辈子的运动 打，都可以打，打到天荒地老； 杀，都可以杀，杀到震天动地； 挑，都可以挑，挑到日月星辰； 吊，都可以吊，吊到望尘莫及； 劈，都可以劈，劈到石破天惊； 推，都可以推，推到悬崖勒马； 抽，都可以抽，抽到风声鹤唳； 搓，都可以搓，搓到晕头转向； … "],["ball_1.html", "19.1 发球", " 19.1 发球 19.1.1 正手发球 3步学会正手发高远球！ 正手发高远球超级慢动作 19.1.2 反手发球 反手发球 三个小技巧，发小球贴网过 "],["ball_2.html", "19.2 网前技术", " 19.2 网前技术 19.2.1 勾对角 被动勾球总下网？出界？不好控制？主动勾球在被动却打不出？ 低位勾对角，你学会了吗 白嫖课｜反手勾对角｜今天的教学很正经 19.2.2 扑球 反手网前扑球 19.2.3 封网 羽毛球双打封网教学！黄雅琼正反手封网技巧揭秘！12分钟极限干货 双打封网总是慢？这些细节助你成为网前雨刮器。 19.2.4 平抽 握拍微调 羽毛球万能平抽挡教学！职业级抽档握拍细节！进阶必学 平抽档直接进入专业级！价值十亿 19.2.5 抹球 白嫖课｜网前漏勺必学的反手抹球！ 19.2.6 搓球 4步学会平搓技术，打出颗颗都是混网球～ 展搓 反手位三种控网方式：展搓 收搓 挡网 分别对应对手的贴网球，近网球和远网球 19.2.7 挡网 挡网 "],["ball_3.html", "19.3 后场技术", " 19.3 后场技术 19.3.1 杀球 刘辉教练告诉你杀球压不下去怎么办 杀球如何更快更猛！快来学习吧 暴力杀球，鞭打/闪动发力是如何正确的做？教你如何打出有速度，隐蔽与突然的球！【李宇轩教练】 改掉这5点，杀球碾压90%的人 如何加快你的杀球速度，2步搞定！ 杀球如何更有力？ 19.3.2 吊球 五种国家队吊球手法 劈吊精髓 羽毛球滑板吊球教学！小白从0到1滑拍斜线细节纠错！打太直和压球问题解决方案 头顶滑板4个重要细节 正手滑板直线 劈吊和吊球 肯定更想学隐蔽性高，球速快，容易一击得分的那一个啊～ 19.3.3 反手 反手高远 反手高远球速通 反手高远球4个重要细节 反手只会过渡可不行，反手抽球学起来 反手过渡对角线 反手被动球处理不好，来看过渡斜线教学 反手吊球没有球速，或者又高又远。 你要知道发力靠手腕，下压靠手腕，手臂外旋以手腕为轴是关键。4个错误动作看看你有没有中招 "],["ball_4.html", "19.4 假动作", " 19.4 假动作 羽毛球假动作合集 19.4.1 发接发 10种羽毛球接发假动作 金庭经典接发球假动作-正手“假放真拐” 接发球假动作-反拍正打 正手接发球：假扑真放 接发球假动作：让点反推 白嫖课｜回南宁找姐夫学个接发“骚招”｜大年初一就开始卷！ 双打接发假动作 实用双打接发假动作～让点推球（反手） 19.4.2 网前 羽毛球五种超级实用正手网前假动作 羽毛球假动作“反手切勾” 羽毛球停顿假动作教学！网前反手假推真放细节纠错！你骗不到人可能是因为这个 假动作反手提拉教学！挑战十秒教会你！炫酷实用又好学！价值一亿！ 有没有被这招停顿骗过？现在给你一个学习反手停顿假动作的机会，去球场操作对手， 看会了就拿起球拍试试 "],["ball_5.html", "19.5 双打", " 19.5 双打 19.5.1 发接发 羽毛球双打单数区接发，如果你还打不开只会向上走。降低重心，立拍面接球。只要落点在双打边线，都是好球 双打接发只会起球？放小球容易被抓？教你如何快速接发作球。 双打单数区接发还不会做球？网前、中腰常用线路球学起来 你还不会双打接发？？？ 羽毛球接发球“邪修”练法合集（右区） 接发还是只会挑？不知道往哪打，今天告诉你五种回球线路。想抢到合适的点位，一定要降重心准备 19.5.2 站位 双打网前小细节 双打站位走位快来学学吧 男双中，就算在网前也要随时跟球移动，和搭档保持一定距离，并且根据队友的出球变换启动脚步，而不是站在中间傻等球来 5种双打入门跑位，一条视频全部学会 双打进阶-封网站位技巧 混双小秘籍，女生专辑 19.5.3 分球 双打的变线 国家队必学双打分球技巧 19.5.4 球路 混双男生中后场处理 "],["ball_6.html", "19.6 球路", " 19.6 球路 双打高手常用7种接发套路 “6种双打前三拍”球路 常见又好用的球路三字经，今晚就用起来 羽毛球正手后场10个常用球路合集｜动作一致性 羽毛球高手路上必学的4种球路 "],["ball_7.html", "19.7 步伐", " 19.7 步伐 步法弹性 羽毛球步伐 还不会前后移动的小伙伴，来看基础的前进后退步伐，上三步退三步，别在场上定在原地，或者横冲直撞了 为什么总来不及侧身转体？解决办法在这儿 接杀总差一步？接完球重心总回不来？重心不知道怎么转换？ 网前两点接吊还不会？只要对手球速快，贴网栽下来，总被定在原地看…来看正确的步伐需要几步去，怎样启动➕怎样练习～ 四种头顶后场步伐 回动步应用 "],["jap.html", "20 日语", " 20 日语 在英语外再学新的语言吧，姑且先拿日语试试水。 "],["参考文献.html", "参考文献", " 参考文献 [1] SIMON N. FRIEDMAN J H. HASTIE T. 等. Regularization paths for Cox’s proportional hazards model via coordinate descent[J]. Journal of Statistical Software, 2011, 39: 1-13. [2] 何勇. 石伟. 陈旭辉. 基于社会网络分析的文旅产业融合空间结构研究[J]. 东南大学学报（哲学社会科学版）, 2024, 26(3): 107-116. [3] 张正峰. 张栋. 基于社会网络分析的京津冀地区碳排放空间关联与碳平衡分区[J]. 中国环境科学, 2023, 43(4): 2057-2068. [4] 邱志萍. 刘举胜. 何建佳. 我国商贸流通网络的结构特征及驱动因素——基于引力模型的社会网络分析[J]. 中国流通经济, 2023, 37(2): 31-42. [5] 张露露. 刘安诺. 尤梅. 等. 社区老年人老化态度的潜在剖面分析及其与社会网络的关系[J]. 军事护理, 2024, 41(8): 39-42, 73. [6] 黄文胜. 肖利斌. 郑向敏. 民营酒店员工社会网络类型和结构特性研究——基于中心性、凝聚子群的视角[J]. 企业经济, 2020(5): 87-94. [7] 张鑫. 刘秉权. 王晓龙. 复杂网络中社区发现方法的研究[J]. 计算机工程与应用, 2015, 51(24): 1-7. [8] 端祥宇. 袁冠. 孟凡荣. 动态社区发现方法研究综述[J]. 计算机科学与探索, 2021, 15(4): 612-630. [9] 刘军. QAP:测量\"关系\"之间关系的一种方法[J]. 社会, 2007, 27(4): 164-174. [10] 姜小鱼. 陈秧分. 农业对外投资布局对母国参与全球价值链的影响——基于社会网络分析视角[J]. 华中农业大学学报（社会科学版）, 2023(3): 44-53. [11] 王珊珊. 孙程九. 经济制裁是否破坏了增加值贸易网络——基于全球制裁数据的社会网络分析[J]. 国际贸易问题, 2023(2): 74-91. [12] 张亚明. 付尧飞. 宋雯婕. 等. 社交媒体时代全球智库国际传播力研究:基于87家智库Twitter账户的社交网络分析[J]. 智库理论与实践, 2024, 9(2): 48-59. [13] FRIEDMAN J. HASTIE T. TIBSHIRANI R. Regularization paths for generalized linear models via coordinate descent[J]. Journal of Statistical Software, 2010, 33(1): 1. [14] FAN J. LI R. Variable selection via nonconcave penalized likelihood and its oracle properties[J]. Journal of the American Statistical Association, 2001, 96(456): 1348-1360. [15] TIBSHIRANI R. Regression shrinkage and selection via the lasso[J]. Journal of the Royal Statistical Society Series B: Statistical Methodology, 1996, 58(1): 267-288. [16] DONOHO D L. JOHNSTONE I M. Ideal spatial adaptation by wavelet shrinkage[J/OL]. Biometrika, 1994, 81: 425-455. https://api.semanticscholar.org/CorpusID:239520. [17] ZOU. HUI. The Adaptive Lasso and Its Oracle Properties[J]. Publications of the American Statistical Association, 2006, 101(476): 1418-1429. [18] 王小燕. 谢邦昌. 马双鸽. 等. 高维数据下群组变量选择的惩罚方法综述[J]. 数理统计与管理, 2015, 34(6): 978-988. [19] HOERL A E. KENNARD R W. Ridge regression: Biased estimation for nonorthogonal problems[J]. Technometrics, 1970, 12(1): 55-67. [20] ZOU H. HASTIE T. Regularization and variable selection via the elastic net[J]. Journal of the Royal Statistical Society Series B: Statistical Methodology, 2005, 67(2): 301-320. [21] ZHANG C H. Penalized linear unbiased selection[J]. Department of Statistics and Bioinformatics, Rutgers University, 2007, 3(2010): 894-942. [22] EFRON B. HASTIE T. TIBSHIRANI J R. Least Angle Regression[J]. Annals of Statistics, 2004, 32(2): 407-451. [23] HUANG J. BREHENY P. LEE S. 等. The Mnet method for variable selection[J]. Statistica Sinica, 2016: 903-923. [24] ZENG L. XIE J. Group variable selection via SCAD-L2[J/OL]. Statistics, 2014, 48: 49-66. https://api.semanticscholar.org/CorpusID:14065404. [25] ZENG L. XIE J. Group variable selection for data with dependent structures[J]. Journal of Statistical Computation and Simulation, 2012, 82(1): 95-106. [26] YUAN M. LIN Y. Model selection and estimation in regression with grouped variables[J]. Journal of the Royal Statistical Society Series B: Statistical Methodology, 2006, 68(1): 49-67. [27] ZHAO P. ROCHA G. YU B. The composite absolute penalties family for grouped and hierarchical variable selection[J]. 2009. [28] HUANG J. MA S. XIE H. 等. A group bridge approach for variable selection[J]. Biometrika, 2009, 96(2): 339-355. [29] BREHENY P. HUANG J. Penalized methods for bi-level variable selection[J]. Statistics and Its Interface, 2009, 2(3): 369. [30] SIMON N. FRIEDMAN J. HASTIE T. 等. A Sparse-Group Lasso[J]. Journal of Computational and Graphical Statistics: A Joint Publication of American Statistical Association, Institute of Mathematical Statistics, Interface Foundation of North America, 2013, 22(2): 231-245. [31] 马双鸽. 王小燕. 方匡南. 大数据的整合分析方法[J]. 统计研究, 2015, 32(11): 3-11. [32] MA S. HUANG J. A Concave Pairwise Fusion Approach to Subgroup Analysis[J/OL]. Journal of the American Statistical Association, 2017, 112(517): 410-423. DOI:10.1080/01621459.2016.1148039. [33] CAI T. HU T. Subgroup detection in the heterogeneous partially linear additive Cox model[J/OL]. Journal of Nonparametric Statistics, 2024, 0(0): 1-26. DOI:10.1080/10485252.2024.2303103. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
